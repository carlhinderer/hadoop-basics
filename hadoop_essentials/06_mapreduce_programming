------------------------------------------------------------
CHAPTER 6 - MAPREDUCE PROGRAMMING
------------------------------------------------------------

- The Hadoop WordCount Example

    - The 'WordCount.java' program is the Hadoop MapReduce equivalent of 'Hello World'.  WordCount
        is a simple application that counts the number of occurrences of each word in a given input
        set.


    - The 'map' method processes one line at a time as provided by the specified 'TextInputFormat' 
        class.  It then splits the line into tokens separated by whitespaces using the 
        'StringTokenizer' and emits a key-value pair of <word, 1>.

      Given 2 input files with contents 'Hello World Bye World' and 'Hello Hadoop Goodbye Hadoop', the
        WordCount mapper will produce 2 maps:

          < Hello, 1>
          < World, 1>
          < Bye, 1>
          < World, 1>

          < Hello, 1>
          < Hadoop, 1>
          < Goodbye, 1>
          < Hadoop, 1>


    - WordCount sets a mapper, a combiner, and a reducer:

        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
      
      Here, the output of each map is passed through the local combiner for local aggregation and then
        sends the data onto the final reducer.  Thus, each map above the combiner performs the 
        following pre-reductions:

        < Bye, 1>
        < Hello, 1>
        < World, 2>

        < Goodbye, 1>
        < Hadoop, 2>
        < Hello, 1>


    - The 'reduce' method simply sums the values, which are the occurrence counts for each key.  
        The final output of the reducer is:

        < Bye, 1>
        < Goodbye, 1>
        < Hadoop, 2>
        < Hello, 2>
        < World, 2>



- Compiling and Running the Hadoop WordCount Example

    1. Make a local 'wordcount_classes' directory

         $ mkdir wordcount_classes


    2. Compile the 'WordCount.java' program using the 'hadoop classpath' command to include
         all the available Hadoop class paths.

         $ javac -cp `hadoop classpath` -d wordcount_classes WordCount.java


    3. The jar file can be created using the following command:

         $ jar -cvf wordcount.jar -C wordcount_classes/


    4. To run the example, create an input directory in HDFS and place a text file in the new
         directory.  For this example, we will use the 'war-and-peace.txt' file.

         $ hdfs dfs -mkdir war-and-peace-input
         $ hdfs dfs -put war-and-peace.txt war-and-peace-input


    5. Run the WordCount application.

         $ hadoop jar wordcount.jar WordCount war-and-peace-input war-and-peace-output


    6. If the job ran correctly, there should be 'war-and-peace-output' directory.  The following
         files should be in the directory:

         $ hdfs dfs -ls war-and-peace-output

           Found 2 items
           -rw-r--r--   2 hdfs hdfs          0 2015-05-24 11:14 war-and-peace-output/_SUCCESS
           -rw-r--r--   2 hdfs hdfs     467839 2015-05-24 11:14 war-and-peace-output/part-r-00000


    7. The complete list of word counts can be copied from HDFS to the working directory:

         $ hdfs dfs -get war-and-peace-output/part-r-00000.


    8. If the WordCount program is run again using the same outputs, it will fail when it tries
         to overwrite the 'war-and-peace-output' directory.  The output directory and all of its
         contents can be removed:

         $ hdfs dfs -rm -r -skipTrash war-and-peace-output



- Using the Streaming Interface

    - The Apache Hadoop streaming interface enables almost any program to use the MapReduce engine.
        The 'streams' interface will work with any program that can read and write to 'stdin' and
        'stdout'.  


    - When working in the Hadoop streaming mode, only the mapper and the reducer are created by the
        user.  This approach does have the advantage that the mapper and the reducer can be easily
        tested from the command line.  In this example, a Python mapper and reducer, 'mapper.py'
        and 'reducer.py' will be used.


    - To observe the operation of the 'mapper.py' script:

        $ echo "foo foo quux labs foo bar quux" | ./mapper.py

        Foo     1
        Foo     1
        Quux    1
        Labs    1
        Foo     1
        Bar     1
        Quux    1


    - Piping the results of the map into the 'sort' command can create a simulated shuffle phase:

        $ echo "foo foo quux labs foo bar quux" | ./mapper.py|sort -k1,1

        Bar     1
        Foo     1
        Foo     1
        Foo     1
        Labs    1
        Quux    1
        Quux    1


    - Finally, the full MapReduce process can be simulated by adding the 'reducer.py' script to the
        following command pipeline:

        $ echo "foo foo quux labs foo bar quux" | ./mapper.py|sort -k1,1|./reducer.py

        Bar     1
        Foo     3
        Labs    1
        Quux    2


    - To run this application using a Hadoop installation, 

        # Create the directory for 'War and Peace' input and remove the previous output

        $ hdfs dfs -mkdir war-and-peace-input
        $ hdfs dfs -put war-and-peace.txt war-and-peace-input
        $ hdfs dfs -rm -r -skipTrash war-and-peace-output


        # Locate the 'hadoop-streaming.jar' file in your distribution and call it

        $ hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \
            -file ./mapper.py \
            -mapper ./mapper.py \
            -file ./reducer.py -reducer ./reducer.py \
            -input war-and-peace-input/war-and-peace.txt \
            -output war-and-peace-output


        # The output should be the same as with WordCount.java


    - Note that the Python scripts used in this example could be Bash, Perl, Tcl, Awk, compiled
        C code, or any language that can read from 'stdin' and 'stdout'.


    - Although the streaming interface is rather simple, it does have some disadvantages over using
        Java directly.  In particular, not all applications are string and character based, and it
        would be awkward to try and use 'stdin' and 'stdout' as a way to transmit binary data.
        Another disadvantage is that many tuning parameters available through the full Java
        Hadoop API are not available in streaming.