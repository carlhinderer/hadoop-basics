------------------------------------------------------------
CHAPTER 7C - ESSENTIAL TOOLS - SQOOP
------------------------------------------------------------

- Apache Sqoop

    - Sqoop is a tool designed to transfer data between Hadoop and relational databases.  You can
        use Sqoop to import data from a RDBMS into HDFS, transform the data in Hadoop, and then
        export the data back into a RDBMS.


    - Sqoop can be used with any JDBC-compliant database, and has been tested with SQL Server, 
        PostgreSQL, MySQL, and Oracle.


    - The data import is done in 2 steps:

        1. Sqoop examines the database to gather the necessary metadata for the data to be imported.

        2. A map-only Hadoop job that Sqoop submits to the cluster.  This job does the actual data
             transfer using the metadata captured in the previous step.  Note that each node doing
             the import must have access to the database.

      The imported data are saved in an HDFS directory.  Sqoop will use the database name for the
        directory, or the user can specify any alternative directory where the files should be
        populated.  By default, these files contain comma-delimited fields, with new lines separating
        different records.  The format in which data are copied can be overrided.


    - Data export is also done in 2 steps:

        1. As with the import, the first step is to examine the database for metadata.  

        2. The export step again uses a map-only Hadoop job to write the data to the database.  Sqoop
             divides the input data set into splits, then uses individual map tasks to push the splits
             into the database.  Again, this process assumes that all map tasks have access to the 
             database.


    - Sqoop v1 uses specialized connectors, each of which is optimized for a specific RDBMS.  
        Connectors are plug-in components based on Sqoop's extension framework.  

      In contrast, Sqoop v2 no longer supports specialized connectors or direct imports/exports into 
        HBase or Hive.  All imports and exports are done through the JDBC interface.  Any new 
        development should be done with Sqoop v2.



- Downloading Sqoop and Loading Sample MySql Database

    # Install sqoop if its not already installed
    $ yum install sqoop sqoop-metast


    # Get the world example database, which has 3 tables:
    #   1. Country = countries of the world
    #   2. City = information about cities in those countries
    #   3. CountryLanguage = languages spoken in each country

    $ wget http://downloads.mysql.com/docs/world_innodb.sql.gz
    $ gunzip world_innodb.sql


    # Log into mysql and import the database
    $ mysql -u root

    mysql> CREATE DATABASE world;
    mysql> USE world;
    mysql> SOURCE world_innodb.sql;
    mysql> SHOW TABL


    # Look at the details of each table
    mysql> SHOW CREATE TABLE Country;
    mysql> SHOW CREATE TABLE City;
    mysql> SHOW CREATE TABLE CountryLanguage;



- Adding Sqoop User Permissions for the Local Machine and Cluster

    # Add the permissions
    mysql> GRANT ALL PRIVILEGES ON world.* To 'sqoop'@'limulus' IDENTIFIED BY 'sqoop';
    mysql> GRANT ALL PRIVILEGES ON world.* To 'sqoop'@'10.0.0.%' IDENTIFIED BY 'sqoop';
    mysql> quit


    # Log in as sqoop to test the permissions
    $ mysql -u sqoop -p

    mysql> USE world;
    mysql> SHOW TABLES;
    mysql> quit
