------------------------------------------------------------
CHAPTER 7C - ESSENTIAL TOOLS - SQOOP
------------------------------------------------------------

- Apache Sqoop

    - Sqoop is a tool designed to transfer data between Hadoop and relational databases.  You can
        use Sqoop to import data from a RDBMS into HDFS, transform the data in Hadoop, and then
        export the data back into a RDBMS.


    - Sqoop can be used with any JDBC-compliant database, and has been tested with SQL Server, 
        PostgreSQL, MySQL, and Oracle.


    - The data import is done in 2 steps:

        1. Sqoop examines the database to gather the necessary metadata for the data to be imported.

        2. A map-only Hadoop job that Sqoop submits to the cluster.  This job does the actual data
             transfer using the metadata captured in the previous step.  Note that each node doing
             the import must have access to the database.

      The imported data are saved in an HDFS directory.  Sqoop will use the database name for the
        directory, or the user can specify any alternative directory where the files should be
        populated.  By default, these files contain comma-delimited fields, with new lines separating
        different records.  The format in which data are copied can be overrided.


    - Data export is also done in 2 steps:

        1. As with the import, the first step is to examine the database for metadata.  

        2. The export step again uses a map-only Hadoop job to write the data to the database.  Sqoop
             divides the input data set into splits, then uses individual map tasks to push the splits
             into the database.  Again, this process assumes that all map tasks have access to the 
             database.


    - Sqoop v1 uses specialized connectors, each of which is optimized for a specific RDBMS.  
        Connectors are plug-in components based on Sqoop's extension framework.  

      In contrast, Sqoop v2 no longer supports specialized connectors or direct imports/exports into 
        HBase or Hive.  All imports and exports are done through the JDBC interface.  Any new 
        development should be done with Sqoop v2.



- Downloading Sqoop and Loading Sample MySql Database

    # Install sqoop if its not already installed
    $ yum install sqoop sqoop-metast


    # Get the world example database, which has 3 tables:
    #   1. Country = countries of the world
    #   2. City = information about cities in those countries
    #   3. CountryLanguage = languages spoken in each country

    $ wget http://downloads.mysql.com/docs/world_innodb.sql.gz
    $ gunzip world_innodb.sql


    # Log into mysql and import the database
    $ mysql -u root

    mysql> CREATE DATABASE world;
    mysql> USE world;
    mysql> SOURCE world_innodb.sql;
    mysql> SHOW TABL


    # Look at the details of each table
    mysql> SHOW CREATE TABLE Country;
    mysql> SHOW CREATE TABLE City;
    mysql> SHOW CREATE TABLE CountryLanguage;



- Adding Sqoop User Permissions for the Local Machine and Cluster

    # Add the permissions
    mysql> GRANT ALL PRIVILEGES ON world.* To 'sqoop'@'limulus' IDENTIFIED BY 'sqoop';
    mysql> GRANT ALL PRIVILEGES ON world.* To 'sqoop'@'10.0.0.%' IDENTIFIED BY 'sqoop';
    mysql> quit


    # Log in as sqoop to test the permissions
    $ mysql -u sqoop -p

    mysql> USE world;
    mysql> SHOW TABLES;
    mysql> quit



- Importing Data Using Sqoop

    # As a test, use Sqoop to list databases in MySql (limulus is the local host name)
    $ sqoop list-databases --connect jdbc:mysql://limulus/world --username sqoop --password sqoop

    information_schema
    test
    world


    # Similarly, we can list the tables in the world database
    $ sqoop list-tables --connect jdbc:mysql://limulus/world --username sqoop --password sqoop

    City
    Country
    CountryLanguage


    # To import data, we need to make a directory in HDFS
    $ hdfs dfs -mkdir sqoop-mysql-import


    # Import the Country table into HDFS
    $ sqoop import --connect jdbc:mysql://limulus/world  --username sqoop --password sqoop 
           --table Country  -m 1 --target-dir /user/hdfs/sqoop-mysql-import/country


    # Confirm that the import succeeded
    $ hdfs dfs -ls sqoop-mysql-import/country

    sqoop-mysql-import/world/_SUCCESS
    sqoop-mysql-import/world/part-m-00000


    # View the countries file
    $ hdfs dfs -cat sqoop-mysql-import/country/part-m-00000

    ABW,Aruba,North America,Caribbean,193.0,null,103000,78.4,828.0,793.0,Aruba,
        Nonmetropolitan Territory of The Netherlands,Beatrix,129,AW
    ...
    ZWE,Zimbabwe,Africa,Eastern Africa,390757.0,1980,11669000,37.8,5951.0,8670.0,
        Zimbabwe,Republic,Robert G. Mugabe,4068,Z



- Importing with an Options File

    To make the Sqoop command more convenient, we can create an options file and use it
      on the command line.

    /* world-options.txt */
    import
    --connect
    jdbc:mysql://limulus/world
    --username
    sqoop
    --password
    sqoop


    # Perform an input with the options file
    $ sqoop  --options-file world-options.txt --table City  -m 1 
         --target-dir /user/hdfs/sqoop-mysql-import/city



- Importing with an Included SQL Query

    It is also possible to include a SQL query in the import step.

    # Only import Canadian cities, the '-m 1' option means only a single mapper is used
    $ sqoop  --options-file world-options.txt -m 1 
         --target-dir /user/hdfs/sqoop-mysql-import/canada-city 
         --query "SELECT ID,Name from City WHERE CountryCode='CAN' AND \$CONDITIONS"


    # Inspecting the results confirms that only Canadian cities were imported
    $  $ hdfs dfs -cat sqoop-mysql-import/canada-city/part-m-00000

    1810,MontrÃ„al
    1811,Calgary
    1812,Toronto
    ...
    1856,Sudbury
    1857,Kelowna
    1858,Barrie



- Importing with Multiple Mappers

    Since there was only one mapper process, only one copy of the query needed to be run on the
      database.  The results are also reported in a single file 'part-m-0000'.

    Multiple mappers can be used to process the query if the --split-by-- option is used.  The
      'split-by' option is used to parallelize the SQL query.  Each parallel task runs a subset
      of the main query, with the results of each subquery being partitioned by bounding
      conditions inferred by Sqoop.  

    Your query must include the tokens $CONDITIONS that each Sqoop process will replace with a 
      unique condition expression based on the --split-by option. Note that $CONDITIONS is not an
      environment variable. Although Sqoop will try to create balanced sub-queries based on the 
      range of your primary key, it may be necessary to split on another column if your primary 
      key is not uniformly distributed.


    # First, remove the results of the previous query
    $ hdfs dfs -rm -r -skipTrash  sqoop-mysql-import/canada-city


    # Run the query using 4 mappers, where we split by the ID number
    $ sqoop  --options-file world-options.txt -m 4 
         --target-dir /user/hdfs/sqoop-mysql-import/canada-city 
         --query "SELECT ID,Name from City WHERE CountryCode='CAN' AND \$CONDITIONS" 
         --split-by ID


    # Now, we have 4 results files
    $ hdfs dfs -ls  sqoop-mysql-import/canada-city

    sqoop-mysql-import/canada-city/_SUCCESS
    sqoop-mysql-import/canada-city/part-m-00000
    sqoop-mysql-import/canada-city/part-m-00001
    sqoop-mysql-import/canada-city/part-m-00002
    sqoop-mysql-import/canada-city/part-m-00003



- Exporting Data from HDFS to MySQL

    Sqoop can also be used to export data from HDFS.  The first step to create tables for
      exported data.  There are actually 2 tables needed for each exported table.  The first
      table holds the exported data ('CityExport') and the second is used for staging the
      exported data ('CityExportStaging').


    # Create the export tables in mysql
    mysql> CREATE TABLE 'CityExport' (
       'ID' int(11) NOT NULL AUTO_INCREMENT,
       'Name' char(35) NOT NULL DEFAULT '',
       'CountryCode' char(3) NOT NULL DEFAULT '',
       'District' char(20) NOT NULL DEFAULT '',
       'Population' int(11) NOT NULL DEFAULT '0',
       PRIMARY KEY ('ID'));

    mysql> CREATE TABLE 'CityExportStaging' (
       'ID' int(11) NOT NULL AUTO_INCREMENT,
       'Name' char(35) NOT NULL DEFAULT '',
       'CountryCode' char(3) NOT NULL DEFAULT '',
       'District' char(20) NOT NULL DEFAULT '',
       'Population' int(11) NOT NULL DEFAULT '0',
       PRIMARY KEY ('ID'));


    Next, create a 'cities-export-options.txt' file similar to the 'world-options.txt' 
      created earlier, but use the 'export' command instead of the 'import' command.  Then,
      we can perform the export.


    # Create export options file

    /* cities-export-options.txt */
    export
    --connect
    jdbc:mysql://limulus/world
    --username
    sqoop
    --password
    sqoop


    # Export the cities data back into mysql
    $ sqoop --options-file cities-export-options.txt --table CityExport  
         --staging-table CityExportStaging  --clear-staging-table -m 4 
         --export-dir /user/hdfs/sqoop-mysql-import/city


    # Check to make sure the export worked correctly
    $ mysql> select * from CityExport limit 10;



- Handy Cleanup Commands

    # Remove a table in MySql
    mysql> drop table 'CityExportStaging';

    # Remove all data in a table
    mysql> delete from CityExportStaging;

    # Remove imported files
    $ hdfs dfs -rm -r  -skipTrash sqoop-mysql-import/{country,city, canada-city}