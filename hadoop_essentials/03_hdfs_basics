------------------------------------------------------------
CHAPTER 3 - HDFS BASICS
------------------------------------------------------------

- HDFS Features

    - HDFS was not designed as a true parallel file system.  Rather, the design assumes a large
        file write-once/read-many model.  For instance, HDFS rigorously restricts data writing
        to one user at a time.  All additional writes are append-only, and there is no random
        writing to HDFS files.  Bytes are always appended to the end of a stream, and byte
        streams are guaranteed to be stored in the order written.


    - Design is based on Google File System.


    - HDFS is designed for data streaming where large amounts of data are read from disk in bulk.
        The HDFS block size is typically 64MB or 128MB.  Thus, this approach is entirely 
        unsuitable for standard POSIX file system use.  In addition, due to the sequential 
        nature of the data, there is no local caching mechanism.


    - The most interesting aspect of HDFS is data locality.  HDFS is designed to work on the same
        hardware as the compute portion of the cluster.  So, a single server node in the cluster
        is often both a computation engine and a storage engine for the application.


    - HDFS has a redundant design that can tolerate system failure and still provide the data
        needed by the compute portion of the program.



- HDFS Components

    - The design of HDFS is based on 2 types of nodes: a NameNode and multiple DataNodes.  For
        a minimal Hadoop installation, there needs to be a single NameNode daemon and a single
        DataNode daemon running on at least one machine.


    - The design is a master/slave architecture in which the master (NameNode) manages the file
        system namespace and regulates access to files by clients.  File system namespace 
        operations such as opening, closing, and renaming files and directories are all
        managed by the NameNode.  The NameNode also determines the mapping of blocks to
        DataNodes and handles DataNode failures.


    - The slaves (DataNodes) are responsible for serving read and write requests from the file
        system to the clients.  The NameNode manages block creation, deletion, and replication.


    - For an example of writing,

        1. A client wants to write data, so it first communicates with the NameNode and requests
             to create a file.

        2. The NameNode determines how many blocks are needed and provides the client with the
             DataNodes that will store the data.

        3. As part of the storage process, the data blocks are replicated after they are written
             to the assigned node.  Depending on how many nodes are in the cluster, the NameNode
             will attempt to write replicas of the data blocks on nodes that are on 
             separate racks if possible.  If there is only one rack, the replicated blocks are
             written to other servers in the same rack.

        4. After the DataNode acknowledges that the file block replication is complete, the 
             client closes the file and informs the NameNode that the operation is complete. 
             Note that the NameNode does not write directly to the DataNodes.  It does, however,
             give the client a limited amount of time to complete the operation.  If it does not
             complete


    - For an example of reading,

        1. The client requests a file from the NameNode, which returns the best DataNodes from 
             which to read the data.  The client then accesses the the data directly from the
             DataNodes.

        2. Thus, once the metadata has been delivered to the client, the NameNode steps back and
             lets the conversation between the client and the DataNodes proceed.  While the data
             transfer is progressing, the NameNode also monitors the DataNodes by listening for
             heartbeats sent from DataNodes.  

        3. The lack of a heartbeat signal indicates a potential node failure.  In such a case, 
             the NameNode will route around the failed DataNode and begin re-replicating the 
             now-missing blocks.  Because the file system is redundant, DataNodes can be taken
             offline for maintenance by informing the NameNode of the DataNodes to exclude from
             the HDFS pool.


    - The NameNode stores all mappings between data blocks and physical DataNodes in memory.  Upon
        startup, each DataNode provides a block report to the NameNode.  The block reports are sent
        every 10 heartbeats.  The reports enable the NameNode to keep an up-to-date account of all
        data blocks in the cluster.


    - In almost all Hadoop deployments, there is a SecondaryNameNode.  It is not explicitly required,
        but is highly recommended.  It is not an active failover node and cannot replace the primary
        NameNode in case of failure.  It's purpose is to perform periodic checkpoints that 
        evaluate the status of the NameNode.  

      Recall that the NameNode keeps all system metadata in memory for fast access.  It also has
        2 disk files that track changes to the metadata:

          1. An image of the file system state when NameNode was started.  This file begins with
               'fsimage_' and is used only at startup by the NameNode.

          2. A series of modifications done to the file system after starting the NameNode.  These
               files begin with 'edit_' and reflect the changes made after the 'fsimage_' file was
               read.  

      The location of these files is set by the 'dfs.namenode.name.dir' property in the 
        'hdfs-site.xml' file.

      The SecondaryNameNode periodically downloads 'fsimage' and 'edits' files, joins them into a
        new 'fsimage', and uploads the new 'fsimage' file to the NameNode.  Thus, when the NameNode
        restarts, the 'fsimage' file is reasonably up to date, and requires only edit logs to be
        applied since the last checkpoint.  If the SecondaryNameNode were not running, a restart of
        the NameNode could take a prohibitively long time due to the number of changes in the file
        system.



- HDFS Block Replication

    - When HDFS writes a file, it is replicated across the cluster.  The amount of replication is 
        based on the value of 'dfs.replication' in the 'hdfs-site.xml' file.  This default value
        can be overruled with the 'dfs-setrep' command.


    - For Hadoop clusters containing more than 8 DataNodes, the replication value is usually set
        to 3.  For a cluster with 2-8 DataNodes, the default is 2.  


    - If several machines must be involved in the serving of a file, then a file could be rendered
        unavailable by the loss of any of those machines.  HDFS combats this problem by replicating
        each block across a number of machines.


    - The HDFS default block size is 64MB.  In a typical OS, the block size is 4KB or 8KB.  The
        HDFS default block size is not the minimum block size, however.  If a 20KB file is written
        to HDFS, it will create a block that is approximately 20KB in size.  If a file of size
        80MB is written to HDFS, a 64MB block and a 16MB block will be created.


    - HDFS blocks are not exactly the same as the data splits used by the MapReduce process. The 
        HDFS blocks are based on size, while the splits are based on a logical partitioning of the 
        data. For instance, if a file contains discrete records, the logical split ensures that a 
        record is not split physically across two separate servers during processing. Each HDFS 
        block may consist of one or more splits.



- HDFS Safe Mode

    - When the NameNode starts, it enters a read-only safe mode where blocks cannot be replicated 
        or deleted. Safe Mode enables the NameNode to perform two important processes:

        1. The previous file system state is reconstructed by loading the fsimage file into memory 
             and replaying the edit log.

        2. The mapping between blocks and data nodes is created by waiting for enough of the 
             DataNodes to register so that at least one copy of the data is available. Not all 
             DataNodes are required to register before HDFS exits from Safe Mode. The registration 
             process may continue for some time.


    - HDFS may also enter Safe Mode for maintenance using the 'hdfs dfsadmin-safemode' command or 
        when there is a file system issue that maust be addressed by the administrator.



- Rack Awareness

    - Rack awareness deals with data locality. Recall that one of the main design goals of Hadoop 
        MapReduce is to move the computation to the data. Assuming that most data center networks 
        do not offer full bisection bandwidth, a typical Hadoop cluster will exhibit three levels 
        of data locality:

        1. Data resides on the local machine (best).

        2. Data resides in the same rack (better).

        3. Data resides in a different rack (good).


    - When the YARN scheduler is assigning MapReduce containers to work as mappers, it will try to 
        place the container first on the local machine, then on the same rack, and finally on another 
        rack.


    - In addition, the NameNode tries to place replicated data blocks on multiple racks for improved 
        fault tolerance. In such a case, an entire rack failure will not cause data loss or stop HDFS 
        from working. Performance may be degraded, however.


    - HDFS can be made rack-aware by using a user-derived script that enables the master node to map 
        the network topology of the cluster. A default Hadoop installation assumes all the nodes belong 
        to the same (large) rack. In that case, there is no option 3.



- NameNode High Availability

    - With early Hadoop installations, the NameNode was a single point of failure that could bring 
        down the entire Hadoop cluster.  NameNode hardware often employed redundant power supplies 
        and storage to guard against such problems, but it was still susceptible to other failures.
        The solution was to implement NameNode High Availability as a means to provide true failover
        service.


    - A HA Hadoop cluster has 2 or more separate NameNode machines.  Each machine is configured with
        the exact same software.  One of the machines is in an Active state, and the other is in a
        Standby state.  The Active NameNode is resposible for all client HDFS operations in the 
        cluster.  The Standby NameNode maintains enough state to provide a fast failover.


    - To guarantee the file system is preserved, both the Active and Standby NameNodes receive block
        reports from the DataNodes.  The Active node also sends all file system edits to a quorum of
        Journal nodes.  The Standby node continuously reads the edits from the JournalNodes to ensure
        its namespace is synchronized with that of the Active node.


    - Apache ZooKeeper is used to monitor the NameNode health.  ZooKeeper is a highly available service
        for maintaining small amounts of coordination data, notifying clients of changes in that 
        data, and monitoring clients for failures.  HDFS Failover relies on ZooKeeper for failure
        detection and for Standby to Active NameNode election.



- HDFS NameNode Federation

    - Older versions of HDFS provided a single namespace for the entire cluster managed by a single
        NameNode.  Thus, the resources of a single NameNode determined the size of the namespace.
        Federation addresses this limitation by adding support for multiple NameNodes/namespaces
        to the HDFS file system.


    - The key benefits are:

        1. Namespace scalability = the HDFS cluster scales horizontally

        2. Better performance = adding more NameNodes scales the file system reads/writes

        3. System isolation = multiple NameNodes enable different categories of applications



- HDFS Checkpoints and Backups

    - An HDFS BackupNode is similar to a SecondaryNameNode, but it also maintains an up-to-date
        copy of the file system namespace both in memory and on disk.  Unlike a CheckpointNode,
        the BackupNode does not need to download the 'fsimage' and 'edits' files from the 
        active NameNode, because it already has an up-to-date namespace state in memory.


    - A NameNode supports one BackupNode at a time.  No CheckpointNodes may be registered if a
        BackupNode is in use.



- HDFS Snapshots

    - HDFS snapshots are similar to backups, but are created by administrators using the 
        'hdfs dfs snapshot' command.  HDFS snapshots are read-only, point-in-time copies of the
        file system.


    - Snapshots have these features:

        1. Snapshots can be taken of a sub-tree of the file system or the entire file system

        2. Snapshots can be used for data backup, protection against user errors, and disaster
             recover.

        3. Snapshot creation is instantaneous

        4. Blocks on the DataNode are not copied, because the snapshot files record the block 
             list and file size.  There is no data copying, although it appears to the user that
             there are duplicate files.

        5. Snapshots do not adversely afect regular HDFS operations.



- HDFS NFS Gateway

    - The HDFS NFS Gateway supports NFSv3 and enables HDFS to be mounted as part of the client's
        local file system.


    - This feature offers these capabilities:

        1. Users can easily download/upload files from/to the HDFS file system to/from their 
             local file system.

        2. Users can stream data directly to HDFS through the mount point.  Appending to a file 
             is supported, but random write capability is not supported.



- HDFS User Commands

    # Get hdfs version
    $ hdfs version


    # List files in hdfs
    $ hdfs dfs -ls /


    # List files in home directory
    $ hdfs dfs -ls
    $ ddfs dfs -ls /user/hdfs


    # Create a directory in hdfs
    $ hdfs dfs -mkdir stuff


    # Copy files to hdfs (file 'test' is copied to directory 'stuff')
    $ hdfs dfs -put test stuff


    # Confirm that the file transfer occurred
    $ hdfs dfs -ls stuff


    # Copy files from hdfs
    $ hdfs dfs -get stuff/test test-local


    # Copy files within hdfs
    $ hdfs dfs -cp stuff/test test.hdfs


    # Delete a file within hdfs
    $ hdfs dfs -rm test.hdfs
    $ hdfs dfs -rm -skipTrash test.hdfs


    # Delete a directory in hdfs
    $ hdfs dfs -rm -r -skipTrash stuff


    # Get an hdfs status report
    $ hdfs dfsadmin -report



- HDFS Java Application Example

    Here, we have the java application 'HadoopDFSFileReadWrite.java' and we'll go through the
      steps needed to build and run it.

    To be able to read or write to HDFS in a Java application, you need to create a 
      'Configuration' object and pass configuration parameters.  This example assumes that the
      Hadoop configuration files are in '/etc/hadoop.conf'.


    # First, create a directory to hold the classes
    $ mkdir HDFSClient-classes


    # Compile the program using 'hadoop classpath' to ensure all the class paths are available
    $ javac -cp 'hadoop classpath' -d HDFSClient-classes HDFSClient.java


    # Create a Java archive file
    $ jar -cvfe HDFSClient.jar org/myorg.HDFSClient -C HDFSClient-classes/ .


    # Run to check for available options
    $ hadoop jar ./HDFSClient.jar


    # Copy file from local system to hdfs
    $ hadoop jar ./HDFSClient.jar add ./NOTES.txt /user/hdfs


    # Check that the file is now in hdfs
    hdfs dfs -ls NOTES.txt



- HDFS C Application Example

    HDFS can be used in C programs by incorporating the Java Native Interface (JNI)-based API
      for HDFS.  The library, 'libhdfs', provices a simple C API to manipulate HDFS files and the
      file system.  'libhdfs' is normally available as part of the Hadoop installation.


    # Load the Hadoop environment paths
    $ . /etc/hadoop/conf/hadoop-env.sh


    # Compile with gcc
    $ gcc hdfs-simple-test.c -I$HADOOP_LIB/include -I$JAVA_HOME/include -L$HADOOP_LIB/lib  \
          -L$JAVA_HOME/jre/lib/amd64/server -ljvm -lhdfs -o hdfs-simple-test


    # Set the run-time library path
    $ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_HOME/jre/lib/amd64/server:$HADOOP_LIB/lib


    # Set the hadoop classpath
    $ export CLASSPATH='hadoop classpath -glob'


    # Run the program
    $ /hdfs-simple-test


    # Inspect the new file contents
    $ hdfs dfs -cat /tmp/testfile.txt