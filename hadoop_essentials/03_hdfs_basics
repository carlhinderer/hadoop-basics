------------------------------------------------------------
CHAPTER 3 - HDFS BASICS
------------------------------------------------------------

- HDFS Features

    - HDFS was not designed as a true parallel file system.  Rather, the design assumes a large
        file write-once/read-many model.  For instance, HDFS rigorously restricts data writing
        to one user at a time.  All additional writes are append-only, and there is no random
        writing to HDFS files.  Bytes are always appended to the end of a stream, and byte
        streams are guaranteed to be stored in the order written.


    - Design is based on Google File System.


    - HDFS is designed for data streaming where large amounts of data are read from disk in bulk.
        The HDFS block size is typically 64MB or 128MB.  Thus, this approach is entirely 
        unsuitable for standard POSIX file system use.  In addition, due to the sequential 
        nature of the data, there is no local caching mechanism.


    - The most interesting aspect of HDFS is data locality.  HDFS is designed to work on the same
        hardware as the compute portion of the cluster.  So, a single server node in the cluster
        is often both a computation engine and a storage engine for the application.


    - HDFS has a redundant design that can tolerate system failure and still provide the data
        needed by the compute portion of the program.



- HDFS Components

    - The design of HDFS is based on 2 types of nodes: a NameNode and multiple DataNodes.  For
        a minimal Hadoop installation, there needs to be a single NameNode daemon and a single
        DataNode daemon running on at least one machine.


    - The design is a master/slave architecture in which the master (NameNode) manages the file
        system namespace and regulates access to files by clients.  File system namespace 
        operations such as opening, closing, and renaming files and directories are all
        managed by the NameNode.  The NameNode also determines the mapping of blocks to
        DataNodes and handles DataNode failures.


    - The slaves (DataNodes) are responsible for serving read and write requests from the file
        system to the clients.  The NameNode manages block creation, deletion, and replication.


    - For an example of writing,

        1. A client wants to write data, so it first communicates with the NameNode and requests
             to create a file.

        2. The NameNode determines how many blocks are needed and provides the client with the
             DataNodes that will store the data.

        3. As part of the storage process, the data blocks are replicated after they are written
             to the assigned node.  Depending on how many nodes are in the cluster, the NameNode
             will attempt to write replicas of the data blocks on nodes that are on 
             separate racks if possible.  If there is only one rack, the replicated blocks are
             written to other servers in the same rack.

        4. After the DataNode acknowledges that the file block replication is complete, the 
             client closes the file and informs the NameNode that the operation is complete. 
             Note that the NameNode does not write directly to the DataNodes.  It does, however,
             give the client a limited amount of time to complete the operation.  If it does not
             complete


    - For an example of reading,

        1. The client requests a file from the NameNode, which returns the best DataNodes from 
             which to read the data.  The client then accesses the the data directly from the
             DataNodes.

        2. Thus, once the metadata has been delivered to the client, the NameNode steps back and
             lets the conversation between the client and the DataNodes proceed.  While the data
             transfer is progressing, the NameNode also monitors the DataNodes by listening for
             heartbeats sent from DataNodes.  

        3. The lack of a heartbeat signal indicates a potential node failure.  In such a case, 
             the NameNode will route around the failed DataNode and begin re-replicating the 
             now-missing blocks.  Because the file system is redundant, DataNodes can be taken
             offline for maintenance by informing the NameNode of the DataNodes to exclude from
             the HDFS pool.


    - The NameNode stores all mappings between data blocks and physical DataNodes in memory.  Upon
        startup, each DataNode provides a block report to the NameNode.  The block reports are sent
        every 10 heartbeats.  The reports enable the NameNode to keep an up-to-date account of all
        data blocks in the cluster.


    - In almost all Hadoop deployments, there is a SecondaryNameNode.  It is not explicitly required,
        but is highly recommended.  It is not an active failover node and cannot replace the primary
        NameNode in case of failure.  It's purpose is to perform periodic checkpoints that 
        evaluate the status of the NameNode.  

      Recall that the NameNode keeps all system metadata in memory for fast access.  It also has
        2 disk files that track changes to the metadata:

          1. An image of the file system state when NameNode was started.  This file begins with
               'fsimage_' and is used only at startup by the NameNode.

          2. A series of modifications done to the file system after starting the NameNode.  These
               files begin with 'edit_' and reflect the changes made after the 'fsimage_' file was
               read.  

      The location of these files is set by the 'dfs.namenode.name.dir' property in the 
        'hdfs-site.xml' file.

      The SecondaryNameNode periodically downloads 'fsimage' and 'edits' files, joins them into a
        new 'fsimage', and uploads the new 'fsimage' file to the NameNode.  Thus, when the NameNode
        restarts, the 'fsimage' file is reasonably up to date, and requires only edit logs to be
        applied since the last checkpoint.  If the SecondaryNameNode were not running, a restart of
        the NameNode could take a prohibitively long time due to the number of changes in the file
        system.



- HDFS Block Replication

    - When HDFS writes a file, it is replicated across the cluster.  The amount of replication is 
        based on the value of 'dfs.replication' in the 'hdfs-site.xml' file.  This default value
        can be overruled with the 'dfs-setrep' command.


    - For Hadoop clusters containing more than 8 DataNodes, the replication value is usually set
        to 3.  For a cluster with 2-8 DataNodes, the default is 2.  


    - If several machines must be involved in the serving of a file, then a file could be rendered
        unavailable by the loss of any of those machines.  HDFS combats this problem by replicating
        each block across a number of machines.


    - The HDFS default block size is 64MB.  In a typical OS, the block size is 4KB or 8KB.  The
        HDFS default block size is not the minimum block size, however.  If a 20KB file is written
        to HDFS, it will create a block that is approximately 20KB in size.  If a file of size
        80MB is written to HDFS, a 64MB block and a 16MB block will be created.


    - HDFS blocks are not exactly the same as the data splits used by the MapReduce process. The 
        HDFS blocks are based on size, while the splits are based on a logical partitioning of the 
        data. For instance, if a file contains discrete records, the logical split ensures that a 
        record is not split physically across two separate servers during processing. Each HDFS 
        block may consist of one or more splits.



- HDFS Safe Mode

    - When the NameNode starts, it enters a read-only safe mode where blocks cannot be replicated 
        or deleted. Safe Mode enables the NameNode to perform two important processes:

        1. The previous file system state is reconstructed by loading the fsimage file into memory 
             and replaying the edit log.

        2. The mapping between blocks and data nodes is created by waiting for enough of the 
             DataNodes to register so that at least one copy of the data is available. Not all 
             DataNodes are required to register before HDFS exits from Safe Mode. The registration 
             process may continue for some time.


    - HDFS may also enter Safe Mode for maintenance using the 'hdfs dfsadmin-safemode' command or 
        when there is a file system issue that maust be addressed by the administrator.



- Rack Awareness

    - Rack awareness deals with data locality. Recall that one of the main design goals of Hadoop 
        MapReduce is to move the computation to the data. Assuming that most data center networks 
        do not offer full bisection bandwidth, a typical Hadoop cluster will exhibit three levels 
        of data locality:

        1. Data resides on the local machine (best).

        2. Data resides in the same rack (better).

        3. Data resides in a different rack (good).


    - When the YARN scheduler is assigning MapReduce containers to work as mappers, it will try to 
        place the container first on the local machine, then on the same rack, and finally on another 
        rack.


    - In addition, the NameNode tries to place replicated data blocks on multiple racks for improved 
        fault tolerance. In such a case, an entire rack failure will not cause data loss or stop HDFS 
        from working. Performance may be degraded, however.


    - HDFS can be made rack-aware by using a user-derived script that enables the master node to map 
        the network topology of the cluster. A default Hadoop installation assumes all the nodes belong 
        to the same (large) rack. In that case, there is no option 3.