------------------------------------------------------------
CHAPTER 1 - BACKGROUND AND CONCEPTS
------------------------------------------------------------

- Features of Hadoop Data Processing

    1. Core parts are open source under Apache licensing

    2. Analysis usually involves large, unstructured data sometimes in the petabyte range

    3. Traditionally, data is stored across multiple servers using HDFS, but other storage
         systems are now used

    4. MapReduce jobs can scale from a single server to thousands

    5. Other programming models are supported in Hadoop v2 with YARN

    6. Hadoop core components were designed to run on commodity hardware and the cloud

    7. Hadoop offers many fault-tolerant features that enable operation over large number
         of servers

    8. Many projects and applications are built on top of the Hadoop infrastructure

    9. Although the core components are written in Java, Hadoop applications can use almost
         any programming language



- Defining Big Data

    - Characteristics of Big Data

        1. Volume = sheer size of the data

        2. Variety = comes from a variety of sources and is not necessarily related to other 
                       data sources

        3. Velocity = speed at which data can be generated and processed

        4. Variability = data may be highly variable, incomplete, and inconsistent

        5. Complexity = relationships between data may be complex and non-relational


    - "5 V's of Big Data"

        1. Volume
        2. Variety
        3. Velocity
        4. Value = how to monetize the data
        5. Veracity = varying quality of the data


    - Size of Big Data

        - Sweet spot starts at ~100 GB
        - Most common amount of data for the average company is 10-30 TB
        - 90% of jobs on Facebook cluster are < 100 GB


    - Types of Data

        - Media, including video, audio, and photos
        - Web data like logs, click trails, and emails
        - Documents, periodicals, books
        - Scientific research data (ie simulations or human genome data)
        - Stock transactions, customer data, retail purchases
        - Telecommunications data like phone records
        - Public records
        - IOT data
        - Real-time sensor data



- Hadoop as a Data Lake

    - Data Lake 

        - Schema-on-read
        - Vast repository for raw data, use it as needed
        - All data remains available, no need to make assumptions about future use
        - Add data is sharable, not compartmentalized
        - All access methods are available (ie MapReduce, graph processing, etc)


    - Data Warehouse

        - Schema-on-write with ETL and predetermined schema
        - Valuable business tool, will not be replaced by Hadoop



- Hadoop v1 vs v2

    - Hadoop v1

        - Only meant for MapReduce jobs
        - Task scheduler and MapReduce engine were a single component
        - Only Java could be used
        - Maximum cluster size was 4000 nodes
        - Job tracker failure killed all running and queued jobs
        - Iterative applications were 10x slower


    - Hadoop v2

        - Scheduling and resource management are separate from jobs running
        - MapReduce is a separate application that runs on YARN
        - YARN presents a generalized interface for any application framework



- MapReduce Example

    We load 'War and Peace' and we want to find out how many times the word 'Kutuzov' appears.

      [Load Step]
      Load file into HDFS, it is sliced and added to nodes

      [Map Step]
      User query is 'mapped' to all slices.
      The number of times 'Kutuzov' appears on each node is tallied.

      [Reduce Step]
      Results are 'reduced' to all slices.
      The counts from each node are summed into the output value.



- MapReduce Advantages

    - Uses a 'functional approach' because the original input data does not change.  The steps
        only create new data.

    - Highly scalable (often linear scalability)

    - Easily managed work flow

    - Fault tolerance, a failed process can be restarted on other nodes, since inputs are 
        immutable

    - MapReduce (aka 'Data Parallel Problem' or 'SIMD') is a powerful paradigm for solving many
        problems.



- Hadoop v2 YARN Operation Design

    - YARN schedules and manages all jobs on the cluster

    - Worker nodes are managed by a NodeManager process that works with the Resource Manager

    - Job resources are portioned out in containers, which are computing resources usually 
        defined as one processing core and an amount of memory

    - The Resource Manager and NodeManager have no information about the jobs.  They manage the
        conatiners running on the cluster and are task neutral.


    - Typical Hadoop v2 MapReduce job:

        1. Clients submit jobs to YARN

        2. YARN selects a node and instructs the Node Manager to start an Application Master 
             (App Mstr)

        3. The Application Master (running in a container) requests additional containers 
             (resources) from YARN

        4. The assigned containers are started and managed on the appropriate nodes by the Node
             Managers

        5. Once the Application Master and containers are connected and running, YARN and the
             Node Managers step away from the job

        6. All job progress is reported back to the Application Master

        7. When a task that runs in a container is completed, the Node Manager makes the container
             available to YARN


    - Advantages of separating user jobs from the scheduler

        - Better scale
        - New programming models and services (ie Giraph, Spark, MPI)
        - Improved cluster utilization
        - Application agility
        - Move beyond Java



- 