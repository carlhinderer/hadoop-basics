------------------------------------------------------------
CHAPTER 7 - ESSENTIAL TOOLS
------------------------------------------------------------

- Apache Pig

    - Pig is a high-level language that enables programmers to write complex MapReduce
        transformations using a simple scripting language.  Pig Latin (the actual language) defines
        a set of transformations on the data such as 'aggregate', 'join', and 'sort'.  Pig is
        often used for ETL in data pipelines, quick research on raw data, and iterative data
        processing.


    - Pig has several usage modes:

        1. Local Mode (with MapReduce)
        2. Local Mode (with Tez)
        3. Interactive Mode
        4. Batch Mode

      This allows Pig applications to be developed locally in interactive modes, using small amounts
        of data, and then run at scale on the cluster in a production mode.



- Pig Walkthrough

    - In this simple example, we'll use Pig to extract user names from the '/etc/passwd' file.  The
        following assumes the user is 'hdfs', but we could use another user name.


      # Copy the 'passwd' file to a working directory for local Pig operation
      $ cp /etc/passwd .


      # Copy the file into HDFS
      $ hdfs dfs -put passwd passwd
      $ hdfs dfs -ls passwd


      # In this example, all processing is done on the local machine (Hadoop is not used)
      $ pig -x local


      # If Pig starts correctly, you'll see a grunt> prompt
      grunt> A = load 'passwd' using PigStorage(':');
      grunt> B = foreach A generate $0 as id;
      grunt> dump B;


      # The processing will start and a list of user names will be printed to the screen
      # To exit the interactive session, use the 'quit' command
      grunt> quit



- Other Pig Options

    - Here, we'll start pig in other modes.

        # Either of these will start Hadoop MapReduce Pig
        $ pig
        $ pig -x mapreduce


        # If you have tez installed, it can be used as an engine
        $ pig -x tez


    - Pig can also be run from a script.

        /* id.pig */
        A = load 'passwd' using PigStorage(':');   -- load the passwd fild
        B = foreach A generate $0 as id;           -- extract the user ids
        dump B;
        store B into 'id.out';                     -- write results to directory id.out


        # Run the script
        $ /bin/rm -r id.out/
        $ pig -x local id.pig

        # If the script worked correctly, you should see a data file with results and a 
        #   zero-length file with the name _SUCCESS


        # Run the script with the MapReduce version
        $ hdfs dfs -rm -r id.out
        $ pig id.pig



- Apache Hive

    - Hive is a data warehouse infrastructure built on top of Hadoop for providing data
        summarization, ad hoc queries, and the analysis of large data sets using the SQL-like
        language HiveQL.  Hive is considered the de facto standard for interactive SQL queries over
        petabytes of data using Hadoop.  It has the following features:

        - Tools to enable easy ETL
        - A mechanism to impose structure on a variety of data formats
        - Access to files stored either directly in HDFS or in other data storage systems like HBase
        - Query optimization via MapReduce and Tez (optimized MapReduce)


    - Hive provides users who are already familiar with SQL the capability to query the data on 
        Hadoop clusters.  At the same time, Hive makes it possible for programmers who are familiar
        with the MapReduce framework to add their own custom mappers and reducers to Hive queries.


    - Hive queries can be dramatically accelerated using the Apache Tez framework.



- Simple Hive Example

    # To start hive, just use the 'hive' command
    $ hive


    # As a simple test, create and drop a table
    hive> CREATE TABLE pokes (foo INT, bar STRING);
    hive> SHOW TABLES;
    hive> DROP TABLE pokes;


    # Create a table to summarize web server logs
    hive> CREATE TABLE logs (t1 string, t2, string, t3 string, t4 string, t5 string, t6 string, 
              t7 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ';


    # Load the 'sample.log' file from the book examples (stored in local directory, not hdfs)
    hive> LOAD DATA LOCAL INPATH 'sample.log' OVERWRITE INTO TABLE logs;


    # Apply the select step to the file (note this invokes a MapReduce operation)
    hive> SELECT t4 AS sev, COUNT(*) AS cnt FROM logs WHERE t4 LIKE '[%' GROUP BY t4;

    [DEBUG] 434
    [ERROR] 3
    [FATAL] 1
    [INFO]  96
    [TRACE] 816
    [WARN]  4



- More Advanced Hive Example

    - A more advanced usage case from the Hive documentation can be developed using the movie
        rating data files obtained from the GroupLens Research webpage.  The files contain various
        numbers of movie reviews, starting at 100,000 and going up to 20 million entries.

      In this example, 100,000 records will be transformed from 'userid', 'movieid', 'rating',
        'unixtime' to 'userid', 'movieid', 'rating', and 'weekday' using Apache Hive and a 
        Python program.


        # Download and extract the data
        $ wget http://files.grouplens.org/datasets/movielens/ml-100k.zip
        $ unzip ml-100k.zip
        $ cd ml-100k


        # weekday_mapper.py
        import sys
        import datetime

        for line in sys.stdin:
            line = line.strip()
            userid, movieid, rating, unixtime = line.split('\t')
            weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
            print '\t'.join([userid, movieid, rating, str(weekday)])LOAD DATA LOCAL INPATH

        './u.data' OVERWRITE INTO TABLE u_data;


        # Next, we start hive and create the data table
        hive> CREATE TABLE u_data (
                userid INT,
                movieid INT,
                rating INT,
                unixtime STRING)
              ROW FORMAT DELIMITED
              FIELDS TERMINATED BY '\t'
              STORED AS TEXTFILE;


        # Load the movie table
        hive> LOAD DATA LOCAL INPATH './u.data' OVERWRITE INTO TABLE u_data;


        # Get the number of rows in the table, this starts a single MapReduce job
        hive> SELECT COUNT(*) FROM u_data;


        # Now that the table data are loaded, make the new table
        hive> CREATE TABLE u_data_new (
                userid INT,
                movieid INT,
                rating INT,
                weekday INT)
              ROW FORMAT DELIMITED
              FIELDS TERMINATED BY '\t';


        # Add the weekday mapper to Hive resources
        hive> add FILE weekday_mapper.py;


        # Now we can perform the transform query
        hive> INSERT OVERWRITE TABLE u_data_new
                SELECT
                  TRANSFORM (userid, movieid, rating, unixtime)
                  USING 'python weekday_mapper.py'
                  AS (userid, movieid, rating, weekday)
                FROM u_data;


        # Sort and group the reviews by weekday
        hive> SELECT weekday, COUNT(*) 
              FROM u_data_new 
              GROUP BY weekday;

        1        13278
        2        14816
        3        15426
        4        13774
        5        17964
        6        12318
        7        12424


        # Remove the tables used in the example
        $ hive -e 'drop table u_data_new'
        $ hive -e 'drop table u_data'



- Apache Sqoop

    - Sqoop is a tool designed to transfer data between Hadoop and relational databases.  You can
        use Sqoop to import data from a RDBMS into HDFS, transform the data in Hadoop, and then
        export the data back into a RDBMS.


    - Sqoop can be used with any JDBC-compliant database, and has been tested with SQL Server, 
        PostgreSQL, MySQL, and Oracle.


    - The data import is done in 2 steps:

        1. Sqoop examines the database to gather the necessary metadata for the data to be imported.

        2. A map-only Hadoop job that Sqoop submits to the cluster.  This job does the actual data
             transfer using the metadata captured in the previous step.  Note that each node doing
             the import must have access to the database.

      The imported data are saved in an HDFS directory.  Sqoop will use the database name for the
        directory, or the user can specify any alternative directory where the files should be
        populated.  By default, these files contain comma-delimited fields, with new lines separating
        different records.  The format in which data are copied can be overrided.


    - Data export is also done in 2 steps:

        1. As with the import, the first step is to examine the database for metadata.  

        2. The export step again uses a map-only Hadoop job to write the data to the database.  Sqoop
             divides the input data set into splits, then uses individual map tasks to push the splits
             into the database.  Again, this process assumes that all map tasks have access to the 
             database.


    - Sqoop v1 uses specialized connectors, each of which is optimized for a specific RDBMS.  
        Connectors are plug-in components based on Sqoop's extension framework.  

      In contrast, Sqoop v2 no longer supports specialized connectors or direct imports/exports into 
        HBase or Hive.  All imports and exports are done through the JDBC interface.  Any new 
        development should be done with Sqoop v2.
