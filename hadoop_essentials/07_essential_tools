------------------------------------------------------------
CHAPTER 7 - ESSENTIAL TOOLS
------------------------------------------------------------

- Apache Pig

    - Pig is a high-level language that enables programmers to write complex MapReduce
        transformations using a simple scripting language.  Pig Latin (the actual language) defines
        a set of transformations on the data such as 'aggregate', 'join', and 'sort'.  Pig is
        often used for ETL in data pipelines, quick research on raw data, and iterative data
        processing.


    - Pig has several usage modes:

        1. Local Mode (with MapReduce)
        2. Local Mode (with Tez)
        3. Interactive Mode
        4. Batch Mode

      This allows Pig applications to be developed locally in interactive modes, using small amounts
        of data, and then run at scale on the cluster in a production mode.



- Pig Walkthrough

    - In this simple example, we'll use Pig to extract user names from the '/etc/passwd' file.  The
        following assumes the user is 'hdfs', but we could use another user name.


      # Copy the 'passwd' file to a working directory for local Pig operation
      $ cp /etc/passwd .


      # Copy the file into HDFS
      $ hdfs dfs -put passwd passwd
      $ hdfs dfs -ls passwd


      # In this example, all processing is done on the local machine (Hadoop is not used)
      $ pig -x local


      # If Pig starts correctly, you'll see a grunt> prompt
      grunt> A = load 'passwd' using PigStorage(':');
      grunt> B = foreach A generate $0 as id;
      grunt> dump B;


      # The processing will start and a list of user names will be printed to the screen
      # To exit the interactive session, use the 'quit' command
      grunt> quit



- Other Pig Options

    - Here, we'll start pig in other modes.

        # Either of these will start Hadoop MapReduce Pig
        $ pig
        $ pig -x mapreduce


        # If you have tez installed, it can be used as an engine
        $ pig -x tez


    - Pig can also be run from a script.

        /* id.pig */
        A = load 'passwd' using PigStorage(':');   -- load the passwd fild
        B = foreach A generate $0 as id;           -- extract the user ids
        dump B;
        store B into 'id.out';                     -- write results to directory id.out


        # Run the script
        $ /bin/rm -r id.out/
        $ pig -x local id.pig

        # If the script worked correctly, you should see a data file with results and a 
        #   zero-length file with the name _SUCCESS


        # Run the script with the MapReduce version
        $ hdfs dfs -rm -r id.out
        $ pig id.pig



- Apache Hive

    - Hive is a data warehouse infrastructure built on top of Hadoop for providing data
        summarization, ad hoc queries, and the analysis of large data sets using the SQL-like
        language HiveQL.  Hive is considered the de facto standard for interactive SQL queries over
        petabytes of data using Hadoop.  It has the following features:

        - Tools to enable easy ETL
        - A mechanism to impose structure on a variety of data formats
        - Access to files stored either directly in HDFS or in other data storage systems like HBase
        - Query optimization via MapReduce and Tez (optimized MapReduce)


    - Hive provides users who are already familiar with SQL the capability to query the data on 
        Hadoop clusters.  At the same time, Hive makes it possible for programmers who are familiar
        with the MapReduce framework to add their own custom mappers and reducers to Hive queries.


    - Hive queries can be dramatically accelerated using the Apache Tez framework.



- Simple Hive Example

    # To start hive, just use the 'hive' command
    $ hive


    # As a simple test, create and drop a table
    hive> CREATE TABLE pokes (foo INT, bar STRING);
    hive> SHOW TABLES;
    hive> DROP TABLE pokes;


    # Create a table to summarize web server logs
    hive> CREATE TABLE logs (t1 string, t2, string, t3 string, t4 string, t5 string, t6 string, 
              t7 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ';


    # Load the 'sample.log' file from the book examples (stored in local directory, not hdfs)
    hive> LOAD DATA LOCAL INPATH 'sample.log' OVERWRITE INTO TABLE logs;


    # Apply the select step to the file (note this invokes a MapReduce operation)
    hive> SELECT t4 AS sev, COUNT(*) AS cnt FROM logs WHERE t4 LIKE '[%' GROUP BY t4;

    [DEBUG] 434
    [ERROR] 3
    [FATAL] 1
    [INFO]  96
    [TRACE] 816
    [WARN]  4



- More Advanced Hive Example

    - A more advanced usage case from the Hive documentation can be developed using the movie
        rating data files obtained from the GroupLens Research webpage.  The files contain various
        numbers of movie reviews, starting at 100,000 and going up to 20 million entries.

      In this example, 100,000 records will be transformed from 'userid', 'movieid', 'rating',
        'unixtime' to 'userid', 'movieid', 'rating', and 'weekday' using Apache Hive and a 
        Python program.


        # Download and extract the data
        $ wget http://files.grouplens.org/datasets/movielens/ml-100k.zip
        $ unzip ml-100k.zip
        $ cd ml-100k


        # weekday_mapper.py
        import sys
        import datetime

        for line in sys.stdin:
            line = line.strip()
            userid, movieid, rating, unixtime = line.split('\t')
            weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
            print '\t'.join([userid, movieid, rating, str(weekday)])LOAD DATA LOCAL INPATH

        './u.data' OVERWRITE INTO TABLE u_data;


        # Next, we start hive and create the data table
        hive> CREATE TABLE u_data (
                userid INT,
                movieid INT,
                rating INT,
                unixtime STRING)
              ROW FORMAT DELIMITED
              FIELDS TERMINATED BY '\t'
              STORED AS TEXTFILE;


        # Load the movie table
        hive> LOAD DATA LOCAL INPATH './u.data' OVERWRITE INTO TABLE u_data;


        # Get the number of rows in the table, this starts a single MapReduce job
        hive> SELECT COUNT(*) FROM u_data;


        # Now that the table data are loaded, make the new table
        hive> CREATE TABLE u_data_new (
                userid INT,
                movieid INT,
                rating INT,
                weekday INT)
              ROW FORMAT DELIMITED
              FIELDS TERMINATED BY '\t';


        # Add the weekday mapper to Hive resources
        hive> add FILE weekday_mapper.py;


        # Now we can perform the transform query
        hive> INSERT OVERWRITE TABLE u_data_new
                SELECT
                  TRANSFORM (userid, movieid, rating, unixtime)
                  USING 'python weekday_mapper.py'
                  AS (userid, movieid, rating, weekday)
                FROM u_data;


        # Sort and group the reviews by weekday
        hive> SELECT weekday, COUNT(*) 
              FROM u_data_new 
              GROUP BY weekday;

        1        13278
        2        14816
        3        15426
        4        13774
        5        17964
        6        12318
        7        12424


        # Remove the tables used in the example
        $ hive -e 'drop table u_data_new'
        $ hive -e 'drop table u_data'