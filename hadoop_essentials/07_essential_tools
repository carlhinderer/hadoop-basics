------------------------------------------------------------
CHAPTER 7 - ESSENTIAL TOOLS
------------------------------------------------------------

- Apache Pig

    - Pig is a high-level language that enables programmers to write complex MapReduce
        transformations using a simple scripting language.  Pig Latin (the actual language) defines
        a set of transformations on the data such as 'aggregate', 'join', and 'sort'.  Pig is
        often used for ETL in data pipelines, quick research on raw data, and iterative data
        processing.


    - Pig has several usage modes:

        1. Local Mode (with MapReduce)
        2. Local Mode (with Tez)
        3. Interactive Mode
        4. Batch Mode

      This allows Pig applications to be developed locally in interactive modes, using small amounts
        of data, and then run at scale on the cluster in a production mode.



- Pig Walkthrough

    - In this simple example, we'll use Pig to extract user names from the '/etc/passwd' file.  The
        following assumes the user is 'hdfs', but we could use another user name.


      # Copy the 'passwd' file to a working directory for local Pig operation
      $ cp /etc/passwd .


      # Copy the file into HDFS
      $ hdfs dfs -put passwd passwd
      $ hdfs dfs -ls passwd


      # In this example, all processing is done on the local machine (Hadoop is not used)
      $ pig -x local


      # If Pig starts correctly, you'll see a grunt> prompt
      grunt> A = load 'passwd' using PigStorage(':');
      grunt> B = foreach A generate $0 as id;
      grunt> dump B;


      # The processing will start and a list of user names will be printed to the screen
      # To exit the interactive session, use the 'quit' command
      grunt> quit



- Other Pig Options

    - Here, we'll start pig in other modes.

        # Either of these will start Hadoop MapReduce Pig
        $ pig
        $ pig -x mapreduce


        # If you have tez installed, it can be used as an engine
        $ pig -x tez


    - Pig can also be run from a script.

        /* id.pig */
        A = load 'passwd' using PigStorage(':');   -- load the passwd fild
        B = foreach A generate $0 as id;           -- extract the user ids
        dump B;
        store B into 'id.out';                     -- write results to directory id.out


        # Run the script
        $ /bin/rm -r id.out/
        $ pig -x local id.pig

        # If the script worked correctly, you should see a data file with results and a 
        #   zero-length file with the name _SUCCESS


        # Run the script with the MapReduce version
        $ hdfs dfs -rm -r id.out
        $ pig id.pig



- Apache Hive

    - Hive is a data warehouse infrastructure built on top of Hadoop for providing data
        summarization, ad hoc queries, and the analysis of large data sets using the SQL-like
        language HiveQL.  Hive is considered the de facto standard for interactive SQL queries over
        petabytes of data using Hadoop.  It has the following features:

        - Tools to enable easy ETL
        - A mechanism to impose structure on a variety of data formats
        - Access to files stored either directly in HDFS or in other data storage systems like HBase
        - Query optimization via MapReduce and Tez (optimized MapReduce)


    - Hive provides users who are already familiar with SQL the capability to query the data on 
        Hadoop clusters.  At the same time, Hive makes it possible for programmers who are familiar
        with the MapReduce framework to add their own custom mappers and reducers to Hive queries.


    - Hive queries can be dramatically accelerated using the Apache Tez framework.



- Simple Hive Example

    # To start hive, just use the 'hive' command
    $ hive


    # As a simple test, create and drop a table
    hive> CREATE TABLE pokes (foo INT, bar STRING);
    hive> SHOW TABLES;
    hive> DROP TABLE pokes;


    # Create a table to summarize web server logs
    hive> CREATE TABLE logs (t1 string, t2, string, t3 string, t4 string, t5 string, t6 string, 
              t7 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ';


    # Load the 'sample.log' file from the book examples (stored in local directory, not hdfs)
    hive> LOAD DATA LOCAL INPATH 'sample.log' OVERWRITE INTO TABLE logs;


    # Apply the select step to the file (note this invokes a MapReduce operation)
    hive> SELECT t4 AS sev, COUNT(*) AS cnt FROM logs WHERE t4 LIKE '[%' GROUP BY t4;

    [DEBUG] 434
    [ERROR] 3
    [FATAL] 1
    [INFO]  96
    [TRACE] 816
    [WARN]  4