------------------------------------------------------------
CHAPTER 2 - INSTALLATION RECIPES
------------------------------------------------------------

- HDFS Services

    - HDFS has 2 major components:

        1. The 'NameNode', which is the process that manages the entire file system

        2. The 'DataNodes', which are the processes that manage the actual data


    - In a typical Hadoop cluster, all the worker machines in the cluster (often referred to as
        nodes) are running the 'DataNode' service that reports the the central NameNode.


    - The 'NameNode' is often run on a separate machine from other Hadoop processes.  It can be
        run in federated and/or failover mode as well.


    - There is also a process called the 'SecondaryNameNode'.  This process is not a backup
        NameNode, and is best described as a CheckPointNode.  The SecondaryNameNode periodically
        fetches the in-memory HDFS edits from the NameNode and allows the NameNode to work 
        quickly without having to comming file system changes directly to disk.



- YARN Services

    - YARN has 2 main services that are required to run a program on a Hadoop cluster:

        1. 'ResourceManager', which is a single master scheduler for all cluster jobs.  The 
             ResourceManager works by communicating with the 'NodeManager' service running on
             worker nodes.

        2. The 'NodeManager' manages all the actual work done on the cluster nodes. There needs to 
             be at least one NodeManager running for Hadoop jobs to run. 


    - Both the ResourceManager and NodeManagers are job neutral.  They have no knowledge or interest
        in what the actual user job is doing.


    - In addition, some history servers can be run as a part of YARN.  These services are not
        essential for jobs to run, but they make job tracking much easier.  The 'JobHistoryServer'
        is used for MapReduce job history collection.  The 'ApplicationHistoryServer' is a more
        general history server used for non-MapReduce jobs.



- Hadoop Configuration Files

    All core Hadoop services use XML files for storing parameters.  The files have the following 
      format:

      <property>
          <name>dfs.replication</name>
          <value>1</value>
      </property>



- Steps to Install Hadoop from Apache Sources on a Single Node

    1. Download Hadoop

    2. Set JAVA_HOME and HADOOP_HOME

    3. Create users and groups

    4. Make data and log directories

    5. Configure 'core-site.xml'

    6. Configure 'hdfs-site.xml'

    7. Configure 'mapred-site.xml'

    8. Configure 'yarn-site.xml'

    9. Modify Java heap sizes

    10. Format HDFS

    11. Start HDFS services

          # Check which services are running
          jps

    12. Start YARN services

    13. Verify the running services using the web interface

          # Check HDFS console
          http://localhost:50070

          # Check Resource Manager console
          http://localhost:8088

          # Log into Ambari server
          http://localhost:8080

    (Optional) Install Pig and Hive



- Installing Hadoop in the Cloud with Apache Whirr

    Apache Whirr is a set of libraries for running cloud services.  Whirr provides the following
      benefits:

      1. Cloud-neutral way to run services

      2. Common service API

      3. Smart defaults for services