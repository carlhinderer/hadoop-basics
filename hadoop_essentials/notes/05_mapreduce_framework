------------------------------------------------------------
CHAPTER 5 - HADOOP MAPREDUCE FRAMEWORK
------------------------------------------------------------

- The MapReduce Model

    - Apache Hadoop is often associated with MapReduce computing.  Prior to Hadoop v2, it was the
        main use case.  Virtually all tools developed for Hadoop, such as Pig and Hive, will work
        seamlessly on top of Hadoop v2 MapReduce.


    - The MapReduce computation model is very simple.  It has a 'mapping' step, followed by a 
        'reducing' step.



- MapReduce Example # 1

    Let's say we want to count all of the times the word 'Kutuzov' appears in 'War and Peace'.  We
      can accomplish this with a single bash statement:


      grep " Kutuzov " war-and-peace.txt | wc -l

      in which:
        grep " Kutuzov " war-and-peace.txt       # is the map step
        wc - l                                   # is the reduce step



- MapReduce Example # 2

    [mapper.sh script]
    #!/bin/bash
    while read line ; do
      for token in $line; do
        if [ "$token" = "Kutuzov" ] ; then
          echo "Kutuzov,1"
        elif [ "$token" = "Petersburg" ] ; then
          echo "Petersburg,1"
        fi
      done
    done


    [reducer.sh script]
    #!/bin/bash
    kcount=0
    pcount=0
    while read line ; do
      if [ "$line" = "Kutuzov,1" ] ; then
       let kcount=kcount+1
      elif [ "$line" = "Petersburg,1" ] ; then
       let pcount=pcount+1
      fi
    done
    echo "Kutuzov,$kcount"
    echo "Petersburg,$pcount"


    [Using the scripts]
    $ cat war-and-peace.txt | ./mapper.sh | ./reducer.sh

    Kutuzov,315
    Petersburg,128



- The Functional Nature of MapReduce

    - The MapReduce model is inspired by the map and reduce functions commonly used in many
        functional programming languages.  The functional nature of MapReduce has some important
        properties:

        1. Data flow is in one direction (map to reduce).  It is possible to use the output of a
             reduce step as the input to another MapReduce process.

        2. As with functional programming, the input data are not changed.  By applying the mapping
             and reduction functions to the input data, new data are produced.  The original state
             of the data lake is always preserved.

        3. Because there is no dependency on how the mapping and reducing functions are applied to
             the data, the mapper and reducer data flow can be implemented in any number of ways
             to provide better performance.


    - Distributed (parallel) implementations of MapReduce enable large amounts of data to be analyzed
        quickly.  In general, the mapper process is fully scalable and can be applied to any subset
        of the input data.  Results from multiple parallel mapping functions are then combined in
        the reducer phase.


    - Hadoop accomplishes this parallelism by using HDFS to slice and spread data over multiple
        servers.  Hadoop MapReduce will try to move the mapping tasks to the server that contains
        the data slice.  Results from each data slice are then combined in the reducer step.


    - HDFS is not required for Hadoop MapReduce, however.  A sufficiently fast parallel file system
        can be used in its place.  In these designs, each server in the cluster has access to a 
        high-performance parallel file system that can rapidly provide any data slice.  These designs
        are typically more expensive than the commodity servers used for many Hadoop clusters.



- MapReduce Parallel Data Flow

    - Operationally, the Hadoop parallel MapReduce data flow can be quite complex.  Parallel execution
        of MapReduce requires other steps in addition to the mapper and reducer processes.  The basic
        steps are as follows:


        1. Input Splits

             HDFS distributes and replicates data over multiple servers.  These data slices are 
               physical boundaries determined by HDFS and have nothing to do with the data in the
               files.

             The input splits used by MapReduce are logical boundaries based on the input data.  For
               example, the split size can be based on the number of records in a file or the actual
               size in bytes.  Splits are almost always smaller than the HDFS block size.  The number
               of splits corresponds to the number of mapping processes used in the 'map' stage.


        2. Map Step

             The mapping process is where the parallel nature of Hadoop comes into play.  For large
               amounts of data, many mappers can be operating at the same time.  The user provides
               the specific mapping process.  MapReduce will try to execute the mapper on the 
               machines where the block resides.  

             Because the file is replicated in HDFS, the least busy node with the data will be chosen. 
               If all nodes holding the data are too busy, MapReduce will try to pick a node that is 
               closest to the node that hosts the data block (a characteristic called rack-awareness). 
               The last choice is any node in the cluster that has access to HDFS.


        3. Combiner Step

             It is possible to provide an optimization or pre-reduction as part of the map stage 
               where key–value pairs are combined prior to the next stage. The combiner stage is 
               optional.


        4. Shuffle Step

             Before the parallel reduction stage can complete, all similar keys must be combined and 
               counted by the same reducer process. Therefore, results of the map stage must be 
               collected by key–value pairs and shuffled to the same reducer process. If only a single
               reducer process is used, the shuffle stage is not needed.


        5. Reduce Step

             The final step is the actual reduction. In this stage, the data reduction is performed as 
               per the programmer’s design. The reduce step is also optional. The results are written 
               to HDFS. Each reducer will write an output file. For example, a MapReduce job running 
               four reducers will create files called part-0000, part-0001, part-0002, and part-0003.



- MapReduce Data Flow Example

    1. Input Data

         see spot run
         run spot run
         see the cat


    2. Split 

         1.  see spot run
         2.  run spot run
         3.  see the cat


    3. Map

         1.  see, 1
             spot, 1
             run, 1

         2.  run, 1
             spot, 1
             run, 1

         3.  see, 1
             the, 1
             cat, 1


    4. Shuffle

         1.  see, 1
             see, 1

         2.  spot, 1
             spot, 1

         3.  run, 1
             run, 1
             run, 1

         4.  the, 1

         5.  cat, 1


    5. Reduce

         1.  see, 2

         2.  spot, 2

         3.  run, 3

         4.  the, 1

         5.  cat, 1


    6. Output

         see, 2
         spot, 2
         run, 3
         the, 1
         cat, 1



- Using a Separate Combiner Step as an Optimization

    3A. Map

          run, 1
          spot, 1
          run, 1


    3B. Combiner

          run, 2
          spot, 1



- Fault Tolerance

    - One of the most interesting aspects of parallel MapReduce operation is the strict control
        of data flow throughout the execution of the program.  Mapper processes do not exchange 
        data with other mapper processes.  Data can only go from mappers to reducers, not the other
        direction.  The confined data flow enables MapReduce to operate in a fault-tolerant fashion.


    - MapReduce can easily recover from the failure of one or many map processes.  If a server fails,
        the map tasks that were running on that machine could easily be restarted on another
        working server because there is no dependence on any other map task.  


    - In a similar fashion, failed reducers can be restarted.  However, there may be additional work
        that has to be redone in such a case.  A completed reduce tasks writes results to HDFS.  If
        a node fails after this point, the MapReduce ApplicationMaster will need to restart the
        reducer tasks.  If the mapper output is not available for the newly restarted reducer, then
        these map tasks will need to be restarted.



- Speculative Execution

    - One of the many challenges with large clusters is the inability to predict or manage
        unexpected system bottlenecks or failures.  In theory, it is possible to control and
        monitor resources so that network traffic and processor load can be evenly balanced.
        In practice, this problem represents a difficult challenge for large systems.  Thus,
        it is possible that a congested network, failing disk, high processor load, or some
        other problem might lead to slow performance.


    - When one part of a MapReduce process runs slowly, it ultimately slows down everything,
        since the application cannot complete until all processes are finished.  The nature of
        the parallel MapReduce model provides an interesting solution to this problem.  Since all
        input data are immutable, it is possible to start a copy of a running map process without
        disturbing any other running mapper processes.


    - Suppose that as most of the map tasks are coming to a close, the ApplicationMaster notices
        that some are still running and schedules redundant copies of the remaining nodes on less
        busy or free servers.  Should the secondary processes finish first, the other processes
        are then terminated (or vice versa).  This process is known as 'speculative execution'.
        The same approach can be applied to reducer processes that are taking a long time.


    - Speculative execution can be enabled or disabled in the 'mapred-site.xml' config file.