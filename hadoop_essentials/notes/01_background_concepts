------------------------------------------------------------
CHAPTER 1 - BACKGROUND AND CONCEPTS
------------------------------------------------------------

- Features of Hadoop Data Processing

    1. Core parts are open source under Apache licensing

    2. Analysis usually involves large, unstructured data sometimes in the petabyte range

    3. Traditionally, data is stored across multiple servers using HDFS, but other storage
         systems are now used

    4. MapReduce jobs can scale from a single server to thousands

    5. Other programming models are supported in Hadoop v2 with YARN

    6. Hadoop core components were designed to run on commodity hardware and the cloud

    7. Hadoop offers many fault-tolerant features that enable operation over large number
         of servers

    8. Many projects and applications are built on top of the Hadoop infrastructure

    9. Although the core components are written in Java, Hadoop applications can use almost
         any programming language



- Defining Big Data

    - Characteristics of Big Data

        1. Volume = sheer size of the data

        2. Variety = comes from a variety of sources and is not necessarily related to other 
                       data sources

        3. Velocity = speed at which data can be generated and processed

        4. Variability = data may be highly variable, incomplete, and inconsistent

        5. Complexity = relationships between data may be complex and non-relational


    - "5 V's of Big Data"

        1. Volume
        2. Variety
        3. Velocity
        4. Value = how to monetize the data
        5. Veracity = varying quality of the data


    - Size of Big Data

        - Sweet spot starts at ~100 GB
        - Most common amount of data for the average company is 10-30 TB
        - 90% of jobs on Facebook cluster are < 100 GB


    - Types of Data

        - Media, including video, audio, and photos
        - Web data like logs, click trails, and emails
        - Documents, periodicals, books
        - Scientific research data (ie simulations or human genome data)
        - Stock transactions, customer data, retail purchases
        - Telecommunications data like phone records
        - Public records
        - IOT data
        - Real-time sensor data



- Hadoop as a Data Lake

    - Data Lake 

        - Schema-on-read
        - Vast repository for raw data, use it as needed
        - All data remains available, no need to make assumptions about future use
        - Add data is sharable, not compartmentalized
        - All access methods are available (ie MapReduce, graph processing, etc)


    - Data Warehouse

        - Schema-on-write with ETL and predetermined schema
        - Valuable business tool, will not be replaced by Hadoop



- Hadoop v1 vs v2

    - Hadoop v1

        - Only meant for MapReduce jobs
        - Task scheduler and MapReduce engine were a single component
        - Only Java could be used
        - Maximum cluster size was 4000 nodes
        - Job tracker failure killed all running and queued jobs
        - Iterative applications were 10x slower


    - Hadoop v2

        - Scheduling and resource management are separate from jobs running
        - MapReduce is a separate application that runs on YARN
        - YARN presents a generalized interface for any application framework



- MapReduce Example

    We load 'War and Peace' and we want to find out how many times the word 'Kutuzov' appears.

      [Load Step]
      Load file into HDFS, it is sliced and added to nodes

      [Map Step]
      User query is 'mapped' to all slices.
      The number of times 'Kutuzov' appears on each node is tallied.

      [Reduce Step]
      Results are 'reduced' to all slices.
      The counts from each node are summed into the output value.



- MapReduce Advantages

    - Uses a 'functional approach' because the original input data does not change.  The steps
        only create new data.

    - Highly scalable (often linear scalability)

    - Easily managed work flow

    - Fault tolerance, a failed process can be restarted on other nodes, since inputs are 
        immutable

    - MapReduce (aka 'Data Parallel Problem' or 'SIMD') is a powerful paradigm for solving many
        problems.



- Hadoop v2 YARN Operation Design

    - YARN schedules and manages all jobs on the cluster

    - Worker nodes are managed by a NodeManager process that works with the Resource Manager

    - Job resources are portioned out in containers, which are computing resources usually 
        defined as one processing core and an amount of memory

    - The Resource Manager and NodeManager have no information about the jobs.  They manage the
        conatiners running on the cluster and are task neutral.


    - Typical Hadoop v2 MapReduce job:

        1. Clients submit jobs to Resource Manager

        2. Resource Manager selects a node and instructs the Node Manager to start an Application 
            Master (App Mstr)

        3. The Application Master (running in a container) requests additional containers 
             (resources) from Resource Manager

        4. The assigned containers are started and managed on the appropriate nodes by the Node
             Managers

        5. Once the Application Master and containers are connected and running, the Resource
             Manager and the Node Managers step away from the job

        6. All job progress is reported back to the Application Master

        7. When a task that runs in a container is completed, the Node Manager makes the container
             available to Resource Manager


    - Advantages of separating user jobs from the scheduler

        - Better scale
        - New programming models and services (ie Giraph, Spark, MPI)
        - Improved cluster utilization
        - Application agility
        - Move beyond Java



- The Hadoop Ecosystem

    - Core Components

        - HDFS
            - redundant and highly reliable distributed filesystem

        - YARN
            - resource manager
            - provides all scheduling and resource management

        - MapReduce 
            - YARN application framework that provides MapReduce functionality for the cluster
            - is the basis for many higher-level tools


    - Hadoop Database

        - HCatalog
            - table and storage management service for data created using Hadoop
            - the table abstraction removes the need for the user to know where data is stored

        - HBase
            - is the Hadoop database
            - distributed, scalable, column-oriented database similar to Google BigTable
            - designed for hosting very large tables with billions of rows and millions of columns


    - MapReduce Query Tools

        - Pig
            - high-level language that enables programmers to write complex MapReduce transformations
            - Pig Latin defines a set of transformations on a dataset, like 'aggregate', 'join', 'sort'
            - often used for ETL data pipelines, quick research on raw data, iterative data processing
            - improved programming productivity over Java MapReduce jobs

        - Hive
            - data warehouse infrastructure built on top of Hadoop
            - provides data summary, ad hoc queries, and analysis of large data sets using HiveQL
            - transparently translates queries into MapReduce jobs that are executed in HBase
            - is the de facto standard for interactive Sql queries over petabytes of data


    - Data Import Export

        - Sqoop
            - tool designed for efficiently transferring bulk data between HDFS and relational DBs

        - Flume
            - distributed, reliable service for efficiently collecting, aggregating, and moving
                large amounts of dynamic serial data (ie log data)

        - Avro
            - Serialization format for exchanging data between programs written in any language
            - often used to connect Flume data flows


    - Workflow Automation

        - Oozie
            - workflow/coordination system to manage multistage Hadoop jobs
            - enables workflow decisions based on job dependencies
            - best for designing job execution graphs

        - Falcon
            - enables automation of data movement and processing for ingest, pipeline, replication
            - can trigger a job start when data changes or becomes available


    - Administration

        - Ambari
            - web-based tool for provisioning, managing, monitoring Hadoop clusters


    - YARN Application Frameworks

        - Giraph
            - graph processing

        - Spark
            - in-memory processing

        - Storm
            - stream processing


    - Other

        - ZooKeeper
            - centralized service used by applications
            - used for maintaining configuration, health, and other status elements on nodes
            - maintains common objects in cluster environments (config information, etc)
            - applications can use these services to coordinate distributed processing
            - provides application reliability
            - if an App Mstr dies, ZooKeeper spawns a new App Mstr to resume the tasks

        - Mahout
            - scalable ML library