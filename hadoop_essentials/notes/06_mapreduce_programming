------------------------------------------------------------
CHAPTER 6 - MAPREDUCE PROGRAMMING
------------------------------------------------------------

- The Hadoop WordCount Example

    - The 'WordCount.java' program is the Hadoop MapReduce equivalent of 'Hello World'.  WordCount
        is a simple application that counts the number of occurrences of each word in a given input
        set.


    - The 'map' method processes one line at a time as provided by the specified 'TextInputFormat' 
        class.  It then splits the line into tokens separated by whitespaces using the 
        'StringTokenizer' and emits a key-value pair of <word, 1>.

      Given 2 input files with contents 'Hello World Bye World' and 'Hello Hadoop Goodbye Hadoop', the
        WordCount mapper will produce 2 maps:

          < Hello, 1>
          < World, 1>
          < Bye, 1>
          < World, 1>

          < Hello, 1>
          < Hadoop, 1>
          < Goodbye, 1>
          < Hadoop, 1>


    - WordCount sets a mapper, a combiner, and a reducer:

        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
      
      Here, the output of each map is passed through the local combiner for local aggregation and then
        sends the data onto the final reducer.  Thus, each map above the combiner performs the 
        following pre-reductions:

        < Bye, 1>
        < Hello, 1>
        < World, 2>

        < Goodbye, 1>
        < Hadoop, 2>
        < Hello, 1>


    - The 'reduce' method simply sums the values, which are the occurrence counts for each key.  
        The final output of the reducer is:

        < Bye, 1>
        < Goodbye, 1>
        < Hadoop, 2>
        < Hello, 2>
        < World, 2>



- Compiling and Running the Hadoop WordCount Example

    1. Make a local 'wordcount_classes' directory

         $ mkdir wordcount_classes


    2. Compile the 'WordCount.java' program using the 'hadoop classpath' command to include
         all the available Hadoop class paths.

         $ javac -cp `hadoop classpath` -d wordcount_classes WordCount.java


    3. The jar file can be created using the following command:

         $ jar -cvf wordcount.jar -C wordcount_classes/ .


    4. To run the example, create an input directory in HDFS and place a text file in the new
         directory.  For this example, we will use the 'war-and-peace.txt' file.

         $ hdfs dfs -mkdir war-and-peace-input
         $ hdfs dfs -put war-and-peace.txt war-and-peace-input


    5. Run the WordCount application.

         $ hadoop jar wordcount.jar WordCount war-and-peace-input war-and-peace-output


    6. If the job ran correctly, there should be 'war-and-peace-output' directory.  The following
         files should be in the directory:

         $ hdfs dfs -ls war-and-peace-output

           Found 2 items
           -rw-r--r--   2 hdfs hdfs          0 2015-05-24 11:14 war-and-peace-output/_SUCCESS
           -rw-r--r--   2 hdfs hdfs     467839 2015-05-24 11:14 war-and-peace-output/part-r-00000


    7. The complete list of word counts can be copied from HDFS to the working directory:

         $ hdfs dfs -get war-and-peace-output/part-r-00000


    8. If the WordCount program is run again using the same outputs, it will fail when it tries
         to overwrite the 'war-and-peace-output' directory.  The output directory and all of its
         contents can be removed:

         $ hdfs dfs -rm -r -skipTrash war-and-peace-output



- Using the Streaming Interface

    - The Apache Hadoop streaming interface enables almost any program to use the MapReduce engine.
        The 'streams' interface will work with any program that can read and write to 'stdin' and
        'stdout'.  


    - When working in the Hadoop streaming mode, only the mapper and the reducer are created by the
        user.  This approach does have the advantage that the mapper and the reducer can be easily
        tested from the command line.  In this example, a Python mapper and reducer, 'mapper.py'
        and 'reducer.py' will be used.


    - To observe the operation of the 'mapper.py' script:

        $ echo "foo foo quux labs foo bar quux" | ./mapper.py

        Foo     1
        Foo     1
        Quux    1
        Labs    1
        Foo     1
        Bar     1
        Quux    1


    - Piping the results of the map into the 'sort' command can create a simulated shuffle phase:

        $ echo "foo foo quux labs foo bar quux" | ./mapper.py | sort -k1,1

        Bar     1
        Foo     1
        Foo     1
        Foo     1
        Labs    1
        Quux    1
        Quux    1


    - Finally, the full MapReduce process can be simulated by adding the 'reducer.py' script to the
        following command pipeline:

        $ echo "foo foo quux labs foo bar quux" | ./mapper.py | sort -k1,1 | ./reducer.py

        Bar     1
        Foo     3
        Labs    1
        Quux    2


    - To run this application using a Hadoop installation, 

        # Create the directory for 'War and Peace' input and remove the previous output

        $ hdfs dfs -mkdir war-and-peace-input
        $ hdfs dfs -put war-and-peace.txt war-and-peace-input
        $ hdfs dfs -rm -r -skipTrash war-and-peace-output


        # Locate the 'hadoop-streaming.jar' file in your distribution and call it

        $ hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \
            -file ./mapper.py \
            -mapper ./mapper.py \
            -file ./reducer.py -reducer ./reducer.py \
            -input war-and-peace-input/war-and-peace.txt \
            -output war-and-peace-output


        # The output should be the same as with WordCount.java


    - Note that the Python scripts used in this example could be Bash, Perl, Tcl, Awk, compiled
        C code, or any language that can read from 'stdin' and 'stdout'.


    - Although the streaming interface is rather simple, it does have some disadvantages over using
        Java directly.  In particular, not all applications are string and character based, and it
        would be awkward to try and use 'stdin' and 'stdout' as a way to transmit binary data.
        Another disadvantage is that many tuning parameters available through the full Java
        Hadoop API are not available in streaming.



- Using the Pipes Interface

    - 'Pipes' is a library that allows C++ source code to be used for mapper and reducer code.  
        Applications that require high performance when crunching numbers may actually achieve
        better throughput if written with C++ and the 'Pipes' interface.


    - Both key and value inputs to pipes programs are provided as STL strings (std::string).
        The program must define an instance of a mapper and an instance of a reducer.  A program
        to use with 'Pipes' is defined by writing classes extending 'Mapper' and 'Reducer'.  Hadoop
        must then be informed as to which classes to use to run the job.


    - The 'Pipes' framework on each machine assigned to your job will start an instance of your C++
         program.  Therefore, the executable must be placed in HDFS prior to use.


    - To run 'wordcount.cpp':

         1. The location of Hadoop 'include' files and libraries may need to be specified when
              compiling the code.  If $HADOOP_HOME is defined, the following options should provide
              the correct path.  


         2. To compile the program:

              $ g++ wordcount.cpp -o wordcount -L$HADOOP_HOME/lib/native/  \  
                   -I$HADOOP_HOME/../usr/include -lhadooppipes -lhadooputils -lpthread -lcrypto


         3. If needed, create the 'war-and-peace-input' directory and move the file into HDFS:

              $ hdfs dfs -mkdir war-and-peace-input
              $ hdfs dfs -put war-and-peace.txt war-and-peace-input
              $ hdfs dfs -rm -r -skipTrash war-and-peace-output


         4. The executable must be placed into HDFS so YARN can find the program:

              $ hdfs dfs -put wordcount bin


         5. Run the executable:

              $ mapred pipes \
                   -D hadoop.pipes.java.recordreader=true  \
                   -D hadoop.pipes.java.recordwriter=true \
                   -input war-and-peace-input   \
                   -output war-and-peace-output  \
                   -program bin/wordcount


         6. The program should produce the same output in the 'war-and-peace-output' directory.



- Hadoop Grep Chaining Example

    - The Hadoop 'Grep.java' example extracts matching strings from text files and counts how many
        times they have occurred.


    - The program runs 2 map/reduce jobs in sequence and is an example of 'MapReduce chaining'.
        The first job counts how many times a matching string occurs in the input, and the second
        job sorts matching strings by their frequency and stores the output in a single file.


    - In the example, each mapper of the first job takes a line as input and matches the 
        user-provided regular expression against the line.  The 'RegexMapper' class is used to
        perform this task and extracts task matching using the given regular expression.

      The matching strings are output as < matching_string, 1> pairs.  As in the previous WordCount
        example, each reducer sums up the number of matching strings and employs a combiner to
        do local sums.  The actual reducer uses the 'LongSumReducer' class that outputs the sum of
        long values per reducer input key.


    - The second job takes the output of the first job as input.  The mapper is an inverse map that
        reverses its input <key, value> pairs into <value, key> pairs.  There is not reduction step, 
        so the 'IdentityReducer' class is used by default.  All input is simply passed to the
        output.  The number of reducers is set to 1, so the output is stored in one file and is 
        sorted by count in descending order.  The output text file contains a count and a string
        per line.


    - The example also demonstrates how to pass a command-line parameter to a mapper or reducer.



- Compiling and Running the Grep.java example

    1. Create a directory for the application classes:

         $ mkdir Grep_classes


    2. Compile the WordCount.java program using the following line:

         $ javac -cp `hadoop classpath` -d Grep_classes Grep.java


    3. Create a java archive:

         $ jar -cvf Grep.jar -C Grep_classes/ .


    4. If needed, create a directory and add the 'war-and-peace.txt' file.

         $ hdfs dfs -mkdir war-and-peace-input
         $ hdfs dfs -put war-and-peace.txt war-and-peace-input
         $ hdfs dfs -rm -r -skipTrash war-and-peace-output


    5. Run the Grep program:

         $ hadoop jar Grep.jar org.apache.hadoop.examples.Grep war-and-peace-input \
               war-and-peace-output Kutuzov


    6. To see, the output, examine the output file:

         $ hdfs dfs -cat war-and-peace-output/part-r-00000

         530    Kutuzov



- Debugging MapReduce

    - The best advice for debugging parallel MapReduce applications is this: Don't!!  Debugging
        on a distributed system is hard and should be avoided at all costs.  The best approach
        is to make sure applications run on a simpler system (ie the HDP Sandbox or the
        pseudo-distributed single machine install) with smaller data sets.  Errors on these
        systems are much easier to locate and track.


    - In addition, unit testing applications before running at scale is important.  If 
        applications can run successfully on a single system with a subset of real data, then
        running in parallel should be a simple task since the MapReduce algorithm is 
        transparently scalable.  Note that many higher-level tools (ie Pig and Hive) enable local
        mode development for this reason.


    - When investigating program behavior at scale, the best approach is to use the application
        logs to inspect the actual MapReduce progress.



- Hadoop Log Management

    - The MapReduce logs provide a comprehensive listing of both mappers and reducers.  The actual
        log output consists of 3 files - 'stdout', 'stderr', and 'syslog' (Hadoop system messages)
        for the application.


    - There are 2 modes for log storage:

        1. The first (and best) method is to use log aggregation.  In this mode, logs are 
             aggregated in HDFS and can be displayed in the YARN ResourceManager interface
             or examined with the 'yarn logs' command.  Log aggregation is highly recommended.

        2. If log aggregation is not enabled, the logs will be placed locally on the cluster nodes
             on which the mapper or reducer ran.  The location of the unaggregated local logs is
             given by the 'yarn.nodemanager.log-dirs' property in the 'yarn-site.xml' file.


    - The most convenient way to view logs is to use the YARN ResourceManager web user interface.
        If log aggregation is enabled, the contents of 'stdout', 'stderr', and 'syslog' will be 
        displayed on a single page.  If log aggregation is not enabled, a message stating that the 
        logs are not available will be shown.


    - If log aggregation is enabled, the 'yarn logs' command can also be views to view MapReduce
        logs from the command line.