------------------------------------------------------------
CHAPTER 10 - BASIC HADOOP ADMINISTRATION PROCEDURES
------------------------------------------------------------

- Basic Configuration Files

    - 'core-default.xml' = system-wide properties

    - 'hdfs-default.xml' = HDFS properties

    - 'mapred-default.xml' = MapReduce properties

    - 'yarn-default.xml' = YARN properties



- Decomissioning YARN Nodes

    - If a NodeManager host/node needs to be removed from the cluster, it should be decomissioned
        first.  Assuming the node is responding, it can be decomissioned from the Ambari web UI.

    - To decomission, go to Hosts view, click on host, and select 'Decomission' from the pull-down
        menu.  If the node is also an HDFS DataNode, it needs to be decomissioned from HDFS in a 
        similar fashion.



- YARN WebProxy

    - The Web Application Proxy is a separate proxy server in YARN that addresses security issues
        with the cluster web interface on ApplicationMasters.  By default, the proxy runs as part of
        the ResourceManager itself, but it can be configured to run in a stand-alone mode.



- Using the JobHistory Server

    - The removal of the JobTracker and migration of MapReduce from a system to an application-level
        framework necessitated creation of a place to store MapReduce job history.  The JobHistoryServer
        provides all YARN MapReduce applications with a central location in which to aggregate 
        completed jobs for historical reference and debugging.

    - The settings for the JobHistoryServer can be found in the 'mapred-site.xml' file.



- Managing YARN Jobs

    - YARN jobs can be managed using the 'yarn application' command.  Options including '-kill', '-list',
        and '-status' are available to the administrator with this command.  MapReduce jobs can also be
        controlled with the 'mapred job' command.

    - Neither the YARN ResourceManager UI nor Ambari can be used to kill YARN applications.  If a job
        needs to be killed, give the 'yarn application' command to find the Application Id and use the
        '-kill' argument.



- Setting Container Memory

    - YARN manages application resource containers over the entire cluster.  Controlling the amount of 
        container takes place in he yarn-site.xml file:

        1. 'yarn.nodemanager.resource.memory-mb' is the amount of memory the NodeManager can use for
             containers.

        2. 'scheduler.minimum-allocation-mb' is the smallest container allowed by the ResourceManager. 
             A requested container smaller than this value will result in an allocated container of this 
             size (default 1024MB).

        3. 'yarn.scheduler.maximum-allocation-mb' is the largest container allowed by the 
             ResourceManager (default 8192MB).



- Setting Container Cores

    - You can set the number of cores for containers using the following properties in the 
        'yarn-site.xml':

        1. 'yarn.scheduler.minimum-allocation-vcores': The minimum allocation for every container 
             request at the ResourceManager, in terms of virtual CPU cores. Requests smaller than this
             allocation will not take effect, and the specified value will be allocated the minimum 
             number of cores. The default is 1 core.

        2. 'yarn.scheduler.maximum-allocation-vcores': The maximum allocation for every container request 
             at the ResourceManager, in terms of virtual CPU cores. Requests larger than this allocation 
             will not take effect, and the number of cores will be capped at this value. The default is 32.

        3. 'yarn.nodemanager.resource.cpu-vcores': The number of CPU cores that can be allocated for
             containers. The default is 8.



- Setting MapReduce Properties

    - As noted throughout this book, MapReduce now runs as a YARN application. Consequently, it may be
        necessary to adjust some of the 'mapred-site.xml' properties as they relate to the map and 
        reduce containers. The following properties are used to set some Java arguments and memory size 
        for both the map and reduce containers:

        1. 'mapred.child.java.opts' provides a larger or smaller heap size for child JVMs of maps 
             (e.g., --Xmx2048m).

        2. 'mapreduce.map.memory.mb' provides a larger or smaller resource limit for maps 
             (default = 1536MB).

        3. 'mapreduce.reduce.memory.mb' provides a larger heap size for child JVMs of maps 
             (default = 3072MB).

        4. 'mapreduce.reduce.java.opts' provides a larger or smaller heap size for child reducers.



- The NameNode User Interface

    - Monitoring HDFS can be done in several ways.  One of the more convenient ways to get a quick view
        of HDFS status is through the NameNode user interface.  This web-based tool provides essential
        information about HDFS and offers the capability to browse the HDFS namespace and logs.

    - The web-based UI can be started from within Ambari or from a web browser connected to the 
        NameNode.  In Ambari, simply select the HDFS service window and click on the Quick Links 
        pull-down.  To navigate to the UI directly, use

          http://localhost:50070


    - There are 5 tabs in the UI:

        1. Overview
        2. Datanodes
        3. Snapshot
        4. Startup Progress
        5. Utilities



- Adding Users to HDFS

    # Add a user to the group for your operating system on the HDFS client system
    $ useradd -G hdfs user1

    # Create the username directory in hdfs
    $ hdfs dfs -mkdir /user/user1

    # Give that account ownership over its directory in HDFS
    $ hdfs dfs -chown user1:hdfs /user/user1



- Perform an FSCK on HDFS

    To check the health of HDFS, you can issue the 'hdfs fsck <path>' file system check command.
      This example checks the entire HDFS namespace:

      # Check the entire hdfs
      $ hdfs fsck /



- Balancing HDFS

    - Based on usage patterns and DataNode availability, the number of data blocks across the DataNodes 
        may become unbalanced.  To avoid over-utilized DataNodes, the HDFS balancer tool rebalances
        data blocks across the available DataNodes.  Data blocks are moved from over-utilized to 
        under-utilized nodes to within a certain percent threshold.

    - Rebalancing can be done when new DataNodes are added or when a DataNode is removed from service.
        The HDFS superuser must run the balancer.

        # Run the balancer
        $ hdfs balancer


    - By default, the balancer will continue to rebalance the nodes until the number of data blocks on 
        all DataNodes are within 10% of each other.  To set a different percent threshold:

        # Use a 5% threshold
        $ hdfs balancer -threshold 5


    - To ensure the balancer does not swamp the cluster networks, you can set a bandwidth limit before
        running the balancer:

        # Set the bandwidth limit in bytes per second
        $ dfsadmin -setBalancerBandwidth <new_bandwidth>

        # Now, run the balancer
        $ hdfs balancer



- HDFS Safe Mode

    - When the NameNode starts, it loads the file system state from the 'fsimage' and then applies the
        'edits' log file.  It then waits for the DataNodes to report their blocks.  During this time,
        the NameNode stays in a read-only Safe Mode.  The NameNode leaves Safe Mode automatically
        after the DataNodes have reported that most file system blocks are available.


    - The administrator can place HDFS in Safe Mode or take it out of Safe mode manually:

        # Enter Safe Mode
        $ hdfs dfsadmin -safemode enter

        # Exit Safe Mode
        $ hdfs dfsadmin -safemode leave

        # Check whether the system is in Safe Mode
        $ hdfs dfsadmin -safemode get



- Decomissioning HDFS Nodes

    - If you need to remove a DataNode host/node from the cluster, you should decomission it first.
        Assuming the node is responding, it can easily be decomissioned from the Ambari web UI.  



- SecondaryNameNode

    - To avoid long NameNode restarts and other issues, the performance of the SecondaryNameNode 
        should be verified.  To make sure it's working correctly, we can set the value of 
        'fs.checkpoint.period' in the 'hdfs-site.xml' file to be a short amount of time and 
        validate that the merge is happening.  Then, we can set it back to its original value.


    - Alternatively, we can force a checkpoint with:

        $ hdfs secondarynamenode -checkpoint force



- HDFS Snapshots

    - HDFS snapshots are read-only, point-in-time copies of HDFS.  Snapshots can be taken on a subtree
        of the file system or the entire file system.  Some common use cases for snapshots are data
        backup, protection against user errors, and disaster recovery.


    - Snapshots can be taken on any directory once the directory has been set as 'snapshottable'.  
        A snapshottable directory is able to accomodate 65,536 simultaneous snapshots.  


    - The following procedure will create a snapshot:

        # Declare a directory snapshottable
        $ hdfs dfsadmin -allowSnapshot  /user/hdfs/war-and-peace-input


        # Create the snapshot
        $ hdfs dfs -createSnapshot /user/hdfs/war-and-peace-input wapi-snap-1


        # Verify that the snapshot was created
        $ hdfs dfs -ls /user/hdfs/war-and-peace-input/


        # Delete the file
        $ hdfs dfs -rm -skipTrash /user/hdfs/war-and-peace-input/war-and-peace.txt


        # Verify that the file has been deleted
        $ hdfs dfs -ls /user/hdfs/war-and-peace-input/


        # Restore the file from the snapshot
        $ hdfs dfs -cp /user/hdfs/war-and-peace-input/.snapshot/wapi-snap-1/war-and-peace


        # Confirm the file has been restored
        $ hdfs dfs -ls /user/hdfs/war-and-peace-input/


        # Delete the snapshot
        $ hdfs dfs -deleteSnapshot /user/hdfs/war-and-peace-input wapi-snap-1


        # Make the directory un-snapshottable
        $ hdfs dfsadmin -disallowSnapshot /user/hdfs/war-and-peace-input



- Configuring an NFSv3 Gateway to HDFS

    - HDFS supports an NFS version 3 (NFSv3) gateway. This feature enables files to be easily moved 
        between HDFS and client systems. The NFS gateway supports NFSv3 and allows HDFS to be mounted 
        as part of the client’s local file system. 


    - Currently the NFSv3 gateway supports the following capabilities:

        1. Users can browse the HDFS file system through their local file system using an NFSv3
             client-compatible operating system.

        2. Users can download files from the HDFS file system to their local file system.

        3. Users can upload files from their local file system directly to the HDFS file system.

        4. Users can stream data directly to HDFS through the mount point. File append is supported, 
             but random write is not supported.


    - To enable the NFSv3 gateway, we need to:

        1. Set the required configuration files\

        2. Start the gateway

        3. Mount HDFS



- Capacity Scheduler Background

    - The Capacity Scheduler is the default scheduler for YARN that enables multiple groups to
        securely share a large Hadoop cluster.  Developed by the original Hadoop team at Yahoo, the
        Capacity Scheduler has successfully run many of the largest Hadoop cluster.


    - To use the Capacity Scheduler, one or more queues are configured with a predetermined fraction
        of the total slot capacity.  This assignment guarantees a minimum amount of resources for
        each queue.  Admins can configure soft limits and optional hard limits on the capacity
        allocated to each queue.


    - Each queue has strict Access Control Lists that control which users can submit applications to
        individual queues.  Also, safeguards are in place to ensure that users cannot view or modify
        applications from other users.


    - The ResourceManager UI provides a graphical representation of the scheduler queues and their
        utilization.  


    - In addition to the Capacity Scheduler, YARN also has a Fair Scheduler that can be used.