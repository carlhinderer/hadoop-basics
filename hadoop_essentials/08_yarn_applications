------------------------------------------------------------
CHAPTER 8 - YARN APPLICATIONS
------------------------------------------------------------

- The YARN Distributed-Shell

    - The YARN Project includes the 'Distributed-Shell' application, which is an example of
        a Hadoop non-MapReduce application built on top of YARN.  Distributed-Shell is a simple
        mechanism for running shell commands and scripts in containers on multiple nodes in a
        Hadoop cluster.


    - The Distributed-Shell is not meant to be a production administration tool, but rather a 
        demonstration of the non-MapReduce capability that can be built on top of YARN.  There are 
        multiple mature implementations of a distributed shell that administrators typically use to 
        manage a cluster of machines.


    - The Distributed-Shell can be used as a starting point for exploring and building YARN
        applications.



- Using the YARN Distributed-Shell

    # Assume and assign the HDP installation path
    $ export YARN_DS=/usr/hdp/current/hadoop-yarn-client/hadoop-yarn-applications-


    # Get options supported by Distributed-Shell
    $ yarn org.apache.hadoop.yarn.applications.distributedshell.Client -jar $YARN_DS -help


    # Run the simple 'uptime' command
    $ yarn org.apache.hadoop.yarn.applications.distributedshell.Client -jar $YARN_DS \
         -shell_command uptime



- Examining the Distributed-Shell Output

    - Now we want to examine the output for the application.  Distributed-Shell redirects the output of
        the individual shell commands run on the cluster nodes into the log files.  Assuming log 
        aggregation is enabled, the results for each instance of the command can be found by using
        the 'yarn logs' command.


    # Examine results from uptime command
    $ yarn logs -applicationId application_1432831236474_0001


    /* Abbreviated Output */

    Container: container_1432831236474_0001_01_000001 on n0_45454
    ===============================================================
    LogType:AppMaster.stderr
    Log Upload Time:Thu May 28 12:41:58 -0400 2015
    LogLength:3595

    Log Contents:
    15/05/28 12:41:52 INFO distributedshell.ApplicationMaster: Initializing
    ApplicationMaster
    [...]

    Container: container_1432831236474_0001_01_000002 on n1_45454    
    ===============================================================
    LogType:stderr
    Log Upload Time:Thu May 28 12:41:59 -0400 2015
    LogLength:0
    Log Contents:
    LogType:stdout
    Log Upload Time:Thu May 28 12:41:59 -0400 2015
    LogLength:71

    Log Contents:
     12:41:56 up 33 days, 19:28,  0 users,  load average: 0.08, 0.06, 0.01


     - Notice that there are 2 containers.  The first container ('con..._000001') is the 
         ApplicationMaster for the job.  The second container ('con..._000002') is the actual shell
         script.  The output for the 'uptime' command is located at the second container's 'stdout'
         after the 'Log Contents:' label.



- Using More Containers

    Distributed-Shell can run commands to be executed on any number of containers by way of the
        '-num_containers' argument.  For example, to see on which nodes the 'Distributed-Shell'
        command was run, the following command can be used:


        # See which containers the command was run on
        $ yarn org.apache.hadoop.yarn.applications.distributedshell.Client -jar $YARN_DS 
            -shell_command hostname -num_containers 4


    If we now examine the results for this job, there will be 5 containers in the log.  The four 
      command containers (2 through 5) will print the name of the node on which the container was
      run.



- Distributed-Shell Examples with Shell Arguments

    Arguments can be added to the shell command using the '-shell_args' option.  For example, to do a
      'ls -l' in the directory from where the shell command was run:

      # Run ls -l
      $ yarn org.apache.hadoop.yarn.applications.distributedshell.Client -jar $YARN_DS  
          -shell_command ls -shell_args -l



- Structure of YARN Applications

    - The central YARN 'ResourceManager' runs as a scheduling daemon on a dedicated machine and acts
        as a central authority for allocating resources to the various competing applications on the
        cluster.  The ResourceManager has a central and global view of all cluster resources, and,
        therefore, can ensure fairness, capacity, and locality are shared across all users.


    - Depending on the application demand, scheduling priorities, and resource availability, the
        ResourceManager dynamically allocates resource containers to applications to run on 
        particular nodes.  A container is a logical bundle of resources (ie memory and cores) bound
        to a particular cluster node.


    - To enforce and track such assignments, the ResourceManager interacts with a special system 
        daemon running on each node called the 'NodeManager'.  Communications between the 
        ResourceManager and NodeManagers are heartbeat based for scalability.  NodeManagers are
        responsible for local monitoring of resource availability, fault reporting, and 
        container life-cycle management (ie starting and killing jobs).  The ResourceManager depends
        on the NodeManagers for its 'global view' of the cluster.


    - User applications are submitted to the ResourceManager via a public protocol and go through an
        admission control phase during which security credentials are validated and various
        operational and administrative checks are performed.  Those applications that are accepted
        pass to the scheduler and are allowed to run.  


    - Once the scheduler has enough resources to satisfy the request, the application is moved from an
        'Accepted' state to a 'Running' state.  Aside from internal bookkeeping, this process involves
        allocating a container for the single ApplicationMaster and spawning it on a node in the
        cluster.  Often called 'Container 0', the ApplicationMaster does not have any additional
        resources at this point, but rather must request additional resources from the ResourceManager.


    - The ApplicationMaster is the “master” user job that manages all application life-cycle aspects,
        including dynamically increasing and decreasing resource consumption (i.e., containers), managing
        the flow of execution (e.g., in case of MapReduce jobs, running reducers against the output of
        maps), handling faults and computation skew, and performing other local optimizations. The
        ApplicationMaster is designed to run arbitrary user code that can be written in any programming
        language, as all communication with the ResourceManager and NodeManager is encoded using 
        extensible network protocols (i.e. Google Protocol Buffers).


    - YARN makes few assumptions about the ApplicationMaster, although in practice it expects most jobs 
        will use a higher-level programming framework. By delegating all these functions to 
        ApplicationMasters, YARN’s architecture gains a great deal of scalability, programming model
        flexibility, and improved user agility. For example, upgrading and testing a new MapReduce 
        framework can be done independently of other running MapReduce frameworks.


    - Typically, an ApplicationMaster will need to harness the processing power of multiple servers to
        complete a job. To achieve this, the ApplicationMaster issues resource requests to the
        ResourceManager. The form of these requests includes specification of locality preferences 
        (ie to accommodate HDFS use) and properties of the containers. The ResourceManager will 
        attempt to satisfy the resource requests coming from each application according to availability 
        and scheduling policies.


    - When a resource is scheduled on behalf of an ApplicationMaster, the ResourceManager generates a 
        lease for the resource, which is acquired by a subsequent ApplicationMaster heartbeat. The
        ApplicationMaster then works with the NodeManagers to start the resource. A token-based security
        mechanism guarantees its authenticity when the ApplicationMaster presents the container lease to 
        the NodeManager.


    - In a typical situation, running containers will communicate with the ApplicationMaster through an 
        application-specific protocol to report status and health information and to receive 
        framework-specific commands. In this way, YARN provides a basic infrastructure for monitoring 
        and life-cycle management of containers, while each framework manages application-specific 
        semantics independently. 


    - This design stands in sharp contrast to the original Hadoop version 1 design, in which scheduling 
        was designed and integrated around managing only MapReduce tasks.



- YARN Application Frameworks

    One of the most exciting aspects of Hadoop v2 is the ability to run all types of applications on 
      a Hadoop cluster.  YARN provides a resource management platform, which provides services such
      as scheduling, fault monitoring, data locality, and more to MapReduce and other frameworks.

    This is a brief survey of emerging open source YARN application frameworks that are being 
      developed to run under YARN.  


    - Distributed Shell

        Distributed-Shell is an example application included with the Hadoop core components that
          demonstrates how to write applications on top of YARN.  It provides a simple method
          for running shell commands and scripts in containers in parallel on a Hadoop YARN cluster.


    - Hadoop MapReduce

        MapReduce was the first YARN framework and drove many of YARN's requirements.  It is 
          integrated tightly with the rest of the Hadoop ecosystem projects, such as Pig, Hive, and
          Oozie.


    - Apache Tez

        Many Hadoop jobs involve the execution of a complex DAG of tasks involving separate MapReduce
          stages.  Tez generalizes this process and enables these tasks to be spread across stages so
          that they can be run as a single, all-encompassing job.  Tez can be used as a MapReduce 
          replacement for projects such as Hive and Pig, with no change needed to the Hive or Pig
          applications.


    - Apache Giraph

        Giraph is an iterative graph processing system built for high scalability.  Facebook, Twitter,
          and LinkedIn use it to create social graphs of users.  Giraph was originally written for 
          Hadoop v1, but that approach proved inefficient.  The native Giraph implementation under
          YARN provides the user with an iterative processing model that is not directly available with 
          MapReduce.  


    - Hoya: HBase on YARN

        The Hoya project creates dynamic and elastic Apache HBase clusters on top of YARN.  A client
          application creates the persistent container files, sets up the HBase cluster XML files,
          and then asks YARN to create an ApplicationMaster.  YARN copies all the files listed in the
          client's application-launch request from HDFS into the local file system of the chosen
          server, and then executes the command to start the Hoya ApplicationMaster.  Hoya also asks
          YARN for the number of containers matching the number of HBase region servers it needs.


    - Apache Spark

        Spark was initially developed for applications in which keeping data in memory improves
          performance, such as iterative algorithms, which are common in machine learning and 
          interactive data mining.  

        Spark differs from classic MapReduce in 2 important ways:

          1. Spark holds intermediate results in memory, rather than writing them to disk.

          2. Spark supports more than just MapReduce functions.  It greatly expands the set of 
               possible analyses that can be executed over HDFS data stores.  It also provides APIs
               in Scala, Java, and Python.


    - Apache Storm

        Traditional MapReduce jobs are expected to eventually finish, but Apache Storm continuously
          processes messages until it is stopped.  The framework is designed to process unbounded
          streams of data in real time.  It can be used with any programming language.  

        The basic Storm use cases include real-time analytics, online machine learning, continuous
          computation, distributed RPC, ETL, and more.  Storm provides fast performance, is scalable,
          is fault tolerant, and provides processing guarantees.  It works directly under YARN and
          takes advantage of the common data and resource management substrate.


    - Apache Flink

        Apache Flink is a platform for efficient, distributed, general-purpose data processing. It 
          features powerful programming abstractions in Java and Scala, a high-performance run time, 
          and automatic program optimization. It also offers native support for iterations, incremental
          iterations, and programs consisting of large DAGs of operations.

        Flink is primarily a stream-processing framework that can look like a batch-processing 
          environment. The immediate benefit from this approach is the ability to use the same 
          algorithms for both streaming and batch modes (exactly as is done in Apache Spark). However, 
          Flink can provide low-latency similar to that found in Apache Storm, but which is not 
          available in Apache Spark.

        In addition, Flink has its own memory management system, separate from Java’s garbage collector. 
          By managing memory explicitly, Flink almost eliminates the memory spikes often seen on Spark
          clusters.