---------------------------------------------------------------
CHAPTER 1 - BIG DATA TECHNOLOGY PRIMER
---------------------------------------------------------------

- Google creates big data with MapReduce and GFS projects.
    1. Reliable storage (Google File System)
    2. Processing (MapReduce, Pregel)
    3. Low-Latency Random Access Queries (Bigtable)

  All scaled over thousands of potentially unreliable commodity servers.



- Traditional Approach:
    1. Invest in a few extremely powerful servers
    2. Slurp the data in from the storage layer (SAN or NAS)
    3. Crunch through computation
    4. Write the results back to storage

  This worked fine until the data increased to internet-scale.  Then, it proved both
    expensive and impractical.



- Hadoop project created (2005)

    - Key innovation was to distribute the datasets across many machines and to split
        up any computations on the data into many independent, 'shared nothing'
        chunks, each of which could run on the same machine storing the data.

      This creates a horizontally scalable architecture.

    - Hadoop is an open-source implementation of these ideas.

    - Large-scale, distributed, shared nothing software requires a new approach to 
        operations, security, and governance.



- Misconceptions
    1. Data in Hadoop is schemaless
    2. One copy of the data
    3. One huge cluster


- General Trends
    1. Horizontal Scaling
    2. Adoption of Open Source
    3. Embracing Cloud Compute
    4. Decoupled Compute and Storage



- Cluster Architecture

   - Usually we divide a cluster up into 2 classes of machines: 'master' and 'worker'.
       Worker machines are where real work happens.  Master machines are responsible
       for coordination, maintaining metadata about the data and services running on
       the worker machines, and ensuring the services keep running in the event of
       worker failures.  

    - Typically, there are 2-3 master machines and a much larger number of workers.
        A cluster is scaled up by adding more workers, and eventually when the cluster
        gets big enough, more masters.  

    - Often, we want to allow access to the cluster by users and other applications, so
        we provide some machines to act as 'gateway' or 'edge servers'.  These servers
        often do not run any services at all but have the correct client configuration
        to access cluster services.



- Enabling Technologies for the Rest of the Stack
    1. HDFS
    2. YARN
    3. Apache Zookeeper
    4. Apache Hive Metastore


- HDFS

    - Fault tolerant, distributed file system optimized to store very large amounts of
        immutable data being accessed typically in long sequential scans.  HDFS is the 
        critical supporting technology for many other components in the stack.

    - When storing data, HDFS breaks up a file into blocks (typically 128 MB), and stores
        a replica of each block on multiple servers for resilience and data parallelism.
        Each worker node runs a daemon called a 'DataNode' which accepts new blocks and
        persists them to local disk.

      The 'DataNode' is also responsible for serving up data to clients.

    - The 'DataNode' is only aware of blocks and their ids.  It does not have knowledge
        about the file to which a particular replica belongs.  This information is curated
        by the coordinating process, the 'NameNode', which runs on master servers and is
        responsible for maintaining a mapping of files to blocks and metadata about the
        files themselves.

    - Clients wishing to store blocks must first communicate with the NameNode to be 
        given a list of DataNodes on which to write each block.  The client writes to the
        first DataNode, which in turn streams to the next DataNode, and so on in a 
        pipeline.  The NameNode can also take rack locality into account.

    - When reading data, the client asks the NameNode for a list of DataNodes containing
        the blocks for the files it needs.  The client then reads the data directly from the
        DataNodes, preferring locals that are local or close in network terms.

    - HDFS does not allow in-place updates to the files it stores.  This immutability allows
        it to achieve the required horizontal scalability and resilience in a relatively
        simple way.

    - HDFS is fault-tolerant because the failure of a single disk, data node, or even rack
        doesn't imperil the safety of the data.  In these situations, the NameNode simply
        directs one of the DataNodes that is maintaining a replica to copy the block to
        another DataNode until the 



- YARN

    - YARN is a centralized cluster manager, aware of all available compute resources and
        the current competing workload demands.


    - YARN runs a daemon on each node, called a 'NodeManager', which reports to a master
        process called the 'ResourceManager'.  

      Each NodeManager tells the ResourceManager how much compute resource and how much 
        memory is available on its node.  Resources are parceled out to applications running
        on the cluster in the form of 'containers'.  The NodeManagers are responsible for 
        starting and monitoring containers on their local nodes and killing them if they
        exceed their resource allocations.


    - An application that needs to run computations on the cluster must first ask the 
        ResourceManager for a single container on which to run its own coordinating process,
        called the 'ApplicationMaster'.  The AM actually runs on one of the worker machines.

      AMs of different applications will run on different worker machines, so the failure of
        a single machine will affect only a subset of applications running on the cluster.

      Once the AM is running, it requests additional containers from the ResourceManager
        to run its actual computation.


    - The ResourceManager runs a special thread which is responsible for scheduling applications
        and allocating containers equitably between applications.


    - Note that YARN itself does not perform any computation.  It is a framework for launching
        such applications distributed across a cluster.  YARN provides a suite of APIs for
        creating these applications.



- ZooKeeper

    - Consensus is an important problem in computer science.  When an application is distributed
        across many nodes, a key concern is getting these disparate componenets to agree on the
        values of some shared parameters.  For example, frameworks with multiple masters must know
        which one whould be in 'active master' and which should be in 'standby'.  


    - Apache ZooKeeper is the resilient, distributed configuration service for the Hadoop ecosystem.
        Within ZooKeeper, configuration data is stored and accessed in a filesystem-like tree of
        nodes called 'znodes'.  Each 'znode' can hold data and be the parent of 0 or more child
        nodes.

      Clients open a connection to a single ZooKeeper server to create, read, update, and delete
        the znodes.


    - For resilience, ZooKeeper instances should be deployed on different servers as an ensemble.
        Since ZooKeeper operates on majority consensus, an odd number of servers is required to
        for a quorum.

      Each server is identical in functionality, but one of the ensemble is elected the 'leader' 
        node.  All other servers are designated 'followers'.  ZooKeeper guarantees that data 
        updates are applied by a majority of ZooKeeper servers.  As long as a majority of
        servers are up and running, the ensemble is operational.


    - Clients can open connections to any of the servers to perform reads and writes, but writes
        are forwarded from follower servers to the leader to ensure consistency.  ZooKeeper
        ensures that all state is consistent by guaranteeing that updates are always applied
        in the same order.


    - Many frameworks rely on ZooKeeper:
        1. coordinating high availability in HDFS and YARN
        2. metadata and coordination in HBase, Solr, and Kafka
        3. table and partition locking and high availability in Hive 



- Hive Metastore

    - The Hive Metastore is a key component for other technologies in the stack.  It curates
        information about the structured datasets (as opposed to binary data) that reside in
        Hadoop and organizes them into a logical hierarchy of databases, tables, and views.

    - Hive tables have defined schemas which are specified during table creation.  These
        tables support most of the common data types from the RDBMS world.  The underlying
        storage is expected to match this schema, but it is only checked at runtime (schema
        on read).

    - Hive tables can be defined for data in other storage engines, including HBase and Kudu,
        but HDFS is by far the most common.

    - Hive supports partitioning via subdirectories within the table directory.  Within a 
        single partition, all files should have the same format.  

    - The metastore allows tables to be defined as either 'managed' or 'external'.  For
        managed tables, Hive actively controls the data in the storage engine.  For instance,
        if a table is created, Hive will create the directories in HDFS.  If a table is
        dropped, Hive deletes the data from the storage engine.

      For external tables, Hive makes no modifications to the underlying engine.  It merely
        maintains the metadata for the table in its database.



- Computational Frameworks
    1. MapReduce
    2. Spark
    Also consider... Apache Flink


- MapReduce

    - MapReduce is the original application for which Hadoop was built and is a Java-based
        implementation of Google's MapReduce paper.

    - Many higher-level frameworks compile their inputs into MapReduce jobs for execution.
        These include Hive, Sqoop, Oozie, and Pig.


    - Map = apply a transform function to every element in a collection

      Reduce = apply an aggregation function to each element of a list, combining them into
                 fewer values


    - MapReduce divides a computation into 3 sequential stages:
        1. Map
        2. Shuffle
        3. Reduce

      Sequences of these 3 simple linear stages can be composed and combined into essentially
        any computation of arbitrary complexity.  Examples include joins, aggregations, and
        advanced transformations.  Sometimes, with simple transforms, the reduce phase is not
        required at all.


    - Usually, the outputs from a MapReduce job is stored back into HDFS, where they may 
        form the inputs to other jobs.


    - MapReduce is relatively simple to use, but users must still code and compile 'map()'
        and 'reduce()' functions in Java.  Also, it does a lot of disk-based I/O between
        stages.  For these reasons, a number of successors have been developed.

      However, the conceptual model - data should be split up into multiple independent
        tasks running on different machines (maps), the results of which are then shuffled
        to and grouped and collated together on a different set of machines (reduce) are
        fundamental to all distributed data processing engines.  Spark, Flink, and Impala
        are all essential different implementations of this concept.



- Spark

    - Spark is a distributed computation framework with an emphasis on efficiency
        and usability.  It supports both batch and streaming computations.  


    - Instead of expressing map() and reduce() functions directly, Spark exposes a
        rich API of common operations, such as filtering, joining, grouping, and
        aggregations directly on datasets.  

      As well as using API methods, users can submit operations directly using a
        SQL-style dialect.  


    - A key feature of operations on datasets is that the processing graphs are run 
        through a standard query optimizer before execution, very similar to those
        found in relational DBs.  This optimizer can rearrange, combine, and prune the
        processing graph to obtain the most efficient execution pipeline.  This
        optimization helps avoid much of the intermediate I/O from MapReduce.


    - One of the principal design goals for Spark was to take full advantage of memory
        on worker nodes.  The ability to store and retrieve from main memory makes certain
        workloads drastically more efficient.

      Spark allows datasets to be cached in memory on the executors.  If the data does not
        fit entirely into memory, the partitions that cannot be cached are spilled to
        disk.


    - Spark implements stream processing as a series of periodic microbatches of datasets.
        This approach requires only small differences in the code to handle batch and
        streaming data.

      One drawback of this approach is that it takes at least the interval between batches
        for an event to be processed, so it's not suitable for use cases requiring
        millisecond latencies.

      However, this does allow a much higher data throughput than when processing events
        one by one.


    - Spark has become the de facto choice when creating new batch processing, machine learning,
        and streaming use cases.  



- Analytical SQL Engines
    1. Hive
    2. Impala
    Also consider... Presto, Apache Drill, Apache Phoenix


    - To write Spark or MapReduce jobs, you still need to code in Java/Scala/Python and run
        programs from the command line.  Many analysts can write SQL queries, but wouldn't be
        comfortable with full programming.  For this reason, a lot of effort has been made to
        develop SQL-like interfaces to structured data stored in Hadoop.



- Hive

    - Hive is the original data warehousing technology for Hadoop.  It was developed at
        Facebook and was the first to offer a SQL-like language, HiveQL, that allowed analysts
        to query structured data in HDFS without compiling and deploying code.

    - 




- Storage Engines
    1. HBase
    Also consider... Apache Cassandra, Apache Accumulo
    2. Kudu
    3. Solr
    Also consider... Elasticsearch
    4. Kafka


- Ingestion
    1. Apache Nifi
    2. StreamSets Data Collector


- Orchestration
    1. Oozie
    Also consider... Apache Airflow, Spotify Luigi