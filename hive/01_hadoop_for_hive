------------------------------------------------------------
CHAPTER 1 - HADOOP FOR HIVE
------------------------------------------------------------

- Hadoop Mechanics

    - MR is a traditional batch-oriented processing framework.  New processing engines such as Tez
        are geared more toward near real-time access.  With the advent of YARN, HDFS is now a 
        multitenant environment allowing for many data access patterns, such as batch, real-time, and
        interactive.


    - HDFS is a virtual filesystem on top of the Linux filesystem.  It abstracts away the fact that
        you're storing data on multiple nodes in a cluster.


    - HDFS is a big step away from traditional virtualization approaches that relied on SANs.  HDFS
        provides scalable storage and redundancy using commodity hardware.  



- Traditional High Availability

    - The traditional High Availability systems before Hadoop were very expensive in terms of hardware,
        software, and maintenance costs.  The more uptime you needed, the more expensive the solution.
        Most architectures involved a set of passive systems sitting in wait to be utilized if the
        primary system fails.  Once the primary node fails, a secondary system takes over.  This 
        approach increases costs without increasing available resources.


    - Traditional clusters require shared storage architecture, usually served by a SAN infrastrucure.
        SANs can store a tremendous amount of data, but are expensive to build and maintain.  SANs
        are separate from servers, so all data must be transmitted across networks.  


    - Most storage vendors use RAID to provide data protection.  While RAID provides data protection
        and sometimes speeds up reads and writes, it requires redundant disks.  Most SANs use
        RAID 10.  


    - In essence, SANs are large containers holding multiple disk arrays and managed by a central 
        console.  A company purchases a server, the server is provisioned in a data center with minimal
        storage for the OS, and is connected to the SAN infrastructure via a network.  Applications
        request data from the SAN, which is pulled through the network for processing on the server.



- Hadoop High Availability

    - Hadoop provides an alternative framework for high availability.  First, it assumes node failure
        will occur, and builds mechanism to account for this in the software.  This means Hadoop is
        highly available out of the box, but the extent can be configured.  Because it is open source,
        it removes much of the cost of HA.


    - Hadoop takes advantage of the fact that storage costs have dropped significanly.  It duplicates
        data for the purpose of redundancy, by default 3 times the original size.


    - The use of large block sizes influences much of Hadoop's architecture.  Large block sizes are core
        to how Hadoop is deployed, managed, and provisioned.  This is due to the following considerations:

        1. Large files are more efficiently processed than smaller files
        2. There are fewer memory requirements on the master server
        3. More efficient sequential reads and writes
        4. The seek rate is reduced as a percentage of transfer time


    - The NameNode:

        1. Tracks which blocks in the cluster belong to which file
        2. Maintains where in the cluster each block is located
        3. Determines where to place blocks based on node location
        4. Tracks overall health of the cluster through block reports


    - Hadoop knows all the available DataNodes and on which rack the DataNodes are located.  This is
        known as 'rack awareness'.  Hadoop will replicate data with these steps:

        1. A block is written to [Rack 1, Node 1]
        2. A copy of the block is written to [Rack 2, Node 2]
        3. A copy of the block is written to [Rack 2, Node 3]

      Even if there are more than 2 racks, the third write will still be to the same rack as the second
        one.  This is because Hadoop tries to maximize availability while reducing network traffic.



- Processing with MapReduce

    - If we begin a job process on the cluster, it would be unacceptable to have to restart the entire
        job 5 hours into processing simply because a single node became unavailable.


    - MapReduce is declining in importance, but it is still useful to understand how it works.  
        Applications such as Hive and Pig can execute MapReduce behind the scenes (although its not
        recommended), and it is helpful to understand what MapReduce is doing so that we can better
        tune our queries and understand their behavior.


    - The Map() function runs on each DataNode and processes all the blocks on that DataNode 
        associated with the file.  It does this independently of all other blocks located on the
        other DataNodes.  This provides an 'embarassingly parallel', 'shared nothing' environment.
        The Map() function outputs a key/value pair.


    - Once the Map() phase completes, we have an intermediary phase called Shuffle and Sort.  This 
        phase takes all the key/value pairs from the Map phase and assigns them to a reducer.  Each
        reducer receives all data associated with a single key.  The Shuffle and Sort phase is the 
        only time data is physically moved within the cluster and communication occurs between
        processes.


    - As we dig deeper into Hive performance, we will want to focus on avoiding the reduce phase.
        This phase can be a bottleneck because it requires moving data over the network as well as
        communication between nodes.  Also, the reduce phase cannot run until all mapping has 
        completed.


    - MapReduce Example


                      List of Names
                          DOC

                      /    |    \
                     /     |     \    MAP
                    /      |      \

              James      James       Wendy
              Joan       Peter       Mordecai
              John       Peter       Frank
              Frank      Arthur      Frank
              Peggy      Wendy       Susan
                         Bob         Fredrick

                    SHUFFLE AND SORT

            James, 1     Joan, 1     Susan, 1
            James, 1     Frank, 1    Fredrick, 1
            Mordecai, 1  Frank, 1    Peggy, 1
            John, 1      Frank, 1
            Wendy, 1     Arthur, 1
            Wendy, 1     Wendy, 1
            Bob, 1       Wendy, 1
                         Peter, 1
                         Peter, 1

                         REDUCE

            James, 2     Joan, 1     Susan, 1
            Mordecai, 1  Frank, 3    Fredrick, 1
            John, 1      Arthur, 1   Peggy, 1
            Wendy, 2     Wendy, 2
            Bob, 1       Peter, 2


    - The Shuffle and Sort phase is responsible for sorting the data by keys and sending the data to
        the reducer tasks.  Each reducer will receive all the data from a single key.  


    - Note that you must know your data!  If you have a data set where 50% of the values are for a 
        single key like 'Bob', then a single reducer may become overwhelmed.


    - The final stage is the Reduce phase.  Reduce takes each key/value pair as input and produces a
        count aggregation based on the key.  The reduce phase can be thought of as a GROUP BY clause.



- Beyond MapReduce

    - MR is an extraordinarily flexible parallel processing framework, but as scalable and flexible as
        it is, it also has many limitations.  MR process data in batch.  It exceeds at taking large
        data sets, processing them in parallel, and then aggregating the results.  MR does not work
        well with ad hoc or real-time query patterns.


    - For example, if you want to get all sales for a product from every store in the past decade, and
        this query traverses 10 TB of data, and you are willing to wait 10 hours to get the results,
        MR would be an excellent choice.  But if you want to get the top 2 items sold for 5 stores in
        Missouri, and you need the data in less than 10 seconds, MR is not a good choice.


    - In reality, most organizations center around an ad hoc or near real-time processing business
        intelligence architecture of which MR does not belong.  We take for granted the speeds in which
        RDBMS's process joins, GROUP BY, ORDER BY, and other computations and lose sight of the face
        that the processing speeds are due to the upfront cost of constraining and conforming the
        data to specific schema structures and rules.


    - Hadoop is 'schema-on-read'.  Ingesting data into traditional RDBMS's involves ETL, which works
        well when the source data is relational.  This doesn't work well with unstructured or semi-
        structured data, however.  For instance, it is possible to transform log data into a 
        relational model, but at the cost of slowing down the ingestion rate and the breaking of
        the ingestion process when simple domain constructs change.  Where we lose structure we gain
        flexibility.


    - MR tasks are written in Java.  MR handles the runtime complexities as well as the management and
        scheduling of jobs on the cluster.  MR requires a strong knowledge of Java and the MR APIs.
        As Hadoop moves more mainstream, it has moved away from being a Java development tool and 
        caters more strongly to areas of the business such as ETL and business analytics.



- Modern Data Architecture

    
      --------------------------------------------------------------------
      |  APPLICATIONS      Business      Custom            Packaged      |
      |                    Analytics     Applications      Applications  |
      --------------------------------------------------------------------
                                          ^
                                          |
      --------------------------------------------------------------------------------
      |                                                Batch  Interactive  Real-time |
      |  DATA           Rdbms   Edw   Mpp     <---->              YARN               |
      |  SYSTEM                                                   HDFS               |
      --------------------------------------------------------------------------------
                                          ^
                                          |
      -----------------------------------------------------------------------------------------------
      |  SOURCES    Existing   Clickstream   Web &   Geolocation   Sensor &   Server   Unstructured |
      |             Systems                  Social                Machine    Logs                  |
      -----------------------------------------------------------------------------------------------



    - At its essence, Hadoop is a platform or architecture driving modern analytics.  Industry refers
        to this as the 'Modern Data Architecture'.


    - The architecture incorporates additional sources into the data flow that were previously
        untapped due to the restrictions of traditional RDBMS's.  We can now include sources such as
        clickstream, web and social, sensor and machine, logs, and images.


    - As we pull this data into Hadoop as streaming inputs or batch, we stage them in HDFS for direct
        analysis or movement into other systems. This approach optimizes the RDBMs, EDW, and MPP 
        resources by offloading resource intensive and time-consuming extract, transform, and load
        operations onto the much more economical Hadoop platform. You essentially move from an ETL model 
        to a ELT model. You extract and load everything into Hadoop but only transform the data 
        appropriate to your given platform or analytical needs.


    - YARN is the driving force behind this architecture.  When MR was the only computation engine, 
       Hadoop put MR jobs in a queue and a job couldn't run until the others finished.  YARN 
       introduces the idea of containers.  


    - A Resource Manager schedules jobs and allocates application resources based on assigned policies.
        These policies can include things like 'Marketing gets a maximum of 50% of the cluster memory'.
        This is known as a 'Capacity Scheduler', which is the default Hadoop scheduler.  Another is 
        the 'Fair Scheduler', which just does FIFO.


    - Data Nodes run an ApplicationMaster whose purpose is to control each container on a per-application
        basis.  The ApplicationMaster acts as the messenger for the ResourceManager's ApplicationManager
        component, and controls resource allocation locally on each node.