------------------------------------------------------------
CHAPTER 1 - HADOOP FOR HIVE
------------------------------------------------------------

- Hadoop Mechanics

    - MR is a traditional batch-oriented processing framework.  New processing engines such as Tez
        are geared more toward near real-time access.  With the advent of YARN, HDFS is now a 
        multitenant environment allowing for many data access patterns, such as batch, real-time, and
        interactive.


    - HDFS is a virtual filesystem on top of the Linux filesystem.  It abstracts away the fact that
        you're storing data on multiple nodes in a cluster.


    - HDFS is a big step away from traditional virtualization approaches that relied on SANs.  HDFS
        provides scalable storage and redundancy using commodity hardware.  



- Traditional High Availability

    - The traditional High Availability systems before Hadoop were very expensive in terms of hardware,
        software, and maintenance costs.  The more uptime you needed, the more expensive the solution.
        Most architectures involved a set of passive systems sitting in wait to be utilized if the
        primary system fails.  Once the primary node fails, a secondary system takes over.  This 
        approach increases costs without increasing available resources.


    - Traditional clusters require shared storage architecture, usually served by a SAN infrastrucure.
        SANs can store a tremendous amount of data, but are expensive to build and maintain.  SANs
        are separate from servers, so all data must be transmitted across networks.  


    - Most storage vendors use RAID to provide data protection.  While RAID provides data protection
        and sometimes speeds up reads and writes, it requires redundant disks.  Most SANs use
        RAID 10.  


    - In essence, SANs are large containers holding multiple disk arrays and managed by a central 
        console.  A company purchases a server, the server is provisioned in a data center with minimal
        storage for the OS, and is connected to the SAN infrastructure via a network.  Applications
        request data from the SAN, which is pulled through the network for processing on the server.



- Hadoop High Availability

    - Hadoop provides an alternative framework for high availability.  First, it assumes node failure
        will occur, and builds mechanism to account for this in the software.  This means Hadoop is
        highly available out of the box, but the extent can be configured.  Because it is open source,
        it removes much of the cost of HA.


    - Hadoop takes advantage of the fact that storage costs have dropped significanly.  It duplicates
        data for the purpose of redundancy, by default 3 times the original size.


    - The use of large block sizes influences much of Hadoop's architecture.  Large block sizes are core
        to how Hadoop is deployed, managed, and provisioned.  This is due to the following considerations:

        1. Large files are more efficiently processed than smaller files
        2. There are fewer memory requirements on the master server
        3. More efficient sequential reads and writes
        4. The seek rate is reduced as a percentage of transfer time


    - The NameNode:

        1. Tracks which blocks in the cluster belong to which file
        2. Maintains where in the cluster each block is located
        3. Determines where to place blocks based on node location
        4. Tracks overall health of the cluster through block reports


    - Hadoop knows all the available DataNodes and on which rack the DataNodes are located.  This is
        known as 'rack awareness'.  Hadoop will replicate data with these steps:

        1. A block is written to [Rack 1, Node 1]
        2. A copy of the block is written to [Rack 2, Node 2]
        3. A copy of the block is written to [Rack 2, Node 3]

      Even if there are more than 2 racks, the third write will still be to the same rack as the second
        one.  This is because Hadoop tries to maximize availability while reducing network traffic.



- Processing with MapReduce

    - If we begin a job process on the cluster, it would be unacceptable to have to restart the entire
        job 5 hours into processing simply because a single node became unavailable.


    - MapReduce is declining in importance, but it is still useful to understand how it works.  
        Applications such as Hive and Pig can execute MapReduce behind the scenes (although its not
        recommended), and it is helpful to understand what MapReduce is doing so that we can better
        tune our queries and understand their behavior.


    - The Map() function runs on each DataNode and processes all the blocks on that DataNode 
        associated with the file.  It does this independently of all other blocks located on the
        other DataNodes.  This provides an 'embarassingly parallel', 'shared nothing' environment.
        The Map() function outputs a key/value pair.


    - Once the Map() phase completes, we have an intermediary phase called Shuffle and Sort.  This 
        phase takes all the key/value pairs from the Map phase and assigns them to a reducer.  Each
        reducer receives all data associated with a single key.  The Shuffle and Sort phase is the 
        only time data is physically moved within the cluster and communication occurs between
        processes.


    - As we dig deeper into Hive performance, we will want to focus on avoiding the reduce phase.
        This phase can be a bottleneck because it requires moving data over the network as well as
        communication between nodes.  Also, the reduce phase cannot run until all mapping has 
        completed.


    - Shuffle and Sort Example


                      List of Names
                          DOC

                      /    |    \
                     /     |     \
                    /      |      \

              James      James       Wendy
              Joan       Peter       Mordecai
              John       Peter       Frank
              Frank      Arthur      Frank
              Peggy      Wendy       Susan
                         Bob         Fredrick

                    SHUFFLE AND SORT

            James, 1     Joan, 1     Susan, 1
            James, 1     Frank, 1    Fredrick, 1
            Mordecai, 1  Frank, 1    Peggy, 1
            John, 1      Frank, 1
            Wendy, 1     Arthur, 1
            Wendy, 1     Wendy, 1
            Bob, 1       Wendy, 1
                         Peter, 1
                         Peter, 1


    - The Shuffle and Sort phase is responsible for sorting the data by keys and sending the data to
        the reducer tasks.  Each reducer will receive all the data from a single key.  


    - Note that you must know your data!  If you have a data set where 50% of the values are for a 
        single key like 'Bob', then a single reducer may become overwhelmed.