------------------------------------------------------------
CHAPTER 2 - HIVE
------------------------------------------------------------

- Hive Beginnings

    - The original idea of Hive was for people who don't know Java, but do know SQL, to be able to use
        the parallel processing features of MR.  The first implementation of Hive was just as an
        abstraction layer on top of MR.


    - Hive was created by Facebook to abstract MR from business analysts and make Hadoop accessible to
        those familiar with SQL and who are most responsible for viewing the data and extracting
        valuable analytical insights.


    - Hive is now the de facto standard and most widely used SQL-on-Hadoop tool.



- Cluster Architecture

    - Generally, admins divide cluster servers into 3 categories: master, edge, and worker.  A master
        server contains any component critical to the cluster where high availability is a requirement.
        A worker server contains any cluster service that is easily replaced or can incur downtime
        without fear of data loss.


    - These are examples of services you will want to provision on master nodes in a typical cluster:

        - NameNode and SecondaryNameNode
        - JobTracker
        - ResourceManager
        - HBase Master
        - HiveServer2
        - Oozie Server
        - Zookeeper
        - Storm Server
        - WebHCat Server


    - Hadoop vendors tend to segregate clusters into 3 sizes: small, medium, and large.  

        - Small = <32 nodes (fits on a single rack typically)
        - Medium = 32-150 nodes
        - Large = >150 nodes


    - As much as Hadoop touts its resiliency, master servers are still single points of failure and 
        need to be accounted for appropriately.


    - The Hive client is installed on all worker nodes.  When interacting with Hive, you will most likely
        access it through a web portal such as Ambari or Hue.  These servers tend to be installed on
        edge nodes.  Edge nodes have fewer resources with no master server components.  However, they may
        contain metadata repositories that should be backed up like any other relational database system.

      Edge nodes can be though of as management servers or even web servers.  An edge node may contain
        operational software, such as Ambari, MCS, or Cloudera Manager as well as client components 
        such as Pig or Hive.  They may also be used for firewall purposes such as is the case for 
        Apache Knox.  The point is that edge nodes tend to be smaller servers whose main purpose is
        to act as a client gateway into the larger Hadoop infrastructure.  They still may need 
        substantial storage to account for the application logging they do.


    - Another way to took at edge nodes are as management servers that contain non-distributed components.
        For instance, Ambari runs as a single instance and is not distributed across multiple nodes.



- Hive Installation

    - Hive is only one component of a larger Hadoop ecosystem.  Clusters will most likely include 
        applications like Solr (used for text searching) and HBase (used for transaction-like processing).


    - This book uses the HDP version of Hive, because it is the standard open-source version.  Cloudera
        also provides Hive, but they focus on their Impala as their primary SQL-on-Hadoop solution.
        MapR favors the use of Apache Drill.


    - Hive is a client application using HDFS for its backend storage.  Included in Hive are other 
        server and functional components such as 'HiveServer2' and 'HCatalog'.  At installation time,
        we designate nodes for the Hive client and HiveServer and HCatalog.  


    - When we navigate to Ambari, and go to the Hive tab, we should see 5 services running:

        1. Hive Metastore
        2. HiveServer2
        3. MySQL Server
        4. WebHCat Server
        5. Hive Client

      All of these services are necessary for Hive to operate.



- Navigating Hive Tools

    - Ambari views provide an easy means for executing Hive queries through a GUI.  The Hive view
        has a database explorer, query editor, and various configuration and management options.
        The Hive view is designed for an end user and not an admin.  It is typically used by a business
        analyst or SQL developer.


    - Hive SQL is called HiveQL.  It is case-insensitive and requires statements to be 
        semicolon-terminated.


    - As you create tables and databases in Hive, they will appear in HCatalog.  HCatalog provides a 
        means for other applications besides Hive to access these tables, preventing you from having
        to recreate a table on a per-application basis.  A HCat table, as well as any Hive table you 
        create, can be accessed through an ODBC or JDBC connection as well as through specific HCat
        loaders.  


    - Hive out-of-the-box will not be as performant as an RDBMS.  It is equivalent to running a query
        with no indexes.  There are a number of performance best practices we will become familiar
        with.  Remember too that Hive is an analytic tool and will not replace your existing OLTP
        processes.  As scalable as Hive is, it does not mean you can start using Hive as an 
        e-commerce cart application.



- Hive CLI

    - Besides the GUI option, Hive provides a command-line interface for managing and running scripts,
        DDL commands, and DML commands.  The command line provides flexibility and low overhead for
        interacting with Hive.  


        # Start Hive CLI
        $ hive


        # Show all tables
        hive> show tables;


        # Run a query
        hive> SELECT * FROM sample_07 LIMIT 10;


        # View metadata about objects
        hive> DESCRIBE sample_07;
        hive> DESCRIBE EXTENDED sample_07;
        hive> DESCRIBE FORMATTED sample_07;


        # Exit the hive prompt
        hive> exit;