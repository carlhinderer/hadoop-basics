------------------------------------------------------------
CHAPTER 4 - PIG LATIN
------------------------------------------------------------

- Preliminaries

    - Pig Latin is a data flow language.  Each processing step results in a new dataset or relation.
        In "input = load 'data'", 'input' is the name of a relation that results from loading the
        dataset 'data'.  Relations are immutable.


    - In addition to relation names, Pig also has field names.  They name a field (or column) in a
        relation.  Both relation and field names must start with an alphabetic character, then contain
        0 or more alphanumeric or underscore characters.


    - Keywords in Pig Latin are not case-sensitive, so 'LOAD' is the same as 'load'.  But, relation and
        field names are case-sensitive, as are UDF names.


    - Pig Latin has 2 types of comments:

        -- This is a single-line comment


        /*
         * This is a multiline comment
         */


        B = load /* comment in the middle */ 'bar';



- The 'load' Statement

    - By default, 'load' looks for your data in tab-separated form (and your Pig jobs run) on HDFS in 
        users/yourname.  You can also specify relative pathnames or full URLs.


        -- Load with relative pathname
        divs = load '/data/examples/NYSE_dividends';


        -- Load with full URL
        divs = load 'hdfs://nn.acme.com/data/examples/NYSE_dividends';


    - In practice, most of your data will not be in tab-separated text files.  Pig allows you to specify
        the function for loading your data with the 'using' clause.


        -- Load data from HBase using the HBase loader
        divs = load 'NYSE_dividends' using HBaseStorage();


    - If you don't specify a load function, the built-in function 'PigStorage' will be used by default.
        You can also pass arguments to your load function via the 'using' clause.  


        -- Read comma-separated data
        divs = load 'NYSE_dividends' using PigStorage(',');


    - You can also apply a schema.


        -- Load with a schema
        divs = load 'NYSE_dividends' as (exchange, symbol, date, dividends);



- Loading Entire Directories

    - When specifying a file to read from HDFS, you can also specify a directory.  In this case, Pig will
        use all the files in the directory as input.  


    - 'PigStorage' and 'TextLoader', the 2 built-in Pig load functions that operate on HDFS files, 
        support globs.  With globs, you can read multiple files that are not under the same directory
        or read some but not all files in a directory.


        Glob          Meaning
        -----------------------------------
        ?             Matches any single character.
        *             Matches zero or more characters.
        [abc]         Matches a single character from the character set (a,b,c).
        [a-z]         Matches a single character from the character range (a..z), inclusive. The first 
                        character must be lexicographically less than or equal to the second character.
        [^abc]        Matches a single character that is not in the character set (a,b,c). The ^ 
                        character must appear immediately to the right of the opening bracket.
        [^a-z]        Matches a single character that is not from the character range (a..z), inclusive. 
                        The ^ character must appear immediately to the right of the opening bracket.
        \c            Removes (escapes) any special meaning of the character c.
        {ab,cd}       Matches a string from the string set {ab,cd}.



- The 'store' Statement

    - The 'store' statement is used for writing data after you are done processing it.  By default,
        Pig stores your data on HDFS in a tab-delimited file using 'PigStorage'.


        -- Store processed data into a directory named 'processed'
        store processed into '/data/examples/processed';


        -- Use a full URL instead
        store processed into 'hdfs://nn.acme.com/data/examples/processed';


    - If you don't specify a store function, 'PigStorage' will be used.  You can specify a different
        store function with a 'using' clause.


        -- Store processed data into HBase
        store processed into 'processed' using HBaseStorage();


    - You can also pass arguments to your 'store' function.  


        -- Store data as comma-separated
        store processed into 'processed' using PigStorage(',');



- The 'dump' Statement

    - Sometimes, instead of storing data, you want to see it on the screen instead.  This is particularly
        useful during debugging and prototyping sessions.  It can also be useful for quick ad hoc jobs.


        -- Direct the output of the script to the screen
        dump processed;



- The 'foreach' Statement

    - The 'foreach' statement takes a set of expressions and applies them to every record in the data
        pipeline.  From these expressions it generates new records to send down the pipeline to the
        next operator.  It is Pig's projection operator.


        -- Load the entire records, then remove all but the user and id fields
        A = load 'input' as (user:chararray, id:long, address:chararray, 
                             phone:chararray, preferences:map[]);
        B = foreach A generate user, id;



- Expressions in 'foreach'

    - 'foreach' supports a list of expressions.  The simplest are constants and field references.  Field
        references can be by name or by position.  Positional references are preceded by '$' and start
        from zero.


        prices = load 'NYSE_daily' as (exchange, symbol, date, open, high, low, close, volume, adj_close);

        -- Reference by field name
        gain   = foreach prices generate close - open;

        -- Reference by position
        gain2  = foreach prices generate $6 - $3;


    - You can also refer to all fields using an asterisk ('*').  You can refer to ranges of fields using
        2 periods ('..').


        prices    = load 'NYSE_daily' as (exchange, symbol, date, open, high, low, 
                                          close, volume, adj_close);

        -- Get ranges of fields
        beginning = foreach prices generate ..open;
        middle    = foreach prices generate open..close;
        end       = foreach prices generate volume..;


    - Basic arithmetic operations and conditions can be used.

        -- Use condition
        daily = load 'NYSE_daily' as (exchange:chararray, symbol:chararray, date:chararray, 
                                      open:float, high:float, low:float, close:float,
                                      volume:int, adj_close:float);
        updown = foreach daily generate (close>open?'up':'down');


    - To extract data from complex types, use the projection operators.  For maps, this is '#'.

        -- Extract data from a map
        bball = load 'baseball' as (name:chararray, team:chararray, 
                                    position:bag{t:(p:chararray)}, bat:map[]);
        avg = foreach bball generate bat#'batting_average';


    - To extract data from tuples, use the field names or field positions.  Referencing a field name
        that doesn't exist in the tuple will produce an error.

        -- Extract data from a tuple
        A = load 'input' as (t:tuple(x:int, y:int));
        B = foreach A generate t.x, t.$1;


    - When you project fields in a bag, you are creating a new bag with only those fields.  

        -- Create a new bag with selected fields
        A = load 'input' as (b:bag{t:(x:int, y:int)});
        B = foreach A generate b.x;

      This will produce a new bag whose tuples have only the field 'x' in them.

        -- Project multiple fields in a bag
        B = foreach A generage b.(x, y);