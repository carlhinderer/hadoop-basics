---------------------------------------------------------------------------
CHAPTER 2 - SPARK INTRO
---------------------------------------------------------------------------

- Spark Applications

    - Spark Applications consist of a driver process and a set of executor processes. The driver 
        process runs your main() function, sits on a node in the cluster, and is responsible for 
        three things: 

        - maintaining information about the Spark Application
        - responding to a user’s program or input
        - analyzing, distributing, and scheduling work across the executors 


    - The driver process is absolutely essential—it’s the heart of a Spark Application and maintains 
        all relevant information during the lifetime of the application.


    - The executors are responsible for actually carrying out the work that the driver assigns them. 
        This means that each executor is responsible for only two things: executing code assigned 
        to it by the driver, and reporting the state of the computation on that executor back to the 
        driver node.


    - The cluster manager can be one of three core cluster managers: Spark’s standalone cluster manager,
        YARN, or Mesos. This means that there can be multiple Spark Applications running on a cluster 
        at the same time.  The user can specify how many executors should fall on each node through 
        configurations.


        Driver Process             Executors
        ---------------
        |Spark Session|\     ------
        |  ^    ^     | \---|    |
        |  |    |     |  \  |    |
        |  v    v     |   \ ------    ------
        |User Code    |    \  ^       |    |
        ---------------     \-|-------|    |     ------
          ^    ^             \|       ------     |    |
          |    |              |\---------^-------|    |
          |    |              |          |       ------
          v    v              v          v          |
        ------------------------------------------------------------  
        |               Cluster Manager                            |
        ------------------------------------------------------------


    - Note that Spark, in addition to its cluster mode, also has a local mode. The driver and executors 
        are simply processes, which means that they can live on the same machine or different machines. 
        In local mode, the driver and executurs run (as threads) on your individual computer instead of 
        a cluster. We wrote this book with local mode in mind, so you should be able to run everything 
        on a single machine.


    - Here are the key points to understand about Spark Applications at this point:

        - Spark employs a cluster manager that keeps track of the resources available.

        - The driver process is responsible for executing the driver program’s commands across the 
            executors to complete a given task.

        - The executors, for the most part, will always be running Spark code. However, the driver can 
            be “driven” from a number of different languages through Spark’s language APIs. Let’s take 
            a look at those in the next section.