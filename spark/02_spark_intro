---------------------------------------------------------------------------
CHAPTER 2 - SPARK INTRO
---------------------------------------------------------------------------

- Spark Applications

    - Spark Applications consist of a driver process and a set of executor processes. The driver 
        process runs your main() function, sits on a node in the cluster, and is responsible for 
        three things: 

        - maintaining information about the Spark Application
        - responding to a user’s program or input
        - analyzing, distributing, and scheduling work across the executors 


    - The driver process is absolutely essential—it’s the heart of a Spark Application and maintains 
        all relevant information during the lifetime of the application.


    - The executors are responsible for actually carrying out the work that the driver assigns them. 
        This means that each executor is responsible for only two things: executing code assigned 
        to it by the driver, and reporting the state of the computation on that executor back to the 
        driver node.


    - The cluster manager can be one of three core cluster managers: Spark’s standalone cluster manager,
        YARN, or Mesos. This means that there can be multiple Spark Applications running on a cluster 
        at the same time.  The user can specify how many executors should fall on each node through 
        configurations.


        Driver Process             Executors
        ---------------
        |Spark Session|\     ------
        |  ^    ^     | \---|    |
        |  |    |     |  \  |    |
        |  v    v     |   \ ------    ------
        |User Code    |    \  ^       |    |
        ---------------     \-|-------|    |     ------
          ^    ^             \|       ------     |    |
          |    |              |\---------^-------|    |
          |    |              |          |       ------
          v    v              v          v          |
        ------------------------------------------------------------  
        |               Cluster Manager                            |
        ------------------------------------------------------------


    - Note that Spark, in addition to its cluster mode, also has a local mode. The driver and executors 
        are simply processes, which means that they can live on the same machine or different machines. 
        In local mode, the driver and executurs run (as threads) on your individual computer instead of 
        a cluster. We wrote this book with local mode in mind, so you should be able to run everything 
        on a single machine.


    - Here are the key points to understand about Spark Applications at this point:

        - Spark employs a cluster manager that keeps track of the resources available.

        - The driver process is responsible for executing the driver program’s commands across the 
            executors to complete a given task.

        - The executors, for the most part, will always be running Spark code. However, the driver can 
            be “driven” from a number of different languages through Spark’s language APIs. Let’s take 
            a look at those in the next section.



- The Spark Session

    - A Spark application is controlled through a driver process called the 'SparkSession'.  The
        SparkSession instance is the way Spark executes user-defined manipulations across the 
        cluster.  There is a one-to-one correspondence between a SparkSession and Spark application.


    # Start pyspark
    pyspark

    # Get the spark session
    >>> spark

    # Create a DataFrame with 1 column, containing 1000 rows with values 0-999.
    #   This is a distributed collection.  When run on a cluster, each part of this range of
    #   number exists on a different executor.
    >>> myRange = spark.range(1000).toDF("number")



- DataFrames

    - A DataFrame is the most common Structured API and simply represents a table of data with rows 
        and columns. The list that defines the columns and the types within those columns is called 
        the schema. You can think of a DataFrame as a spreadsheet with named columns. 

    - Unlike Python/R DataFrames, Spark DataFrames exist on multiple machines.  However, because Spark 
        has language interfaces for both Python and R, it’s quite easy to convert Pandas (Python) 
        DataFrames to Spark DataFrames, and R DataFrames to Spark DataFrames.



- Partitions

    - To allow every executor to perform work in parallel, Spark breaks up the data into chunks 
        called partitions.  A partition is a collection of rows that sit on one physical machine in 
        your cluster.  A DataFrame’s partitions represent how the data is physically distributed 
        across the cluster of machines during execution. 

    - If you have one partition, Spark will have a parallelism of only one, even if you have 
        thousands of executors. If you have many partitions but only one executor, Spark will still 
        have a parallelism of only one because there is only one computation resource.

    - An important thing to note is that with DataFrames you do not (for the most part) manipulate
        partitions manually or individually. You simply specify high-level transformations of data in 
        the physical partitions, and Spark determines how this work will actually execute on the 
        cluster. Lower-level APIs do exist (via the RDD interface), but are used less often.



- Transformations

    - In Spark, the core data structures are immutable.  To change a DataFrame, you need to apply
        transformations.

        # Find all even numbers in our DataFrame
        divisBy2 = myRange.where("number % 2 == 0")


    - Notice that these transformations return no output.  This is because we specified only an 
        abstract transformation, and Spark will not act on transformations until we call an action. 
        Transformations are the core of how you express your business logic using Spark. There are 
        two types of transformations: those that specify 'narrow dependencies', and those that specify 
        'wide dependencies'.

    - Transformations consisting of narrow dependencies (we’ll call them narrow transformations) are 
        those for which each input partition will contribute to only one output partition. In the 
        preceding code snippet, the where statement specifies a narrow dependency, where only one 
        partition contributes to at most one output partition.

    - A wide dependency (or wide transformation) style transformation will have input partitions
        contributing to many output partitions. You will often hear this referred to as a shuffle 
        whereby Spark will exchange partitions across the cluster. With narrow transformations, Spark 
        will automatically perform an operation called 'pipelining', meaning that if we specify 
        multiple filters on DataFrames, they’ll all be performed in-memory. The same cannot be said 
        for shuffles. When we perform a shuffle, Spark writes the results to disk.



- Lazy Evaluation

    - Lazy evaulation means that Spark will wait until the very last moment to execute the graph of
        computation instructions. In Spark, instead of modifying the data immediately when you express 
        some operation, you build up a plan of transformations that you would like to apply to your 
        source data. 

    - By waiting until the last minute to execute the code, Spark compiles this plan from your raw 
        DataFrame transformations to a streamlined physical plan that will run as efficiently as 
        possible across the cluster. This provides immense benefits because Spark can optimize the 
        entire data flow from end to end. An example of this is something called predicate pushdown on
        DataFrames. If we build a large Spark job but specify a filter at the end that only requires us 
        to fetch one row from our source data, the most efficient way to execute this is to access 
        the single record that we need. Spark will actually optimize this for us by pushing the filter 
        down automatically.



- Actions

    - Transformations allow us to build up our logical transformation plan. To trigger the 
        computation, we run an action. An action instructs Spark to compute a result from a series of
        transformations. The simplest action is count, which gives us the total number of records in 
        the DataFrame:

        # This action causes all computations to execute
        >>> divisBy2.count()


    - The output of the preceding code should be 500. Of course, count is not the only action. There 
        are three kinds of actions:

        - Actions to view data in the console
        - Actions to collect data to native objects in the respective language
        - Actions to write to output data sources


    - In specifying this action, we started a Spark job that: 

        - runs our filter transformation (a narrow transformation)
        - then an aggregation (a wide transformation) that performs the counts on a per partition basis
        - and then a collect, which brings our result to a native object in the respective language

      You can see all of this by inspecting the Spark UI, a tool included in Spark with which you can 
        monitor the Spark jobs running on a cluster.



- Spark UI

    - You can monitor the progress of a job through the Spark web UI. The Spark UI is available on 
        port 4040 of the driver node. If you are running in local mode, this will be 
        http://localhost:4040. 

    - The Spark UI displays information on the state of your Spark jobs, its environment, and cluster 
        state. It’s very useful, especially for tuning and debugging.



- End-to-End Example

    - In this chapter, we create a more realistic example, and explain step by step what is
        happening under the hood.


    - We're using a csv file of flight data.

        # Look at csv data
        head /data/flight-data/csv/2015-summary.csv


    - Spark includes the ability to read and write from a large number of data sources.  To read
        this data, we will use a DataFrameReader that is associated with our SparkSession. In doing 
        so, we will specify the file format as well as any options we want to specify. 

      In our case, we want to do something called schema inference, which means that we want Spark to 
        take a best guess at what the schema of our DataFrame should be. We also want to specify that 
        the first row is the header in the file, so we’ll specify that as an option, too.

      To get the schema information, Spark reads in a little bit of the data and then attempts to 
        parse the types in those rows according to the types available in Spark. You also have the 
        option of strictly specifying a schema when you read in data (which we recommend in 
        production scenarios.


        # Read the csv file
        flightData2015 = spark.read\
                              .option("inferSchema", "true")\
                              .opttion("header", "true")\
                              .csv("data/flight-data/csv/2015-summary.csv")


    - Each of these DataFrames have a set of columns with an unspecified number of rows.  The 
        number of rows is unspecified, because reading data is a transformation, and is therefore
        a lazy operation.  Spark peeked at a couple of rows to try to guess what types each column
        should be, but did not read the entire file.


        # Read the first 5 rows, like we did from the command line with 'head'
        >>> flightData2015.take(5)


    - Now, we'll apply some transformations.

        # Get the explain plan for sorting by the count column
        >>> flightData2015.sort("count").explain()


      Explain plans can be read from bottom to top, with the bottom being the source of the data,
        and the top being the end result.


    - Now, we'll specify an action to cause the plan to be executed.  First, we'll set a
        configuration.  By default, when we perform a shuffle, Spark outputs 200 shuffle 
        partitions.  Let's set this value to 5 to reduce the number of output partitions from
        the shuffle.


        # Set the shuffle partitions
        >>> spark.conf.set("spark.sql.shuffle.partitions", "5")

        # Execute the transformations
        >>> flightData2015.sort("count").take(2)


    - The logical plan of transformations that we build up defines a lineage for the DataFrame so 
        that at any given point in time, Spark knows how to recompute any partition by performing 
        all of the operations it had before on the same input data. This sits at the heart of Spark’s
        programming model—functional programming where the same inputs always result in the same 
        outputs when the transformations on that data stay constant.

      We do not manipulate the physical data; instead, we configure physical execution 
        characteristics through things like the shuffle partitions parameter that we set a few moments 
        ago.  We ended up with five output partitions because that’s the value we specified in the 
        shuffle partition. 

      You can change this to help control the physical execution characteristics of your Spark jobs. 
        Go ahead and experiment with different values and see the number of partitions yourself. In
        experimenting with different values, you should see drastically different runtimes. 

      Remember that you can monitor the job progress by navigating to the Spark UI on port 4040 to 
        see the physical and logical execution characteristics of your jobs.



- DataFrames and SQL

    - Now, we'll work through a more complex example, and follow along in both DataFrames and
        SQL.  Spark can run the same transformations, regardless of the language, in the exact
        same way.  You can express your business logic in SQL or in DataFrames (in R, Python,
        Scala, or Java) and Spark will compile that logic down to an underlying plan before
        executing your code.

      With Spark SQL, you can register any DataFrame as a table or view.  With Spark SQL, you can 
        register any DataFrame as a table or view (a temporary table) and query it using pure SQL. There is 
        no performance difference between writing SQL queries or writing DataFrame code, they both “compile” 
        to the same underlying plan that we specify in DataFrame code.


    -  You can make any DataFrame into a table or view with one simple method call:

       # Create a table from a DataFrame
       >>> flightData2015.createOrReplaceTempView("flight_data_2015")


       Now we can query our data in SQL. To do so, we’ll use the spark.sql function (remember, spark is our 
         SparkSession variable) that conveniently returns a new DataFrame. Although this might seem a bit 
         circular in logic—that a SQL query against a DataFrame returns another DataFrame—it’s actually quite 
         powerful. This makes it possible for you to specify transformations in the manner most convenient to 
         you at any given point in time and not sacrifice any efficiency to do so! To understand that this is 
         happening, let’s take a look at two explain plans:


       >>> sqlWay = spark.sql("""
                              SELECT DEST_COUNTRY_NAME, count(1)
                              FROM flight_data_2015
                              GROUP BY DEST_COUNTRY_NAME
                              """)

       >>> dataFrameWay = flightData2015.groupBy("DEST_COUNTRY_NAME").count()
       >>> sqlWay.explain()
       >>> dataFrameWay.explain()

       == Physical Plan ==
       *HashAggregate(keys=[DEST_COUNTRY_NAME#182], functions=[count(1)])
       +- Exchange hashpartitioning(DEST_COUNTRY_NAME#182, 5)
          +- *HashAggregate(keys=[DEST_COUNTRY_NAME#182], functions=[partial_count(1)])
             +- *FileScan csv [DEST_COUNTRY_NAME#182] ...
       == Physical Plan ==
       *HashAggregate(keys=[DEST_COUNTRY_NAME#182], functions=[count(1)])
       +- Exchange hashpartitioning(DEST_COUNTRY_NAME#182, 5)
          +- *HashAggregate(keys=[DEST_COUNTRY_NAME#182], functions=[partial_count(1)])
             +- *FileScan csv [DEST_COUNTRY_NAME#182] ...


       Notice that these plans compile to the exact same underlying plan!



- Summary Statistics From SQL

    - Let’s pull out some interesting statistics from our data. One thing to understand is that DataFrames 
        (and SQL) in Spark already have a huge number of manipulations available. There are hundreds of 
        functions that you can use and import to help you resolve your big data problems faster. 

      We will use the 'max' function, to establish the maximum number of flights to and from any given 
        location. This just scans each value in the relevant column in the DataFrame and checks whether 
        it’s greater than the previous values that have been seen. This is a transformation, because we 
        are effectively filtering down to one row.


      # SQL query
      >>> spark.sql("SELECT max(count) from flight_data_2015").take(1)

      # In Python
      >>> from pyspark.sql.functions import max
      >>> flightData2015.select(max("count")).take(1)


    - Great, that’s a simple example that gives a result of 370,002. Let’s perform something a bit more 
        complicated and find the top five destination countries in the data. This is our first multi-
        transformation query, so we’ll take it step by step. Let’s begin with a fairly straightforward 
        SQL aggregation.


      # in Python
      >>> maxSql = spark.sql("""
                             SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
                             FROM flight_data_2015
                             GROUP BY DEST_COUNTRY_NAME
                             ORDER BY sum(count) DESC
                             LIMIT 5
                             """)

      >>> maxSql.show()

      +-----------------+-----------------+
      |DEST_COUNTRY_NAME|destination_total|
      +-----------------+-----------------+
      |    United States|           411352|
      |           Canada|             8399|
      |           Mexico|             7140|
      |   United Kingdom|             2025|
      |            Japan|             1548|
      +-----------------+-----------------+


    - Now, let’s move to the DataFrame syntax that is semantically similar but slightly different in 
        implementation and ordering. But, as we mentioned, the underlying plans for both of them are the 
        same. Let’s run the queries and see their results as a sanity check:


      >>> from pyspark.sql.functions import desc

      >>> flightData2015\
                .groupBy("DEST_COUNTRY_NAME")\
                .sum("count")\
                .withColumnRenamed("sum(count)", "destination_total")\
                .sort(desc("destination_total"))\
                .limit(5)\
                .show()

      +-----------------+-----------------+
      |DEST_COUNTRY_NAME|destination_total|
      +-----------------+-----------------+
      |    United States|           411352|
      |           Canada|             8399|
      |           Mexico|             7140|
      |   United Kingdom|             2025|
      |            Japan|             1548|
      +-----------------+-----------------+


    - Now there are seven steps that take us all the way back to the source data. You can see this in the 
        explain plan on those DataFrames. This execution plan is a directed acyclic graph (DAG) of 
        transformations, each resulting in a new immutable DataFrame, on which we call an action to 
        generate a result.


      1. The first step is to read in the data. We defined the DataFrame previously but, as a reminder, 
           Spark does not actually read it in until an action is called on that DataFrame or one derived 
           from the original DataFrame.

      2. The second step is our grouping; technically when we call groupBy, we end up with a 
           RelationalGroupedDataset, which is a fancy name for a DataFrame that has a grouping specified 
           but needs the user to specify an aggregation before it can be queried further. We basically 
           specified that we’re going to be grouping by a key (or set of keys) and that now we’re going to 
           perform an aggregation over each one of those keys.

      3. Therefore, the third step is to specify the aggregation. Let’s use the sum aggregation method. 
           This takes as input a column expression or, simply, a column name. The result of the sum method 
           call is a new DataFrame. You’ll see that it has a new schema but that it does know the type of 
           each column. It’s important to reinforce (again!) that no computation has been performed. This 
           is simply another transformation that we’ve expressed, and Spark is simply able to trace our 
           type information through it.

      4. The fourth step is a simple renaming. We use the withColumnRenamed method that takes two arguments, 
           the original column name and the new column name. Of course, this doesn’t perform computation: 
           this is just another transformation!

      5. The fifth step sorts the data such that if we were to take results off of the top of the DataFrame, 
           they would have the largest values in the destination_total column.


    - You likely noticed that we had to import a function to do this, the desc function. You might also have 
        noticed that desc does not return a string but a Column. In general, many DataFrame methods will 
        accept strings (as column names) or Column types or expressions. Columns and expressions are actually 
        the exact same thing.



    - Penultimately, we’ll specify a limit. This just specifies that we only want to return the first five 
        values in our final DataFrame instead of all the data.

      The last step is our action! Now we actually begin the process of collecting the results of our DataFrame, 
        and Spark will give us back a list or array in the language that we’re executing. To reinforce all of 
        this, let’s look at the explain plan for the previous query:


        >>> flightData2015\
                    .groupBy("DEST_COUNTRY_NAME")\
                    .sum("count")\
                    .withColumnRenamed("sum(count)", "destination_total")\
                    .sort(desc("destination_total"))\
                    .limit(5)\
                    .explain()

        == Physical Plan ==
        TakeOrderedAndProject(limit=5, orderBy=[destination_total#16194L DESC], outpu...
        +- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[sum(count#7325L)])
           +- Exchange hashpartitioning(DEST_COUNTRY_NAME#7323, 5)
              +- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[partial_sum...
                 +- InMemoryTableScan [DEST_COUNTRY_NAME#7323, count#7325L]
                       +- InMemoryRelation [DEST_COUNTRY_NAME#7323, ORIGIN_COUNTRY_NA...
                             +- *Scan csv [DEST_COUNTRY_NAME#7578,ORIGIN_COUNTRY_NAME...


        Although this explain plan doesn’t match our exact “conceptual plan,” all of the pieces are there. You 
          can see the limit statement as well as the orderBy (in the first line). You can also see how our 
          aggregation happens in two phases, in the partial_sum calls. This is because summing a list of numbers is 
          commutative, and Spark can perform the sum, partition by partition. Of course we can see how we read in 
          the DataFrame, as well.

        Naturally, we don’t always need to collect the data. We can also write it out to any data source that Spark 
          supports. For instance, suppose we want to store the information in a database like PostgreSQL or write 
          them out to another file.