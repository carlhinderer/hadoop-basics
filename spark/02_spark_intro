---------------------------------------------------------------------------
CHAPTER 2 - SPARK INTRO
---------------------------------------------------------------------------

- Spark Applications

    - Spark Applications consist of a driver process and a set of executor processes. The driver 
        process runs your main() function, sits on a node in the cluster, and is responsible for 
        three things: 

        - maintaining information about the Spark Application
        - responding to a user’s program or input
        - analyzing, distributing, and scheduling work across the executors 


    - The driver process is absolutely essential—it’s the heart of a Spark Application and maintains 
        all relevant information during the lifetime of the application.


    - The executors are responsible for actually carrying out the work that the driver assigns them. 
        This means that each executor is responsible for only two things: executing code assigned 
        to it by the driver, and reporting the state of the computation on that executor back to the 
        driver node.


    - The cluster manager can be one of three core cluster managers: Spark’s standalone cluster manager,
        YARN, or Mesos. This means that there can be multiple Spark Applications running on a cluster 
        at the same time.  The user can specify how many executors should fall on each node through 
        configurations.


        Driver Process             Executors
        ---------------
        |Spark Session|\     ------
        |  ^    ^     | \---|    |
        |  |    |     |  \  |    |
        |  v    v     |   \ ------    ------
        |User Code    |    \  ^       |    |
        ---------------     \-|-------|    |     ------
          ^    ^             \|       ------     |    |
          |    |              |\---------^-------|    |
          |    |              |          |       ------
          v    v              v          v          |
        ------------------------------------------------------------  
        |               Cluster Manager                            |
        ------------------------------------------------------------


    - Note that Spark, in addition to its cluster mode, also has a local mode. The driver and executors 
        are simply processes, which means that they can live on the same machine or different machines. 
        In local mode, the driver and executurs run (as threads) on your individual computer instead of 
        a cluster. We wrote this book with local mode in mind, so you should be able to run everything 
        on a single machine.


    - Here are the key points to understand about Spark Applications at this point:

        - Spark employs a cluster manager that keeps track of the resources available.

        - The driver process is responsible for executing the driver program’s commands across the 
            executors to complete a given task.

        - The executors, for the most part, will always be running Spark code. However, the driver can 
            be “driven” from a number of different languages through Spark’s language APIs. Let’s take 
            a look at those in the next section.



- The Spark Session

    - A Spark application is controlled through a driver process called the 'SparkSession'.  The
        SparkSession instance is the way Spark executes user-defined manipulations across the 
        cluster.  There is a one-to-one correspondence between a SparkSession and Spark application.


    # Start pyspark
    pyspark

    # Get the spark session
    >>> spark

    # Create a DataFrame with 1 column, containing 1000 rows with values 0-999.
    #   This is a distributed collection.  When run on a cluster, each part of this range of
    #   number exists on a different executor.
    >>> myRange = spark.range(1000).toDF("number")



- DataFrames

    - A DataFrame is the most common Structured API and simply represents a table of data with rows 
        and columns. The list that defines the columns and the types within those columns is called 
        the schema. You can think of a DataFrame as a spreadsheet with named columns. 

    - Unlike Python/R DataFrames, Spark DataFrames exist on multiple machines.  However, because Spark 
        has language interfaces for both Python and R, it’s quite easy to convert Pandas (Python) 
        DataFrames to Spark DataFrames, and R DataFrames to Spark DataFrames.



- Partitions

    - To allow every executor to perform work in parallel, Spark breaks up the data into chunks 
        called partitions.  A partition is a collection of rows that sit on one physical machine in 
        your cluster.  A DataFrame’s partitions represent how the data is physically distributed 
        across the cluster of machines during execution. 

    - If you have one partition, Spark will have a parallelism of only one, even if you have 
        thousands of executors. If you have many partitions but only one executor, Spark will still 
        have a parallelism of only one because there is only one computation resource.

    - An important thing to note is that with DataFrames you do not (for the most part) manipulate
        partitions manually or individually. You simply specify high-level transformations of data in 
        the physical partitions, and Spark determines how this work will actually execute on the 
        cluster. Lower-level APIs do exist (via the RDD interface), but are used less often.