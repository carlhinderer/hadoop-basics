---------------------------------------------------------------------------
CHAPTER 2 - SPARK INTRO
---------------------------------------------------------------------------

- Spark Applications

    - Spark Applications consist of a driver process and a set of executor processes. The driver 
        process runs your main() function, sits on a node in the cluster, and is responsible for 
        three things: 

        - maintaining information about the Spark Application
        - responding to a user’s program or input
        - analyzing, distributing, and scheduling work across the executors 


    - The driver process is absolutely essential—it’s the heart of a Spark Application and maintains 
        all relevant information during the lifetime of the application.


    - The executors are responsible for actually carrying out the work that the driver assigns them. 
        This means that each executor is responsible for only two things: executing code assigned 
        to it by the driver, and reporting the state of the computation on that executor back to the 
        driver node.


    - The cluster manager can be one of three core cluster managers: Spark’s standalone cluster manager,
        YARN, or Mesos. This means that there can be multiple Spark Applications running on a cluster 
        at the same time.  The user can specify how many executors should fall on each node through 
        configurations.


        Driver Process             Executors
        ---------------
        |Spark Session|\     ------
        |  ^    ^     | \---|    |
        |  |    |     |  \  |    |
        |  v    v     |   \ ------    ------
        |User Code    |    \  ^       |    |
        ---------------     \-|-------|    |     ------
          ^    ^             \|       ------     |    |
          |    |              |\---------^-------|    |
          |    |              |          |       ------
          v    v              v          v          |
        ------------------------------------------------------------  
        |               Cluster Manager                            |
        ------------------------------------------------------------


    - Note that Spark, in addition to its cluster mode, also has a local mode. The driver and executors 
        are simply processes, which means that they can live on the same machine or different machines. 
        In local mode, the driver and executurs run (as threads) on your individual computer instead of 
        a cluster. We wrote this book with local mode in mind, so you should be able to run everything 
        on a single machine.


    - Here are the key points to understand about Spark Applications at this point:

        - Spark employs a cluster manager that keeps track of the resources available.

        - The driver process is responsible for executing the driver program’s commands across the 
            executors to complete a given task.

        - The executors, for the most part, will always be running Spark code. However, the driver can 
            be “driven” from a number of different languages through Spark’s language APIs. Let’s take 
            a look at those in the next section.



- The Spark Session

    - A Spark application is controlled through a driver process called the 'SparkSession'.  The
        SparkSession instance is the way Spark executes user-defined manipulations across the 
        cluster.  There is a one-to-one correspondence between a SparkSession and Spark application.


    # Start pyspark
    pyspark

    # Get the spark session
    >>> spark

    # Create a DataFrame with 1 column, containing 1000 rows with values 0-999.
    #   This is a distributed collection.  When run on a cluster, each part of this range of
    #   number exists on a different executor.
    >>> myRange = spark.range(1000).toDF("number")



- DataFrames

    - A DataFrame is the most common Structured API and simply represents a table of data with rows 
        and columns. The list that defines the columns and the types within those columns is called 
        the schema. You can think of a DataFrame as a spreadsheet with named columns. 

    - Unlike Python/R DataFrames, Spark DataFrames exist on multiple machines.  However, because Spark 
        has language interfaces for both Python and R, it’s quite easy to convert Pandas (Python) 
        DataFrames to Spark DataFrames, and R DataFrames to Spark DataFrames.



- Partitions

    - To allow every executor to perform work in parallel, Spark breaks up the data into chunks 
        called partitions.  A partition is a collection of rows that sit on one physical machine in 
        your cluster.  A DataFrame’s partitions represent how the data is physically distributed 
        across the cluster of machines during execution. 

    - If you have one partition, Spark will have a parallelism of only one, even if you have 
        thousands of executors. If you have many partitions but only one executor, Spark will still 
        have a parallelism of only one because there is only one computation resource.

    - An important thing to note is that with DataFrames you do not (for the most part) manipulate
        partitions manually or individually. You simply specify high-level transformations of data in 
        the physical partitions, and Spark determines how this work will actually execute on the 
        cluster. Lower-level APIs do exist (via the RDD interface), but are used less often.



- Transformations

    - In Spark, the core data structures are immutable.  To change a DataFrame, you need to apply
        transformations.

        # Find all even numbers in our DataFrame
        divisBy2 = myRange.where("number % 2 == 0")


    - Notice that these transformations return no output.  This is because we specified only an 
        abstract transformation, and Spark will not act on transformations until we call an action. 
        Transformations are the core of how you express your business logic using Spark. There are 
        two types of transformations: those that specify 'narrow dependencies', and those that specify 
        'wide dependencies'.

    - Transformations consisting of narrow dependencies (we’ll call them narrow transformations) are 
        those for which each input partition will contribute to only one output partition. In the 
        preceding code snippet, the where statement specifies a narrow dependency, where only one 
        partition contributes to at most one output partition.

    - A wide dependency (or wide transformation) style transformation will have input partitions
        contributing to many output partitions. You will often hear this referred to as a shuffle 
        whereby Spark will exchange partitions across the cluster. With narrow transformations, Spark 
        will automatically perform an operation called 'pipelining', meaning that if we specify 
        multiple filters on DataFrames, they’ll all be performed in-memory. The same cannot be said 
        for shuffles. When we perform a shuffle, Spark writes the results to disk.



- Lazy Evaluation

    - Lazy evaulation means that Spark will wait until the very last moment to execute the graph of
        computation instructions. In Spark, instead of modifying the data immediately when you express 
        some operation, you build up a plan of transformations that you would like to apply to your 
        source data. 

    - By waiting until the last minute to execute the code, Spark compiles this plan from your raw 
        DataFrame transformations to a streamlined physical plan that will run as efficiently as 
        possible across the cluster. This provides immense benefits because Spark can optimize the 
        entire data flow from end to end. An example of this is something called predicate pushdown on
        DataFrames. If we build a large Spark job but specify a filter at the end that only requires us 
        to fetch one row from our source data, the most efficient way to execute this is to access 
        the single record that we need. Spark will actually optimize this for us by pushing the filter 
        down automatically.



- Actions

    - Transformations allow us to build up our logical transformation plan. To trigger the 
        computation, we run an action. An action instructs Spark to compute a result from a series of
        transformations. The simplest action is count, which gives us the total number of records in 
        the DataFrame:

        # This action causes all computations to execute
        >>> divisBy2.count()


    - The output of the preceding code should be 500. Of course, count is not the only action. There 
        are three kinds of actions:

        - Actions to view data in the console
        - Actions to collect data to native objects in the respective language
        - Actions to write to output data sources


    - In specifying this action, we started a Spark job that: 

        - runs our filter transformation (a narrow transformation)
        - then an aggregation (a wide transformation) that performs the counts on a per partition basis
        - and then a collect, which brings our result to a native object in the respective language

      You can see all of this by inspecting the Spark UI, a tool included in Spark with which you can 
        monitor the Spark jobs running on a cluster.



- Spark UI

    - You can monitor the progress of a job through the Spark web UI. The Spark UI is available on 
        port 4040 of the driver node. If you are running in local mode, this will be 
        http://localhost:4040. 

    - The Spark UI displays information on the state of your Spark jobs, its environment, and cluster 
        state. It’s very useful, especially for tuning and debugging.



- End-to-End Example

    - In this chapter, we create a more realistic example, and explain step by step what is
        happening under the hood.


    - We're using a csv file of flight data.

        # Look at csv data
        head /data/flight-data/csv/2015-summary.csv


    - Spark includes the ability to read and write from a large number of data sources.  To read
        this data, we will use a DataFrameReader that is associated with our SparkSession. In doing 
        so, we will specify the file format as well as any options we want to specify. 

      In our case, we want to do something called schema inference, which means that we want Spark to 
        take a best guess at what the schema of our DataFrame should be. We also want to specify that 
        the first row is the header in the file, so we’ll specify that as an option, too.

      To get the schema information, Spark reads in a little bit of the data and then attempts to 
        parse the types in those rows according to the types available in Spark. You also have the 
        option of strictly specifying a schema when you read in data (which we recommend in 
        production scenarios.


        # Read the csv file
        flightData2015 = spark.read\
                              .option("inferSchema", "true")\
                              .opttion("header", "true")\
                              .csv("data/flight-data/csv/2015-summary.csv")


    - Each of these DataFrames have a set of columns with an unspecified number of rows.  The 
        number of rows is unspecified, because reading data is a transformation, and is therefore
        a lazy operation.  Spark peeked at a couple of rows to try to guess what types each column
        should be, but did not read the entire file.


        # Read the first 5 rows, like we did from the command line with 'head'
        >>> flightData2015.take(5)


    - Now, we'll apply some transformations.

        # Get the explain plan for sorting by the count column
        >>> flightData2015.sort("count").explain()


      Explain plans can be read from bottom to top, with the bottom being the source of the data,
        and the top being the end result.


    - Now, we'll specify an action to cause the plan to be executed.  First, we'll set a
        configuration.  By default, when we perform a shuffle, Spark outputs 200 shuffle 
        partitions.  Let's set this value to 5 to reduce the number of output partitions from
        the shuffle.


        # Set the shuffle partitions
        >>> spark.conf.set("spark.sql.shuffle.partitions", "5")

        # Execute the transformations
        >>> flightData2015.sort("count").take(2)


    - The logical plan of transformations that we build up defines a lineage for the DataFrame so 
        that at any given point in time, Spark knows how to recompute any partition by performing 
        all of the operations it had before on the same input data. This sits at the heart of Spark’s
        programming model—functional programming where the same inputs always result in the same 
        outputs when the transformations on that data stay constant.

      We do not manipulate the physical data; instead, we configure physical execution 
        characteristics through things like the shuffle partitions parameter that we set a few moments 
        ago.  We ended up with five output partitions because that’s the value we specified in the 
        shuffle partition. 

      You can change this to help control the physical execution characteristics of your Spark jobs. 
        Go ahead and experiment with different values and see the number of partitions yourself. In
        experimenting with different values, you should see drastically different runtimes. 

      Remember that you can monitor the job progress by navigating to the Spark UI on port 4040 to 
        see the physical and logical execution characteristics of your jobs.