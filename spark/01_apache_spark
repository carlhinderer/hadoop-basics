---------------------------------------------------------------------------
CHAPTER 1 - APACHE SPARK
---------------------------------------------------------------------------

- Apache Spark

    - Spark is a unified computing engine and set of libraries for parallel data processing
        on computer clusters.  It is the most actively developed open source engine for
        this task, making it the standard tool for any developer or data scientist interested
        in big data.

    - Spark supports the Python, Java, Scala, and R programming languages and includes 
        libraries for diverse tasks ranging from SQL to streaming to machine learning.



- Spark Components

    Structured Streaming
    Advanced Analytics
    Libraries and Ecosystems

    [Structured APIs]
    Datasets
    DataFrames
    SQL

    [Low-level APIs]
    RDDs
    Distributed Variables



- Spark's Philosophy

    - Unified 

        - Designed to support a wide range of data analytics tasks, ranging from data loading to SQL 
            queries to machine learning and streaming computation, over the same computing engine 
            with a consistent set of APIs

        - Designed to enable high performance by optimizing across the different libraries and 
            functions composed together in a user program.  For example, if you load data using a SQL 
            query and then evaluate a machine learning model over it, the engine can combine these 
            steps into one scan over the data.

    - Computing Engine

        - Spark carefully limits its scope to a computing engine.  By this, we mean that Spark handles
            loading data from storage systems and performing computation on it, not permanent storage.
            Spark can be used with cloud storage systems like Azure Storage and S3, distributed
            file systems like Hadoop, key-value stores like Cassandra, and message buses like Kafka.

        - Data is expensive to move, so Spark focuses on performing computations over the data, no
            matter where it resides.

    - Libraries

        - Spark's libraries build on its design as a unified engine to provide a unified API for
            common data analytics tasks.  Spark includes libraries for SQL and structured data
            (Spark SQL), machine learning (MLib), stream processing (Spark Streaming and Structured
            Streaming), and graph analytics (GraphX).

        - In addition to the standard libraries, there are hundreds of open source external libraries
            rangin from connectors for various storage systems to machine learning algorithms.



- The Big Data Problem

    - For most of their history, computers became faster every year through processor speed increases.
        As a result, applications automatically became faster every year without code rewrites.
        Unfortunately, this trend in hardware stopped around 2005 due to hard limits in heat
        dissipation.  Hardware developers switched to adding more parallel CPU cores all running
        at the same speed.  This meant that suddenly applications needed to be modified to add
        parallelism in order to run faster.

    - Also, the technologies for storing down and collecting data did not slow down.  The cost to
        store 1 TB of data continues to half every 14 months.  Moreover, the technologies for
        collecting data (sensors, cameras, public datasets, etc.) continue to drop in cost and 
        improve in resolution.

    - The end result is a world in which collecting data is extremely cheap - most organizations
        now consider it negligent not to collect data that may possibly advance the business.  
        However processing it requires large, parallel computations on clusters of machines.
        Moreover, in this new world, software developed during the last 50 years cannot
        automatically scale up, and neither can traditional programming models for data processing
        applications.  Creating new models is what Apache Spark was built for.



- History of Spark

    - Started in UC Berkeley AMPlab in 2009 as a research project.  At the time Hadoop MapReduce was 
        the dominant parallel programming engine for clusters.  AMPlab worked with Hadoop users to
        understand the benefits and drawbacks of this new model.

    - Two things became clear.  First, cluster computing held tremendous potential.  Second, the 
        MapReduce engine made it both challenging and inefficient to build large applications.  For
        instance, the typical machine learning algorithm might need to make 10 or 20 passes over
        the data.  In MapReduce, each pass had to be written as a separate MapReduce job, which had
        to be launched separately on the cluster and load the data from scratch.

    - To address this problem, the Spark team first designed an API based on functional programming
        that could succinctly express multistep applications.  The team then implemented this API
        over a new engine that could perform efficient, in-memory data sharing across computation
        steps.

    - The first version of Spark supported only batch applications, but soon enough the need for
        interactive data science and ad hoc queries became clear.  By simply plugging the Scala
        interpreter into Spark, the project provides a highly usable interactive system for 
        running queries on hundreds of machines.  Shark (an engine that could run SQL queries over
        Spark) was released in 2011.

    - Since the initial releases, most work has been done on adding additional libraries.  Different
        APMlab teams have started MLlib, Spark Streaming, and GraphX.  They have ensured that
        these APIs would be highly interoperable.

    - In 2013, Spark was given to the Apache Software Foundation.  The early AMPlab team launched
        DataBricks.