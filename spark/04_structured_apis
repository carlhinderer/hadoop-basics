---------------------------------------------------------------------------
CHAPTER 4 - STRUCTURED API OVERVIEW
---------------------------------------------------------------------------

- Fundamental Concepts

    - Spark is a distributed programming model in which the user specifies transformation.

    - Multiple transformations build up a DAG of instructions.

    - An action begins the process of executing that graph of instructions, as a single job,
        by breaking it down into stages and tasks to execute across the cluster.

    - The logical structures that we manipulate with transformations and actions are 'DataFrames'
        and 'Datasets'.

    - To create a new DataFrame or Dataset, you call a transformation.

    - To start computation or convert to native language types, you call an action.



- DataFrames and Datasets

    - DataFrames and Datasets are table-like collections with well-defined rows and columns.  Each
        column must have the same number of rows as all the other columns, and each column has
        type information that must be consistent for every row in the collection. 


    - To Spark, DataFrames and Datasets represent immutable, lazily evaluated plans that specify 
        what operations to apply to data residing at a location to generate some output. When we 
        perform an action on a DataFrame, we instruct Spark to perform the actual transformations and 
        return the result. These represent plans of how to manipulate rows and columns to compute the 
        user’s desired result.


    - A schema defines the column names and types of a DataFrame.  You can define schemas manually or
        read a schema from a data source ('schema on read').  Schemas consist of types.



- Structured Spark Types

    - Spark is effectively a programming language of its own. Internally, Spark uses an engine called 
        'Catalyst' that maintains its own type information through the planning and processing of work. 
        In doing so, this opens up a wide variety of execution optimizations that make significant
        differences. 


    - Spark types map directly to the different language APIs that Spark maintains and there exists a 
        lookup table for each of these in Scala, Java, Python, SQL, and R. Even if we use Spark’s 
        Structured APIs from Python or R, the majority of our manipulations will operate strictly on 
        Spark types, not Python types. For example, the following code does not perform addition in 
        Python; it actually performs addition purely in Spark:


      >>> df = spark.range(500).toDF("number")
      >>> df.select(df["number"] + 10)


      This addition operation happens because Spark will convert an expression written in an input 
        language to Spark’s internal Catalyst representation of that same type information. It then 
        will operate on that internal representation. We touch on why this is the case momentarily, but
        before we can, we need to discuss Datasets.



- DataFrames vs Datasets

    - In essence, within the Structured APIs, there are two more APIs, the “untyped” DataFrames and the
        “typed” Datasets. To say that DataFrames are untyped is aslightly inaccurate; they have types, 
        but Spark maintains them completely and only checks whether those types line up to those 
        specified in the schema at runtime. 

      Datasets, on the other hand, check whether types conform to the specification at compile time. 
        Datasets are only available to JVM–based languages (Scala and Java) and we specify types with 
        case classes or Java beans.


    - For the most part, you’re likely to work with DataFrames. To Spark (in Scala), DataFrames are 
        simply Datasets of Type Row. The “Row” type is Spark’s internal representation of its optimized
        in-memory format for computation. 

      This format makes for highly specialized and efficient computation because rather than using JVM 
        types, which can cause high garbage-collection and object instantiation costs, Spark can operate 
        on its own internal format without incurring any of those costs. To Spark (in Python or R), 
        there is no such thing as a Dataset: everything is a DataFrame and therefore we always operate 
        on that optimized format.

      Understanding DataFrames, Spark Types, and Schemas takes some time to digest. What you need to 
        know is that when you’re using DataFrames, you’re taking advantage of Spark’s optimized internal
        format. This format applies the same efficiency gains to all of Spark’s language APIs.



- Columns

    - Columns represent a simple type like an integer or string, a complex type like an array or map, 
        or a null value.  Spark tracks all of this type information for you and offers a variety of ways, with which you can transform columns.  For the most part you can think about Spark Column types as columns in a table.



- Rows

    - A row is nothing more than a record of data. Each record in a DataFrame must be of type Row, as 
        we can see when we collect the following DataFrames. We can create these rows manually from SQL,
        from Resilient Distributed Datasets (RDDs), from data sources, or manually from scratch. Here, 
        we create one by using a range:


        >>> spark.range(2).collect()


      This results in an array of Row objects.



- Spark Types

    - We mentioned earlier that Spark has a large number of internal type representations. We include a
        handy reference table on the next several pages so that you can most easily reference what type, 
        in your specific language, lines up with the type in Spark.


    - To work with the correct Scala types, use the following:

        >>> import org.apache.spark.sql.types._
        >>> val b = ByteType


    - To work with the correct Java types, you should use the factory methods in the following package:

        >>> import org.apache.spark.sql.types.DataTypes;
        >>> ByteType x = DataTypes.ByteType;


    - To work with the correct Python types, use the following:

        >>> from pyspark.sql.types import *
        >>> b = ByteType()



- List of Spark's Python Bindings


Data type:   ByteType
Python type: int or long. Note: Numbers will be converted to 1-byte signed integer numbers at
                        runtime. Ensure that numbers are within the range of –128 to 127.
API:         ByteType()


Data type:   ShortType
Python type: int or long. Note: Numbers will be converted to 2-byte signed integer numbers at runtime. 
               Ensure that numbers are within the range of –32768 to 32767.
API:         ShortType()


Data type:   IntegerType
Python type: int or long. Note: Python has a lenient definition of “integer.” Numbers that are too 
               large will be rejected by Spark SQL if you use the IntegerType(). It’s best practice 
               to use LongType.
API:         IntegerType()


Data type:   LongType
Python type: long. Note: Numbers will be converted to 8-byte signed integer numbers at runtime. Ensure 
               that numbers are within the range of –9223372036854775808 to 9223372036854775807. 
               Otherwise, convert data to decimal.Decimal and use DecimalType.
API:         LongType()


Data type:   FloatType
Python type: float. Note: Numbers will be converted to 4-byte single-precision floating-point numbers 
               at runtime.
API:         FloatType()


Data type:   DoubleType
Python type: float
API:         DoubleType()


Data type:   DecimalType
Python type: decimal.Decimal
API:         DecimalType()


Data type:   StringType
Python type: string
API:         StringType()


Data type:   BinaryType
Python type: bytearray
API:         BinaryType()


Data type:   BooleanType
Python type: bool
API:         BooleanType()


Data type:   TimestampType
Python type: datetime.datetime
API:         TimestampType()


Data type:   DateType
Python type: datetime.date
API:         DateType()


Data type:   ArrayType
Python type: list, tuple, or array
API:         ArrayType(elementType, [containsNull]). Note: The default value of containsNull is True.


Data type:   MapType
Python type: dict
API:         MapType(keyType, valueType, [valueContainsNull]). Note: The default value of 
               valueContainsNull is True.


Data type:   StructType
Python type: list or tuple
API:         StructType(fields). Note: fields is a list of StructFields. Also, fields with the same 
               name are not allowed.


Data type:   StructField
Python type: The value type in Python of the data type of this field (for example, Int for a StructField
               with the data type IntegerType)
API:         StructField(name, dataType, [nullable]) Note: The default value of nullable is True.



- Structured API Execution

    - Here, we will demonstrate how this code is actually executed across a cluster. This will help 
        you understand (and potentially debug) the process of writing and executing code on clusters, 
        so let’s walk through the execution of a single structured API query from user code to executed
        code. Here’s an overview of the steps:

        1. Write DataFrame/Dataset/SQL Code.

        2. If valid code, Spark converts this to a Logical Plan.

        3. Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along the 
            way.

        4. Spark then executes this Physical Plan (RDD manipulations) on the cluster.


    - To execute code, we must write code. This code is then submitted to Spark either through the 
        console or via a submitted job. This code then passes through the Catalyst Optimizer, which 
        decides how the code should be executed and lays out a plan for doing so before, finally, the 
        code is run and the result is returned to the user. Figure 4-1 shows the process.


           SQL ------------------>
           DataFrames -----------> Catalyst Optimizer -----------> Physical Plan
           DataSets ------------->


    - Logical Planning

        The first phase of execution is meant to take user code and convert it into a logical plan.


          User ----> Unresolved ----> (Analysis) ----> Resolved ----> (Logical       ----> Optimized
          Code       Logical              ^            Logical         Optimization)       Logical
                     Plan                 |            Plan                                Plan
                                       Catalog


        This logical plan only represents a set of abstract transformations that do not refer to 
          executors or drivers, it’s purely to convert the user’s set of expressions into the most 
          optimized version. 

        It does this by converting user code into an unresolved logical plan. This plan is unresolved 
          because although your code might be valid, the tables or columns that it refers to might or 
          might not exist. Spark uses the catalog, a repository of all table and DataFrame information, 
          to resolve columns and tables in the analyzer. The analyzer might reject the unresolved 
          logical plan if the required table or column name does not exist in the catalog. 

        If the analyzer can resolve it, the result is passed through the Catalyst Optimizer, a 
          collection of rules that attempt to optimize the logical plan by pushing down predicates or
          selections. Packages can extend the Catalyst to include their own rules for domain-specific
          optimizations.


    - Physical Planning

        After successfully creating an optimized logical plan, Spark then begins the physical planning
          process. The physical plan, often called a Spark plan, specifies how the logical plan will 
          execute on the cluster by generating different physical execution strategies and comparing 
          them through a cost model. 


          Optimized ----> Physical ----> Cost  ----> Best     ----> Executed
          Logical         Plans          Model       Physical       on the
          Plan                                       Plan           Cluster


        An example of the cost comparison might be choosing how to perform a given join by looking at 
          the physical attributes of a given table (how big the table is or how big its partitions are).

        Physical planning results in a series of RDDs and transformations. This result is why you 
          might have heard Spark referred to as a compiler—it takes queries in DataFrames, Datasets, 
          and SQL and compiles them into RDD transformations for you.


    - Execution

        Upon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level 
          programming interface of Spark. Spark performs further optimizations at runtime, generating 
          native Java bytecode that can remove entire tasks or stages during execution. Finally the 
          result is returned to the user.