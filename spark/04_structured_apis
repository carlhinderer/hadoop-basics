---------------------------------------------------------------------------
CHAPTER 4 - STRUCTURED API OVERVIEW
---------------------------------------------------------------------------

- Fundamental Concepts

    - Spark is a distributed programming model in which the user specifies transformation.

    - Multiple transformations build up a DAG of instructions.

    - An action begins the process of executing that graph of instructions, as a single job,
        by breaking it down into stages and tasks to execute across the cluster.

    - The logical structures that we manipulate with transformations and actions are 'DataFrames'
        and 'Datasets'.

    - To create a new DataFrame or Dataset, you call a transformation.

    - To start computation or convert to native language types, you call an action.



- DataFrames and Datasets

    - DataFrames and Datasets are table-like collections with well-defined rows and columns.  Each
        column must have the same number of rows as all the other columns, and each column has
        type information that must be consistent for every row in the collection. 


    - To Spark, DataFrames and Datasets represent immutable, lazily evaluated plans that specify 
        what operations to apply to data residing at a location to generate some output. When we 
        perform an action on a DataFrame, we instruct Spark to perform the actual transformations and 
        return the result. These represent plans of how to manipulate rows and columns to compute the 
        user’s desired result.


    - A schema defines the column names and types of a DataFrame.  You can define schemas manually or
        read a schema from a data source ('schema on read').  Schemas consist of types.



- Structured Spark Types

    - Spark is effectively a programming language of its own. Internally, Spark uses an engine called 
        'Catalyst' that maintains its own type information through the planning and processing of work. 
        In doing so, this opens up a wide variety of execution optimizations that make significant
        differences. 


    - Spark types map directly to the different language APIs that Spark maintains and there exists a 
        lookup table for each of these in Scala, Java, Python, SQL, and R. Even if we use Spark’s 
        Structured APIs from Python or R, the majority of our manipulations will operate strictly on 
        Spark types, not Python types. For example, the following code does not perform addition in 
        Python; it actually performs addition purely in Spark:


      >>> df = spark.range(500).toDF("number")
      >>> df.select(df["number"] + 10)


      This addition operation happens because Spark will convert an expression written in an input 
        language to Spark’s internal Catalyst representation of that same type information. It then 
        will operate on that internal representation. We touch on why this is the case momentarily, but
        before we can, we need to discuss Datasets.



- DataFrames vs Datasets

    - In essence, within the Structured APIs, there are two more APIs, the “untyped” DataFrames and the
        “typed” Datasets. To say that DataFrames are untyped is aslightly inaccurate; they have types, 
        but Spark maintains them completely and only checks whether those types line up to those 
        specified in the schema at runtime. 

      Datasets, on the other hand, check whether types conform to the specification at compile time. 
        Datasets are only available to JVM–based languages (Scala and Java) and we specify types with 
        case classes or Java beans.


    - For the most part, you’re likely to work with DataFrames. To Spark (in Scala), DataFrames are 
        simply Datasets of Type Row. The “Row” type is Spark’s internal representation of its optimized
        in-memory format for computation. 

      This format makes for highly specialized and efficient computation because rather than using JVM 
        types, which can cause high garbage-collection and object instantiation costs, Spark can operate 
        on its own internal format without incurring any of those costs. To Spark (in Python or R), 
        there is no such thing as a Dataset: everything is a DataFrame and therefore we always operate 
        on that optimized format.

      Understanding DataFrames, Spark Types, and Schemas takes some time to digest. What you need to 
        know is that when you’re using DataFrames, you’re taking advantage of Spark’s optimized internal
        format. This format applies the same efficiency gains to all of Spark’s language APIs.



- Columns

    - Columns represent a simple type like an integer or string, a complex type like an array or map, 
        or a null value.  Spark tracks all of this type information for you and offers a variety of ways, with which you can transform columns.  For the most part you can think about Spark Column types as columns in a table.



- Rows

    - A row is nothing more than a record of data. Each record in a DataFrame must be of type Row, as 
        we can see when we collect the following DataFrames. We can create these rows manually from SQL,
        from Resilient Distributed Datasets (RDDs), from data sources, or manually from scratch. Here, 
        we create one by using a range:


        >>> spark.range(2).collect()


      This results in an array of Row objects.



- Spark Types

    - We mentioned earlier that Spark has a large number of internal type representations. We include a
        handy reference table on the next several pages so that you can most easily reference what type, 
        in your specific language, lines up with the type in Spark.


    - To work with the correct Scala types, use the following:

        >>> import org.apache.spark.sql.types._
        >>> val b = ByteType


    - To work with the correct Java types, you should use the factory methods in the following package:

        >>> import org.apache.spark.sql.types.DataTypes;
        >>> ByteType x = DataTypes.ByteType;


    - To work with the correct Python types, use the following:

        >>> from pyspark.sql.types import *
        >>> b = ByteType()



- List of Spark's Python Bindings


Data type:   ByteType
Python type: int or long. Note: Numbers will be converted to 1-byte signed integer numbers at
                        runtime. Ensure that numbers are within the range of –128 to 127.
API:         ByteType()


Data type:   ShortType
Python type: int or long. Note: Numbers will be converted to 2-byte signed integer numbers at runtime. 
               Ensure that numbers are within the range of –32768 to 32767.
API:         ShortType()


Data type:   IntegerType
Python type: int or long. Note: Python has a lenient definition of “integer.” Numbers that are too 
               large will be rejected by Spark SQL if you use the IntegerType(). It’s best practice 
               to use LongType.
API:         IntegerType()


Data type:   LongType
Python type: long. Note: Numbers will be converted to 8-byte signed integer numbers at runtime. Ensure 
               that numbers are within the range of –9223372036854775808 to 9223372036854775807. 
               Otherwise, convert data to decimal.Decimal and use DecimalType.
API:         LongType()


Data type:   FloatType
Python type: float. Note: Numbers will be converted to 4-byte single-precision floating-point numbers 
               at runtime.
API:         FloatType()


Data type:   DoubleType
Python type: float
API:         DoubleType()


Data type:   DecimalType
Python type: decimal.Decimal
API:         DecimalType()


Data type:   StringType
Python type: string
API:         StringType()


Data type:   BinaryType
Python type: bytearray
API:         BinaryType()


Data type:   BooleanType
Python type: bool
API:         BooleanType()


Data type:   TimestampType
Python type: datetime.datetime
API:         TimestampType()


Data type:   DateType
Python type: datetime.date
API:         DateType()


Data type:   ArrayType
Python type: list, tuple, or array
API:         ArrayType(elementType, [containsNull]). Note: The default value of containsNull is True.


Data type:   MapType
Python type: dict
API:         MapType(keyType, valueType, [valueContainsNull]). Note: The default value of 
               valueContainsNull is True.


Data type:   StructType
Python type: list or tuple
API:         StructType(fields). Note: fields is a list of StructFields. Also, fields with the same 
               name are not allowed.


Data type:   StructField
Python type: The value type in Python of the data type of this field (for example, Int for a StructField
               with the data type IntegerType)
API:         StructField(name, dataType, [nullable]) Note: The default value of nullable is True.