
- Example - Pyspark Console

    # Get the spark session
    >>> spark


    # Create a DataFrame with 1 column and 1000 rows, with values 0-999
    >>> myRange = spark.range(1000).toDF("number")


    # Transformation to get all even numbers in our DataFrame
    >>> divisBy2 = myRange.where("number % 2 = 0")


    # Perform an action to get the series of transformations to execute
    >>> divisBy2.count()



- Example - Flight Data

    # Read flight data with schema inference
    >>> flightData2015 = spark \
                         .read \
                         .option("inferSchema", "true") \
                         .option("header", "true") \
                         .csv('/spark-book/data/flight-data/csv/2015-summary.csv')


    # Look at the first 5 rows of data
    >>> flightData2015.take(5)


    # Run the explain plan on sorting the data by count
    >>> flightData2015.sort("count").explain()


    # Reduce the number of shuffle partitions, default is 200
    >>> spark.conf.set("spark.sql.shuffle.partitions", "5")


    # Sort the data by count and get the lowest 2
    >>> flightData2015.sort("count").take(2)



- Example - SQL Queries

    # Convert our DataFrame into a view so we can run SQL queries
    >>> flightData2015.createOrReplaceTempView("flight_data_2015")


    # If we run a SQL query and a DataFrame transformation, they have the same explain plan!
    >>> sqlWay = spark.sql("""
        SELECT DEST_COUNTRY_NAME, count(1)
        FROM flight_data_2015
        GROUP BY DEST_COUNTRY_NAME
        """)

    >>> dataFrameWay = flightData2015 \
                       .groupby("DEST_COUNTRY_NAME") \
                       .count()

    >>> sqlWay.explain()
    >>> dataFrameWay.explain()


    # DataFrames have lots of built-in manipulations

    # Sql way
    >>> spark.sql("SELECT max(count) from flight_data_2015").take(1)

    # Python way
    >>> from pyspark.sql.functions import max
    >>> flightData2015.select(man("count")).take(1)



- Example - More Advanced SQL

    # Begin with simple aggregation
    >>> maxSql = spark.sql("""
        SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
        FROM flight_data_2015
        GROUP BY DEST_COUNTRY_NAME
        ORDER BY sum(count) DESC
        LIMIT 5
        """)


    # Show the results in the console
    >>> maxSql.show()

    +-----------------+-----------------+
    |DEST_COUNTRY_NAME|destination_total|
    +-----------------+-----------------+
    |    United States|           411352|
    |           Canada|             8399|
    |           Mexico|             7140|
    |   United Kingdom|             2025|
    |            Japan|             1548|
    +-----------------+-----------------+


    # Run the same query with a data frame
    >>> from pyspark.sql.functions import desc

    >>> flightData2015 \
            .groupBy("DEST_COUNTRY_NAME") \
            .sum("count") \
            .withColumnRenamed("sum(count)", "destination_total") \
            .sort(desc("destination_total")) \
            .limit(5) \
            .show()


    Here, we have a DAG of 7 steps in the execution plan.

      1. read = Spark does not actually read data until an action is called on that data.

      2. groupBy = When we called 'groupBy', we end up with a 'RelationalGroupedDataset', which is a 
                     DataFrame that has a grouping specified but needs the user to specify an aggregation
                     before it can be queried further.  Basically, we specified we're going to be
                     grouping by a key, and now we're going to perform an aggregation over each of the
                     keys.

      3. sum = Here, we actually specify that aggregation.  The result of the 'sum' method call is a new
                 DataFrame.

      4. withColumnRenamed = This is a simple renaming, with the original column name and new column name
                               as parameters.

      5. sort = Sorts the data.

      6. limit = Specifies that we only want the first 5 values in our final DataFrame.

      7. show = Our action that requires the entire series of transformations to execute.



- Notes

    - A 'partition' is a collection of rows that sits on one machine in the cluster.


    - With 'narrow transformations', each input partition will contribute to only one
        output partition.  Spark pipelines these, and they're all performed in memory.


    - With 'wide transformations', many input partitions will contribute to many
        output partitions.  When we perform a shuffle, Spark writes the results to disk.


    - 3 Kinds of Actions:
        1. Actions to view data in console
        2. Actions to collect data to native objects in language
        3. Actions to write to output data sources