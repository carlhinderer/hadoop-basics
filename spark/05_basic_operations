-------------------------------------------------------------------------------
CHAPTER 5 - BASIC STRUCTURED OPERATIONS
-------------------------------------------------------------------------------

# Create a DataFrame to work with
> df = spark.
           read.
           format("json").
           load("/data/flight-data/json/2015-summary.json")

# Look at the schema of the dataframe
> df.printSchema()



# Check the schema during loading
# The schema has a name, type, nullable, and optional metadata for each column
> spark.
      read.
      format("json").
      load("/data/flight-data/json/2015-summary.json").
      schema

StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),
                StructField(ORIGIN_COUNTRY_NAME,StringType,true),
                StructField(count,LongType,true)))



# Manually specify the schema
> from pyspark.sql.types import StructField, StructType, StringType, LongType

> myManualSchema = StructType([
                       StructField("DEST_COUNTRY_NAME", StringType(), True),
                       StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
                       StructField("count", LongType(), False, metadata={"hello":"world"})
                   ])

> df = spark.
           read.
           format("json").
           schema(myManualSchema).
           load("/data/flight-data/json/2015-summary.json")



# Refer to columns
> from pyspark.sql.functions import col, column
> col("someColumnName")
> column("someColumnName")

# Explicitly refer to column on a single dataframe (used for joins)
> df.col("count")



# Columns are expressions
> from pyspark.sql.functions import expr

> expr("(((someCol + 5) * 200) - 6) < otherCol")



# Get all columns in a DataFrame
> spark.
      read.
      format("json").
      load("/data/flight-data/json/2015-summary.json").
      columns



# Get the first row in a DataFrame
> df.first()



# Create a row
> from pyspark.sql import Row
> myRow = Row("Hello", None, 1, False)



# Access values of rows
> myRow[0]
> myRow[2]



# DataFrame Transformations
      
    1. We can add rows or columns
    2. We can remove rows or columns
    3. We can transform a row into a column (or vice versa)
    4. We can change the order of rows based on the values in columns



# Create a DataFrame from a raw source and create a temporary view to query
> df = spark.read.format("json").load("/data/flight-data/json/2015-summary.json")

> df.createOrReplaceTempView("dfTable")



# Create rows manually
> from pyspark.sql import Row
> from pyspark.sql.types import StructField, StructType, StringType, LongType

> myManualSchema = StructType([
                               StructField("some", StringType(), True),
                               StructField("col", StringType(), True),
                               StructField("names", LongType(), False)
                              ])

> myRow = Row("Hello", None, 1)
> myDf = spark.createDataFrame([myRow], myManualSchema)
> myDf.show()



# Get 2 destination country names
> df.select("DEST_COUNTRY_NAME").show(2)

# In SQL instead
> SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2

+-----------------+
|DEST_COUNTRY_NAME|
+-----------------+
|    United States|
|    United States|
+-----------------+



# Select multiple columns
> df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2)

# In SQL
> SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2

+-----------------+-------------------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|
+-----------------+-------------------+
|    United States|            Romania|
|    United States|            Croatia|
+-----------------+-------------------+



# There are several interchangable ways to refer to columns
> from pyspark.sql.functions import expr, col, column

> df.select(
         expr("DEST_COUNTRY_NAME"),
         col("DEST_COUNTRY_NAME"),
         column("DEST_COUNTRY_NAME")).show(2)



# Note that you cannot use columns and strings in the same expression, 
#   this is a common mistake that will generate a compiler error
> df.select(col("DEST_COUNTRY_NAME"), "DEST_COUNTRY_NAME")



# Column names can be aliased in an expression
> df.select(expr("DEST_COUNTRY_NAME AS destination")).show(2)

# In SQL
> SELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2



# Your expression can be further manipulated as another expression
> df.select(expr("DEST_COUNTRY_NAME as destination").
     alias("DEST_COUNTRY_NAME")).
     show(2)



# Because 'select', followed by a series of 'expr', is so common, Spark has the
#   'selectExpr' shorthand for it
> df.selectExpr("DEST_COUNTRY_NAME as newColumnName", "DEST_COUNTRY_NAME").show(2)



# Adds a new column that specifies whether the origin and destination columns are
#   the same
> df.selectExpr(
                "*", # all original columns
                "(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry").
                show(2)

# In SQL
> SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry
  FROM dfTable
  LIMIT 2

+-----------------+-------------------+-----+-------------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|
+-----------------+-------------------+-----+-------------+
|    United States|            Romania|   15|        false|
|    United States|            Croatia|    1|        false|
+-----------------+-------------------+-----+-------------+



# Specify an aggregation over the entire DataFrame
> df.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2)

# In SQL
> df.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2)

+-----------+---------------------------------+
| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|
+-----------+---------------------------------+
|1770.765625|                              132|
+-----------+---------------------------------+



# Convert a Python literal into a Spark type
> from pyspark.sql.functions import lit
> df.select(expr("*"), lit(1).alias("One")).show(2)

# In SQL
> SELECT *, 1 as One FROM dfTable LIMIT 2

+-----------------+-------------------+-----+---+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|
+-----------------+-------------------+-----+---+
|    United States|            Romania|   15|  1|
|    United States|            Croatia|    1|  1|
+-----------------+-------------------+-----+---+



# Add a column with 'withColumn' instead
> df.withColumn("numberOne", lit(1)).show(2)

# In SQL
> SELECT *, 1 as numberOne FROM dfTable LIMIT 2

+-----------------+-------------------+-----+---------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|
+-----------------+-------------------+-----+---------+
|    United States|            Romania|   15|        1|
|    United States|            Croatia|    1|        1|
+-----------------+-------------------+-----+---------+



# Add a column with a more interesting expression
> df.withColumn("withinCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME")).
     show(2)



# We can also use the 'withColumnRenamed' method to rename a column
> df.withColumnRenamed("DEST_COUNTRY_NAME", "dest").columns