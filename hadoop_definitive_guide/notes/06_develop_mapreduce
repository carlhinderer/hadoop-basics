-----------------------------------------------------------------------
|  CHAPTER 6 - DEVELOPING A MAPREDUCE APPLICATION                     |
-----------------------------------------------------------------------

- Process of MapReduce Development

    1. Write your map and reduce functions, ideally with unit tests to make sure they
         do what you expect.

    2. Write a driver program to run a job and run it locally using a small subset of
         data to check that it is working.  When it is working, you can expand your
         unit tests to cover the entire map-reduce process.

    3. Next, run it on the cluster.  This is likely to expose more issues, which we can
         fix as before, expanding the tests as we go along.  Debugging is more difficult
         on a cluster, but we can use common techniques to make it easier.

    4. After the program is working, we may want to do some tuning, first by running
         through standard checks, then by doing task profiling.  Profiling distributed
         programs is not easy, but Hadoop has hooks to aid in the process.



- The Configuration API

    - An instance of the 'Configuration' class (in 'org.apache.hadoop.conf') represents the
        collection of config properties and their values.

      Configurations read their properties from 'resources', XML files with a simple structure
        for defining name/value pairs.


    - Here is a sample config file, 'configuration-1.xml':

        <?xml version="1.0"?>
        <configuration>
            <property>
                <name>color</name>
                <value>yellow</value>
                <description>Color</description>
            </property>

            <property>
                <name>size</name>
                <value>10</value>
                <description>Size</description>
            </property>

            <property>
                <name>weight</name>
                <value>heavy</value>
                <final>true</final>
                <description>Weight</description>
            </property>

            <property>
                <name>size-weight</name>
                <value>${size},${weight}</value>
                <description>Size and weight</description>
            </property>
        </configuration>


    - We can access the properties in code like this:

        Configuration conf = new Configuration();
        conf.addResource("configuration-1.xml");

        assertThat(conf.get("color"), is("yellow"));
        assertThat(conf.getInt("size", 0), is(10));
        assertThat(conf.get("breadth", "wide"), is("wide"));



- Combining Resources

    - More than one resource can be used to define a Configuration.  The default properties for
        the system are defined in 'core-default.xml'.  Site-specific overrides are defined in
        'core-site.xml'.


    - Resources are added to a Configuration in order:

        Configuration conf = new Configuration();
        conf.addResource("configuration-1.xml");
        conf.addResource("configuration-2.xml");


    - Properties marked 'final' cannot be overridden in later configurations.  An attempt to
        override it will throw a configuration error.



- Variable Expansion

    - Config properties can be defined in terms of other properties.

        <property>
            <name>size-weight</name>
            <value>${size},${weight}</value>
            <description>Size and weight</description>
        </property>

        assertThat(conf.get("size-weight"), is("12,heavy"));



- Setting Up the Development Environment

    - First, we add a 'pom.xml'.



- Managing Configuration

    - When developing Hadoop applications, it is common to switch between running the applcation
        locally and in a cluster.  One way to accomodate this is to have Hadoop config files
        containing the connection settings for each cluster you run against.


    - For this book, we have a directory named 'conf' with 3 config files:

        hadoop-local.xml
        hadoop-localhost.xml
        hadoop-cluster.xml


    - With this setup, it is easy to use any configuration with the '-conf' command-line
        switch.

        # Run in pseudodistributed mode on localhost
        $ hadoop fs -conf conf/hadoop-localhost.xml -ls .



- GenericOptionsParser, Tool, and ToolRunner

    - Hadoop comes with a few helper classes for making it easier to run jobs from the command
        line.  'GenericOptionsParser' is a class that interprets common Hadoop command-line
        options and sets them on a 'Configuration' object for your application to use.

      You don't usually use 'GenericOptionsParser' directly, as it's more convenient to
        implement the 'Tool' interface and run your application with 'ToolRunner'.


    - We have an example located at '/code/ch06/src/main/java/ConfigurationPrinter.java'.

      To compile and run it:

        # Compile and build JAR
        $ mvn clean compile package

        # Run the JAR
        $ hadoop jar hadoop-testProject.jar ConfigurationPrinter -conf ../conf/hadoop-localhost.xml


        # Get the value of a particular property
        $ hadoop ConfigurationPrinter -conf conf/hadoop-localhost.xml \  
            | grep yarn.resourcemanager.address=

        # Set the value of a particular property
        $ hadoop ConfigurationPrinter -D color=yellow | grep color