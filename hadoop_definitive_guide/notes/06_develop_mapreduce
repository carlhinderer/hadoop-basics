-----------------------------------------------------------------------
|  CHAPTER 6 - DEVELOPING A MAPREDUCE APPLICATION                     |
-----------------------------------------------------------------------

- Process of MapReduce Development

    1. Write your map and reduce functions, ideally with unit tests to make sure they
         do what you expect.

    2. Write a driver program to run a job and run it locally using a small subset of
         data to check that it is working.  When it is working, you can expand your
         unit tests to cover the entire map-reduce process.

    3. Next, run it on the cluster.  This is likely to expose more issues, which we can
         fix as before, expanding the tests as we go along.  Debugging is more difficult
         on a cluster, but we can use common techniques to make it easier.

    4. After the program is working, we may want to do some tuning, first by running
         through standard checks, then by doing task profiling.  Profiling distributed
         programs is not easy, but Hadoop has hooks to aid in the process.



- The Configuration API

    - An instance of the 'Configuration' class (in 'org.apache.hadoop.conf') represents the
        collection of config properties and their values.

      Configurations read their properties from 'resources', XML files with a simple structure
        for defining name/value pairs.


    - Here is a sample config file, 'configuration-1.xml':

        <?xml version="1.0"?>
        <configuration>
            <property>
                <name>color</name>
                <value>yellow</value>
                <description>Color</description>
            </property>

            <property>
                <name>size</name>
                <value>10</value>
                <description>Size</description>
            </property>

            <property>
                <name>weight</name>
                <value>heavy</value>
                <final>true</final>
                <description>Weight</description>
            </property>

            <property>
                <name>size-weight</name>
                <value>${size},${weight}</value>
                <description>Size and weight</description>
            </property>
        </configuration>


    - We can access the properties in code like this:

        Configuration conf = new Configuration();
        conf.addResource("configuration-1.xml");

        assertThat(conf.get("color"), is("yellow"));
        assertThat(conf.getInt("size", 0), is(10));
        assertThat(conf.get("breadth", "wide"), is("wide"));



- Combining Resources

    - More than one resource can be used to define a Configuration.  The default properties for
        the system are defined in 'core-default.xml'.  Site-specific overrides are defined in
        'core-site.xml'.


    - Resources are added to a Configuration in order:

        Configuration conf = new Configuration();
        conf.addResource("configuration-1.xml");
        conf.addResource("configuration-2.xml");


    - Properties marked 'final' cannot be overridden in later configurations.  An attempt to
        override it will throw a configuration error.



- Variable Expansion

    - Config properties can be defined in terms of other properties.

        <property>
            <name>size-weight</name>
            <value>${size},${weight}</value>
            <description>Size and weight</description>
        </property>

        assertThat(conf.get("size-weight"), is("12,heavy"));



- Setting Up the Development Environment

    - First, we add a 'pom.xml'.



- Managing Configuration

    - When developing Hadoop applications, it is common to switch between running the applcation
        locally and in a cluster.  One way to accomodate this is to have Hadoop config files
        containing the connection settings for each cluster you run against.


    - For this book, we have a directory named 'conf' with 3 config files:

        hadoop-local.xml
        hadoop-localhost.xml
        hadoop-cluster.xml


    - With this setup, it is easy to use any configuration with the '-conf' command-line
        switch.

        # Run in pseudodistributed mode on localhost
        $ hadoop fs -conf conf/hadoop-localhost.xml -ls .



- GenericOptionsParser, Tool, and ToolRunner

    - Hadoop comes with a few helper classes for making it easier to run jobs from the command
        line.  'GenericOptionsParser' is a class that interprets common Hadoop command-line
        options and sets them on a 'Configuration' object for your application to use.

      You don't usually use 'GenericOptionsParser' directly, as it's more convenient to
        implement the 'Tool' interface and run your application with 'ToolRunner'.


    - We have an example located at '/code/ch06/src/main/java/ConfigurationPrinter.java'.

      To compile and run it:

        # Compile and build JAR
        $ mvn clean compile package

        # Run the JAR
        $ hadoop jar hadoop-testproject.jar ConfigurationPrinter -conf ../conf/hadoop-localhost.xml


        # Get the value of a particular property
        $ hadoop ConfigurationPrinter -conf conf/hadoop-localhost.xml \  
            | grep yarn.resourcemanager.address=

        # Set the value of a particular property
        $ hadoop ConfigurationPrinter -D color=yellow | grep color



- Writing a Unit Test with MRUnit

    - The map and reduce functions are easy to test in isolation due to their functional
        style.  We use 'MRUnit', which works in conjunction with JUnit.


    - First, we add 'NcdcRecordParser.java' and 'MapTemperatureMapper.java', which implement
        the mapper.


    - Then, we add unit tests to make sure they work as expected in 'MaxTemperatureMapperTest.java'.

        $ mvn test


    - With those tests passing, we'll add the 'MaxTemperatureReducer' also.



- Running a Job in a Local Job Runner

    - Now that we have the mapper and reducer working on controlled inputs, the next step is
        to write a job driver and run it on some test data on a development machine.

      Using the 'Tool' interface introduced earlier in the chapter, it's easy to write a driver
        to run our job.


    - We add the driver in 'MaxTemperatureDriver.java', which implements the 'Tool' interface,
        so we get the benefit of being able to set the options that 'GenericOptionsParser' 
        supports.


    - Now, we can run this application against some local files using the local job runner.

        $ mvn compile package

        $ hadoop jar hadoop-testproject.jar v4.MaxTemperatureDriver -conf ../conf/hadoop-local.xml \
            input/ncdc/micro output



- Testing the Driver

    - Implementing the 'Tool' interface also makes your code easier to test, since you can
        inject an arbitrary Configuration.


    - The first way to test the 'MaxTemperatureDriver' is to run the job against a test file
        on the local filesystem.

        @Test
        public void test() throws Exception {
            Configuration conf = new Configuration();
            conf.set("fs.defaultFS", "file:///");
            conf.set("mapreduce.framework.name", "local");
            conf.setInt("mapreduce.task.io.sort.mb", 1);
    
            Path input = new Path("input/ncdc/micro");
            Path output = new Path("output");
    
            FileSystem fs = FileSystem.getLocal(conf);
            fs.delete(output, true); // delete old outputMaxTemperatureDriver
    
            MaxTemperatureDriver driver = new MaxTemperatureDriver();
            driver.setConf(conf);
    
            int exitCode = driver.run(new String[] { input.toString(), output.toString()});
    
            assertThat(exitCode, is(0));
            checkOutput(conf, output);
        }


    - The second way to test the driver is to use a 'mini-cluster'.  This approach is used widely
        in Hadoop's own automated test suite.



- Running on a Cluster - Packaging a Job

    - Now that we're happy with how the program runs on a small test dataset, we are ready to try
        it on a full dataset on a Hadoop cluster.

      First, we need to package the job.


    - The local job runner uses a single JVM to run a job, so as long as the classes needed to run
        the job are on its classpath, things will just work.

      In a distributed setting, things are more complex.  First, a job's classes must be packaged
        into a 'job JAR file' to send to the cluster.   Hadoop will find the job JAR automatically by
        searching for the JAR on the driverâ€™s classpath that contains the class set in the
        setJarByClass()method (on JobConf or Job).


    - To create a job JAR file using Maven:

        $ mvn package -DskipTests


    - If you have a single job per JAR, you can specify the main class to run in the JAR's manifest.
        Otherwise, you'll need to specify it on the command line.


    - The user's client-side classpath set by 'hadoop jar' is made up of:

        - The job JAR file
        - Any JAR files in the lib directory of the job JAR file, and the classes directory
        - The classpath defined by HADOOP_CLASSPATH, if set


    - On a cluster, map and reduce tasks run in separate JVMs, their classpaths are not controlled
        by 'HADOOP_CLASSPATH'.  


    - To package any dependencies for a job, there are a few options:

        1. Unpack the libraries and repackage them in the job JAR
        2. Package the libraries in the 'lib' directory of the job JAR
        3. Keep the libraries separate and add them to the classpath via 'HADOOP_CLASSPATH'



- Launching a Job

    - To launch a job, we need to run the driver, specifying the cluster that we want to run
        the job on with the '-conf' option.

        $ unset HADOOP_CLASSPATH

        $ hadoop jar hadoop-examples.jar v4.MaxTemperatureDriver -conf conf/hadoop-cluster.xml \
            input/ndcc/all max-temp



- The MapReduce Web UI

    - The MapReduce web UI is useful for following a job's progress while it is running, as well
        as finding job statistics and logs after the job has completed.

        http://resource-manager-host:8088/


    - The 'Resource Manager' page shows you information about all the applications currently
        running, the available resources, and information about Node Managers.


    - The 'Job Page' for a specific page monitors the progress of a job.  It will show both map
        and reduce progress.



- Retrieving the Results

    - Once a job is finished, there are various ways to retrieve the results.  Each reducer 
        produces one output file, so there are 30 part files named 'part-r-00000' to 'part-r-00029'
        in the 'max-temp' directory if there are 30 reducers.


    - It is often useful to copy the output from HDFS to our development machine.  The '-getmerge'
        option is usefulhere, as it will merge all the results files into a single file, then
        put it on the local filesystem.

        $ hadoop fs -getmerge max-temp max-temp-local


    - Another way to look at the output is to print the output files to the console.

        $ hadoop fs -cat max-temp/*



- Debugging a Job

    - Debugging a job with print statements doesn't scale well across a cluster.  So, we need
        to be a little more creative.  Common techniques include:

        - Logging anomalies to stderr
        - Having a counter that counts implausible records to see how common they are


    - If your program produces a lot of log data, you could either:

        1. Rewrite your program to add this log data to the job output
        2. Just produce logs and write a separate program to analyze them


    - Here, we add some debugging logic to our mapper.  If the temperature is over 100 degrees
        Celsius, we print a line to standard error, update the map's status message, and
        increment a counter.

        public class MaxTemperatureMapper 
            extends Mapper<LongWritable, Text, Text, IntWritable> {
          
            enum Temperature {
                OVER_100
            }

            private NcdcRecordParser parser = new NcdcRecordParser();

            @Override
            public void map (LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {

                parser.parse(value);
                if (parser.isValidTemperature()) {
                    int airTemperature = parser.getAirTemperature();
                    if (airTemperature > 1000) {
                        System.err.println("Temperature over 100 degrees for input: " + value);
                        context.setStatus("Detected possibly corrupt record: see logs.");
                        context.getCounter(Temperature.OVER_100).increment(1);
                    }
                    context.write(new Text(parser.getYear()),new IntWritable(airTemperature));
                }
            }
        }


    - We can look at log line and counter values on the job page on the MapReduce web UI.


    - When the job is finished, we can check the value of the counter:

        $ mapred job -counter job_1410450250506_0006 'v4.MaxTemperatureMapper$Temperature' OVER_100
        
        # Output
        3



- Handling Malformed Data

    - Capturing input data that causes a problem is valuable, since then you can use it in a
        unit test.

      Here is a test to check that our counter is being incremented correctly when bad records 
        are encountered:

        @Test public void parsesMalformedTemperature () 
            throws IOException, InterruptedException {

            String year = "0335999999433181957042302005+37950+139117SAO  +0004";
            String temperature = "RJSN V02011359003150070356999999433201957010100005+353";
            Text value = new Text(year + value);

            Counters counters = new Counters();
            new MapDriver<LongWritable, Text, Text, IntWritable>()
                .withMapper(new MaxTemperatureMapper())
                .withInput(new LongWritable(0), value)
                .withCounters(counters).
                runTest();

            Counter c = counters.findCounter(MaxTemperatureMapper.Temperature.MALFORMED);
            assertThat(c.getValue(), is(1L));



- Hadoop Logs


- Remote Debugging


- Tuning a Job


- Profiling Tasks


- The HPROF Profiler


- MapReduce Workflows


- Decomposing a Problem into MapReduce Joins


- JobControl


- Apache Oozie


- Defining an Oozie Workflow


- Packaing and Deploying an Oozie Workflow Application


- Running an Oozie Workflow Job