-----------------------------------------------------------------------
|  CHAPTER 5 - HADOOP I/O                                             |
-----------------------------------------------------------------------

- Data Integrity

    - Because we process so much data, the chance of data corruption happening somewhere is
        high.  Typically, we use checksums (like CRC-32) to ensure data is not corrupted.


    - Data Integrity in HDFS

        - HDFS transparently checkums all data written to it, and by default verifies checksums
            when reading data.  It uses CRC-32C, which has a 4-byte checksum for each 512 bytes.


        - DataNodes are responsible for verifying the data they receive before storing the data
            and its checksum.  This applies to data they receive from clients and from other
            DataNodes during replication.


        - When clients read from DataNodes, they verify checksums as well.


        - Each DataNode also runs a 'DataBlockScanner' in a background thread that periodically
            verifies all the blocks stored on the DataNode.  If it finds a corruption, it can
            heal using replicated data on another node.


        - It is possible to disable You can find a file's checksum using:

            $ hadoop fs -checksum fileName
            verification of checksums by using 'setVerifyChecksum(false)'
            before using 'open()' to read a file.


        - You can find a file's checksum using:

            $ hadoop fs -checksum fileName


    - LocalFileSystem

        - The 'HadoopLocalFileSystem' performs client-side checksumming.  When you write a file
            'filename', a hidden file 'filename.crc' is added in the same directory that keeps
            checksums for each 512-byte block in the file.


        - Checksums are verified when a file is read.  If an error is detected, a 
            'ChecksumException' is thrown.


        - Although the performance penalty is not that big, you might still want to avoid it
            by disabling checksumming.  To do this globally:

            Configuation conf = ...
            FileSystem fs = new RawLocalFileSystem();
            fs.initialize(fs, conf);


        - 'LocalFileSystem' uses 'ChecksumFileSystem', which is just a wrapper around
            'FileSystem.'



- Compression

    - Compessing files reduces the space needed to store files and the bandwidth used to
        send them across a network or to/from disk.  Here are the most commonly used formats
        in Hadoop:

        Compression format     Tool     Algorithm     Filename extension     Splittable?
        -------------------------------------------------------------------------------------
        DEFLATE                N/A      DEFLATE       .deflate               No
        gzip                   gzip     DEFLATE       .gz                    No
        bzip2                  bzip2    bzip2         .bz2                   Yes
        LZO                    lzop     LZO           .lzo                   No
        LZ4                    N/A      LZ4           .lz4                   No
        Snappy                 N/A      Snappy        .snappy                No


    - The different algorithms have different characteristics:

        - gzip is in the middle of the space/time trade-off
        - bzip2 compresses more effectively, but is slower
        - LZO, LZ4, and Snappy are optimized for speed


    - Codecs

        - In Hadoop, a codec is represented by an implementation of the 'CompressionCodec'
            interface.

            Compression format     Hadoop CompressionCodec
            ----------------------------------------------------------------------
            DEFLATE                org.apache.hadoop.io.compress.DefaultCodec
            gzip                   org.apache.hadoop.io.compress.GzipCodec
            bzip2                  org.apache.hadoop.io.compress.BZip2Codec
            LZO                    com.hadoop.compression.lzo.LzopCodec
            LZ4                    org.apache.hadoop.io.compress.Lz4Codec
            Snappy                 org.apache.hadoop.io.compress.SnappyCodec


        - The LZO codecs are GPL licensed and cannot be included in Apache distributions, 
            so they must be downloaded separately.



- Compressing and Decompressing Streams with CompressionCodec

    - 'CompressionCodec' has 2 methods that allow you to easily compress and decompress data.

        createOutputStream (OutputStream out)
        createInputStream (InputStream in)


    - We have an example of a program that compresses data read from standard input and writes
        it to standard output in 'code/ch05/StreamCompressor.java'.

      To run it:

        # Compile the StreamProcessor
        $ javac -classpath ${HADOOP_CLASSPATH} *

        # Build the jar
        $ jar -cvf StreamProcessor.jar

        # Run the StreamProcessor
        $ echo "Text" \
          | hadoop jar StreamProcessor.jar StreamCompressor org.apache.hadoop.io.compress.GzipCodec \
          | gunzip -

        # Output
        Text



- Inferring CompressionCodecs using CompressionCodecFactory

    - If you are reading a compressed file, normally you can infer which codec to use by
        looking at the file extension.  The 'CompressionCodecFactory' has a way to infer
        the codec using the 'getCodec()' method.


    - We have an example of a file decompressor at '/code/ch05/FileDecompressor.java'.

      To run it:

        $ hadoop FileDecompressor.jar FileDecompressor ../../weather/1901.gz



- Native Libraries

    - For performance, it is preferable to use a native library for compression and 
        decompression.  For example, in one test, decompression time was reduced by 
        50% (decompression) and 10% (compression) when using the native gzip library instead
        of the Java implementation.


    - The Apache Hadoop binary tarball includes a set of prebuilt native compression binaries
        called 'libhadoop.so'.  Hadoop will look in these native libraries first when picking
        a codec to use.



- Codec Pool

    - If you are using a native library and doing a lot of compression and decompression,
        you can use a 'CodecPool' to reuse the compressors, amortizing the cost of creating the
        objects.


    - An example of this is located in '/code/ch05/PooledStreamCompressor.java'.



- Compression and Input Splits

    - If a file is compressed using a codec that is not splittable, HDFS will not be able
        to split the file into 128-MB blocks.  MapReduce will handle the single block correctly,
        but won't be able to parallelize the processing.


    - The bzip2 codec does support splitting, so it is a good choice for compression of very large
        files.



- Which Compression Format Should Be Used?

    - Since Hadoop applications use large datasets, we should strive to take advantage of 
        compression.  Which method to use depends on file size, format, and the processing
        tools we are using.


    - Here are some suggestions, arranged from most to least effective in general:

        1. Use a container file format such as sequence files, Avro, ORC, or Parquet.  All of
             these support compression and splitting.  A fast compressor such as LZO, LZ4, or
             Snappy is generally a good choice.

        2. Use a compression format that supports splitting, such as bzip2.  Or use one that can
             be indexed to support splitting such as LZO.

        3. Split the file into chunks in the application, and compress each chunk separately using
             any supported compression format.

        4. Store the files uncompressed.



- Using Copression in MapReduce

    - If your input files are compressed, they will be automatically decompressed when they
        are read by MapReduce.


    - To compress the output of a MapReduce job, set:

        mapreduce.output.fileoutputformat.compress = true
        mapreduce.output.fileoutputformat.compress.codec = "codecClassName"


    - An example of compressing the output of a MapReduce job is located at 
        '/code/ch05/MaxTemperatureWithCompression.java'.

      To run it:

        $ hadoop MaxTemperatureWithCompression.jar MaxTemperatureWithCompression \
            ../../data/weather output



- Compressing Map Output

    - Even if your MapReduce application reads and writes uncompressed data, it may benefit 
        from compressing the intermediate output of the map phase.  Using a fast codec like
        LZO, LZ4, or Snappy will reduce the amount of data transferred across the network 
        during the shuffle.


    - To enable gzip compression for the map output, we can add this to the configuration:

        Configuration conf = new Configuration();
        conf.setBoolean(Job.MAP_OUTPUT_COMPRESS,true);
        conf.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class);

        Job job = new Job(conf);



- Serialization

    - Serialization is used in 2 quite distinct areas of distributed data processing:

        1. Interprocess communication (RPC calls)
        2. Persistent storage


    - In general, it is desirable that an RPC serialization format is:

        1. Compact
        2. Fast
        3. Extensible
        4. Interoperable

      It turns out that a persistent storage serialization format has the same requirements.


    - Hadoop uses its own serialization format, 'Writables', which is certainly compact and fast.
        However, it is not easy to extend or use with languages other than Java.



- The Writable Interface

    - The 'Writable' interface defines 2 methods:

        package org.apache.hadoop.io;

        import java.io.DataOutput;
        import java.io.DataInput;
        import java.io.IOException;

        public interface Writable { 
            void write (DataOutputout) throws IOException;
            void readFields (DataInputin) throws IOException;
        }


    - We can create a Writable.  For instance, an 'IntWritable' is a wrapper for a Java int.

        // Create Writable and set value
        IntWritable writable = new IntWritable();
        writable.set(163);

        // Create and set value at construction time
        IntWritable writable = new IntWritable(163);


    - An example of serializing a writable is located in the 'serialize()' method in 
        '/code/ch05/SerializerDeserializer.java'.

      An example of deserializing a writable in located in the 'deserialize()' method in
        '/code/ch05/SerializerDeserializer.java'.



- WritableComparable and Comparators



- Writable Classes

    - Writable Wrappers for Java Primitives

    - Text

    - Indexing

    - Unicode

    - Iteration

    - Mutability

    - Resorting to String

    - BytesWritable

    - NullWritable

    - ObjectWritable and GenericWritable

    - Writable Collections



- Implementing a Custom Writable



- Implementing a RawComparator for Speed



- Custom Comparators



- Serialization Frameworks

    - Serialization IDL



- File-Based Data Structures

    - SequenceFile



- Writing a SequenceFile



- Reading a SequenceFile



- Displaying a SequenceFile with the Command-Line Interface



- Sorting and Merging SequenceFiles



- The SequenceFile Format



- MapFile

    - MapFile Variants



- Other File Formats and Column-Oriented Formats