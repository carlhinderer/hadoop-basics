-----------------------------------------------------------------------
|  CHAPTER 5 - HADOOP I/O                                             |
-----------------------------------------------------------------------

- Data Integrity

    - Because we process so much data, the chance of data corruption happening somewhere is
        high.  Typically, we use checksums (like CRC-32) to ensure data is not corrupted.


    - Data Integrity in HDFS

        - HDFS transparently checkums all data written to it, and by default verifies checksums
            when reading data.  It uses CRC-32C, which has a 4-byte checksum for each 512 bytes.


        - DataNodes are responsible for verifying the data they receive before storing the data
            and its checksum.  This applies to data they receive from clients and from other
            DataNodes during replication.


        - When clients read from DataNodes, they verify checksums as well.


        - Each DataNode also runs a 'DataBlockScanner' in a background thread that periodically
            verifies all the blocks stored on the DataNode.  If it finds a corruption, it can
            heal using replicated data on another node.


        - It is possible to disable You can find a file's checksum using:

            $ hadoop fs -checksum fileName
            verification of checksums by using 'setVerifyChecksum(false)'
            before using 'open()' to read a file.


        - You can find a file's checksum using:

            $ hadoop fs -checksum fileName


    - LocalFileSystem

        - The 'HadoopLocalFileSystem' performs client-side checksumming.  When you write a file
            'filename', a hidden file 'filename.crc' is added in the same directory that keeps
            checksums for each 512-byte block in the file.


        - Checksums are verified when a file is read.  If an error is detected, a 
            'ChecksumException' is thrown.


        - Although the performance penalty is not that big, you might still want to avoid it
            by disabling checksumming.  To do this globally:

            Configuation conf = ...
            FileSystem fs = new RawLocalFileSystem();
            fs.initialize(fs, conf);


        - 'LocalFileSystem' uses 'ChecksumFileSystem', which is just a wrapper around
            'FileSystem.'



- Compression

    - Compessing files reduces the space needed to store files and the bandwidth used to
        send them across a network or to/from disk.  Here are the most commonly used formats
        in Hadoop:

        Compression format     Tool     Algorithm     Filename extension     Splittable?
        -------------------------------------------------------------------------------------
        DEFLATE                N/A      DEFLATE       .deflate               No
        gzip                   gzip     DEFLATE       .gz                    No
        bzip2                  bzip2    bzip2         .bz2                   Yes
        LZO                    lzop     LZO           .lzo                   No
        LZ4                    N/A      LZ4           .lz4                   No
        Snappy                 N/A      Snappy        .snappy                No


    - The different algorithms have different characteristics:

        - gzip is in the middle of the space/time trade-off
        - bzip2 compresses more effectively, but is slower
        - LZO, LZ4, and Snappy are optimized for speed


    - Codecs

        - In Hadoop, a codec is represented by an implementation of the 'CompressionCodec'
            interface.

            Compression format     Hadoop CompressionCodec
            ----------------------------------------------------------------------
            DEFLATE                org.apache.hadoop.io.compress.DefaultCodec
            gzip                   org.apache.hadoop.io.compress.GzipCodec
            bzip2                  org.apache.hadoop.io.compress.BZip2Codec
            LZO                    com.hadoop.compression.lzo.LzopCodec
            LZ4                    org.apache.hadoop.io.compress.Lz4Codec
            Snappy                 org.apache.hadoop.io.compress.SnappyCodec


        - The LZO codecs are GPL licensed and cannot be included in Apache distributions, 
            so they must be downloaded separately.



- Compressing and Decompressing Streams with CompressionCodec

    - 'CompressionCodec' has 2 methods that allow you to easily compress and decompress data.

        createOutputStream (OutputStream out)
        createInputStream (InputStream in)


    - We have an example of a program that compresses data read from standard input and writes
        it to standard output in 'code/ch05/StreamCompressor.java'.

      To run it:

        # Compile the StreamProcessor
        $ javac -classpath ${HADOOP_CLASSPATH} *

        # Build the jar
        $ jar -cvf StreamProcessor.jar

        # Run the StreamProcessor
        $ echo "Text" \
          | hadoop jar StreamProcessor.jar StreamCompressor org.apache.hadoop.io.compress.GzipCodec \
          | gunzip -

        # Output
        Text



- Inferring CompressionCodecs using CompressionCodecFactory

    - If you are reading a compressed file, normally you can infer which codec to use by
        looking at the file extension.  The 'CompressionCodecFactory' has a way to infer
        the codec using the 'getCodec()' method.


    - We have an example of a file decompressor at '/code/ch05/FileDecompressor.java'.

      To run it:

        $ hadoop FileDecompressor.jar FileDecompressor ../../weather/1901.gz



- Native Libraries

    - For performance, it is preferable to use a native library for compression and 
        decompression.  For example, in one test, decompression time was reduced by 
        50% (decompression) and 10% (compression) when using the native gzip library instead
        of the Java implementation.


    - The Apache Hadoop binary tarball includes a set of prebuilt native compression binaries
        called 'libhadoop.so'.  Hadoop will look in these native libraries first when picking
        a codec to use.



- Codec Pool

    - If you are using a native library and doing a lot of compression and decompression,
        you can use a 'CodecPool' to reuse the compressors, amortizing the cost of creating the
        objects.


    - An example of this is located in '/code/ch05/PooledStreamCompressor.java'.



- Compression and Input Splits

    - If a file is compressed using a codec that is not splittable, HDFS will not be able
        to split the file into 128-MB blocks.  MapReduce will handle the single block correctly,
        but won't be able to parallelize the processing.


    - The bzip2 codec does support splitting, so it is a good choice for compression of very large
        files.



- Which Compression Format Should Be Used?

    - Since Hadoop applications use large datasets, we should strive to take advantage of 
        compression.  Which method to use depends on file size, format, and the processing
        tools we are using.


    - Here are some suggestions, arranged from most to least effective in general:

        1. Use a container file format such as sequence files, Avro, ORC, or Parquet.  All of
             these support compression and splitting.  A fast compressor such as LZO, LZ4, or
             Snappy is generally a good choice.

        2. Use a compression format that supports splitting, such as bzip2.  Or use one that can
             be indexed to support splitting such as LZO.

        3. Split the file into chunks in the application, and compress each chunk separately using
             any supported compression format.

        4. Store the files uncompressed.



- Using Copression in MapReduce

    - If your input files are compressed, they will be automatically decompressed when they
        are read by MapReduce.


    - To compress the output of a MapReduce job, set:

        mapreduce.output.fileoutputformat.compress = true
        mapreduce.output.fileoutputformat.compress.codec = "codecClassName"


    - An example of compressing the output of a MapReduce job is located at 
        '/code/ch05/MaxTemperatureWithCompression.java'.

      To run it:

        $ hadoop MaxTemperatureWithCompression.jar MaxTemperatureWithCompression \
            ../../data/weather output



- Compressing Map Output

    - Even if your MapReduce application reads and writes uncompressed data, it may benefit 
        from compressing the intermediate output of the map phase.  Using a fast codec like
        LZO, LZ4, or Snappy will reduce the amount of data transferred across the network 
        during the shuffle.


    - To enable gzip compression for the map output, we can add this to the configuration:

        Configuration conf = new Configuration();
        conf.setBoolean(Job.MAP_OUTPUT_COMPRESS,true);
        conf.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class);

        Job job = new Job(conf);



- Serialization

    - Serialization is used in 2 quite distinct areas of distributed data processing:

        1. Interprocess communication (RPC calls)
        2. Persistent storage


    - In general, it is desirable that an RPC serialization format is:

        1. Compact
        2. Fast
        3. Extensible
        4. Interoperable

      It turns out that a persistent storage serialization format has the same requirements.


    - Hadoop uses its own serialization format, 'Writables', which is certainly compact and fast.
        However, it is not easy to extend or use with languages other than Java.



- The Writable Interface

    - The 'Writable' interface defines 2 methods:

        package org.apache.hadoop.io;

        import java.io.DataOutput;
        import java.io.DataInput;
        import java.io.IOException;

        public interface Writable { 
            void write (DataOutputout) throws IOException;
            void readFields (DataInputin) throws IOException;
        }


    - We can create a Writable.  For instance, an 'IntWritable' is a wrapper for a Java int.

        // Create Writable and set value
        IntWritable writable = new IntWritable();
        writable.set(163);

        // Create and set value at construction time
        IntWritable writable = new IntWritable(163);


    - An example of serializing a writable is located in the 'serialize()' method in 
        '/code/ch05/SerializerDeserializer.java'.

      An example of deserializing a writable in located in the 'deserialize()' method in
        '/code/ch05/SerializerDeserializer.java'.



- WritableComparable and Comparators

    - 'IntWritable' implements the 'WritableComparable' interface, which is a subinterface of the
        'Writable' and 'java.lang.Comparable' interfaces:

        package org.apache.hadoop.io;

        public interface WritableComparable<T> extends Writable, Comparable<T> {
        }


    - Comparison of types is crucial for MapReduce, where there is a sorting phase where keys are
        compared to each other.  One optimization Hadoop provides is the 'RawComparator' extension
        of Java's 'Comparator':

        package org.apache.hadoop.io;

        import java.util.Comparator;

        public interface RawComparator<T> extends Comparator<T> {
            public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2);
        }

      This interface permits implementors to compare records read from a stream without deserializing
        them into objects.


    - 'WritableComparator' is a general-purpose implementation of 'RawComparator'.

        // Get Comparator
        RawComparator<IntWritable> comparator = WritableComparator.get(IntWritable.class);

        // Compare 2 IntWritables
        IntWritable w1 = new IntWritable(163);
        IntWritable w2 = new IntWritable(67);
        assertThat(comparator.compare(w1, w2), greaterThan(0));

        // Compare the serialized representations of 2 IntWritables
        byte[] b1 = serialize(w1);
        byte[] b2 = serialize(w2);
        assertThat(comparator.compare(b1, 0, b1.length, b2, 0, b2.length),greaterThan(0));



- Writable Classes

    - Writable Wrappers for Java Primitives

        - There are Writable wrappers for all the Java primitives:

            Java primitive     Writable implementation     Serialized size (bytes)
            --------------------------------------------------------------------------
            boolean            BooleanWritable             1
            byte               ByteWritable                1
            short              ShortWritable               2
            int                IntWritable                 4
                               VIntWritable                1–5
            float              FloatWritable               4
            long               LongWritable                8
                               VLongWritable               1–9
            double             DoubleWritable              8


    - Text

        - 'Text' is a Writable for UTF-8 sequences.  It is the Writable equivalent of 'String'.
            It has a maximum value size of 2 GB, since it's length is stored in an


    - Indexing

        - Indexing for the 'Text' class is in terms of the encoded byte sequence, not in terms
            of the Unicode character as with Strings.  For ASCII strings, these are the same.

            // Text's charAt() returns a Unicode code point, not a character
            Text t = new Text("Hadoop");
            assertThat(t.getLength(), is(6));
            assertThat(t.charAt(2), is((int) 'd'));


    - Unicode

        - When we start using characters that are encoded with more than a single byte, the
            differences between 'Text' and 'String' become clear.  For example:

            Unicode codepoint     U+0041          U+00DF           U+6771            U+10400
            -----------------------------------------------------------------------------------------
            Name                  LATIN CAPITAL   LATIN SMALL      N/A               DESERET CAPITAL
                                  LETTER A        LETTER SHARP S   (Han ideograph)   LETTER LONG I
            -----------------------------------------------------------------------------------------
            UTF-8 code units      41              c3 9f            e6 9d b1          f0 90 90 80
            -----------------------------------------------------------------------------------------
            Java representation   \u0041          \u00DF           \u6771            \uD801\uDC00


        - We have a full set of junit tests that show the difference between Text and String types
            at '/code/ch05/StringTextComparisonTest'.


        - To run this, 
        
            1. Install JUnit by downloading the 'junit' and 'hamcrest' and add them to a directory
                 like /usr/local/juint.

            2. Create environment variables so that the junit jars will be in the classpath.

                 .bashrc
                 -----------------------------------
                 export JUNIT_HOME="/usr/local/junit"
                 export JUNIT_CLASSPATH=$JUNIT_HOME/junit-4-13.1.jar:$JUNIT_HOME/hamcrest-core-1.3.jar

                 export HADOOP_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath):$JUNIT_CLASSPATH

            3. When you run the tests, you also need your current directory in the path.

                java -cp .:${HADOOP_CLASSPATH} org.junit.runner.JUnitCore StringTextComparisonTest


    - Iteration

        - Iterating over the Unicode characters in Text is complicated by the use of byte offsets 
            for indexing, since you can’t just increment the index.  The idiom for iteration is a 
            little obscure:

            public class TextIterator { 

                public static void main(String[]args) {
                    Text t = new Text("\u0041\u00DF\u6771\uD801\uDC00");
                
                    ByteBuffer buf = ByteBuffer.wrap(t.getBytes(), 0, t.getLength());
                    int cp;

                    while (buf.hasRemaining() && (cp = Text.bytesToCodePoint(buf)) != -1) {
                        System.out.println(Integer.toHexString(cp));
                    }
                }
            }


    - Mutability

        - Another difference from Java Strings is that Text is mutable, like all other Writable
            types.

            Text t = new Text("hadoop");
            t.set("pig");

            assertThat(t.getLength(), is(3));
            assertThat(t.getBytes().length, is(3));


    - Resorting to String

        - Since Text doesn't have the rich API that Strings have, often the easiest way to do
            some processing is to just convert it to a String.

            assertThat(newText("hadoop").toString(), is("hadoop"));


    - BytesWritable

        - 'BytesWritable' is a wrapper for an array of binary data.  Its serialized format is a 
            4-byte integer field that specifies the number of bytes to follow, followed by the 
            bytes themselves.

            BytesWritable b = new BytesWritable(new byte[]{3,5});
            byte[] bytes = serialize(b);

            assertThat(StringUtils.byteToHexString(bytes), is("000000020305"));


        - BytesWritable is mutable, and its value may be changed by calling its 'set()' method.

            b.setCapacity(11);

            assertThat(b.getLength(), is(2));
            assertThat(b.getBytes().length, is(11));


    - NullWritable

        - 'NullWritable' is a special type of Writable, as it has a zero-length serialization. 
            No bytesare written to or read from the stream. 

          It is used as a placeholder.


    - ObjectWritable and GenericWritable

        - 'ObjectWritable' is a general-purpose wrapper for the following: Java primitives, 
            String, enum, Writable, null, or arrays of any of these types. It is used in Hadoop 
            RPC to marshal and unmarshal method arguments and return types.

          It is useful when a field can be more than one type.


        - 'GenericWritable' is useful when the number of types is small and known ahead of time.


    - Writable Collections

        - The 'org.apache.hadoop.io' package includes six Writable collection types:

            - ArrayWritable
            - ArrayPrimitiveWritable
            - TwoDArrayWritable
            - MapWritable
            - SortedMapWritable
            - EnumSetWritable


        - Here is an example of using 'MapWritable':

            MapWritable src =new MapWritable();
            src.put(newIntWritable(1), new Text("cat"));
            src.put(newVIntWritable(2), new LongWritable(163));

            MapWritable dest = new MapWritable();
            WritableUtils.cloneInto(dest, src);

            assertThat((Text)dest.get(newIntWritable(1)), is(newText("cat")));
            assertThat((LongWritable)dest.get(newVIntWritable(2)), is(newLongWritable(163)));



- Implementing a Custom Writable

    - The built-in 'Writable' implementations serve most purposes, but on occasion you may need
        to write your own custom implementation.


    - An example of a custom Writable implementation is at '/code/ch05/TextPair.java'.



- Implementing a RawComparator for Speed

    - There is one further optimization we can make to our 'TextPair' class.  Right now, if we
        use 'TextPair' as a key in MapReduce, it will have to be deserialized into an object
        for the 'compareTo' method to be invoked.  


    - We want to compare 2 TextPair objects just by looking at their serialized representations.
        We can do this by leveraging the Text type's 'RawComparator'.  An example of this is
        implemented in the Comparator class in '/code/ch05/TextPair.java'.



- Custom Comparators

    - 



- Serialization Frameworks

    - Serialization IDL



- File-Based Data Structures

    - SequenceFile



- Writing a SequenceFile



- Reading a SequenceFile



- Displaying a SequenceFile with the Command-Line Interface



- Sorting and Merging SequenceFiles



- The SequenceFile Format



- MapFile

    - MapFile Variants



- Other File Formats and Column-Oriented Formats