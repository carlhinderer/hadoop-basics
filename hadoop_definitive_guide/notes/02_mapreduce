-----------------------------------------------------------------------
|  CHAPTER 2 - MAPREDUCE                                              |
-----------------------------------------------------------------------

- NCDC Weather Data

    - The data files we have are ASCII, with each line of the file representing a record
        from a weather station. 


    - Datafiles are organized by data and weather station.  There is a directory for each
        year from 1901 to 2001, each containing a gzipped file for each weather station
        with its reading for the year.


    - There are tens of thousands of weather stations, so the data was preprocessed so
        that each year's readings were concatenated into a single file.

        1901.gz
        1902.gz
        ...



- Analyzing the Data with Unix Tools

    - We created a simple Unix script to find the maximum recorded temperature for each year.
        An awk script extracts 2 fields from each record: the air temperature and quality
        code.


    - Here we run it:

        $ ./max-temperature.sh
        1901     317
        1902     244


    - The temperatures are scaled by a factor of 10, so the high temperature in 1901 was 
        31.7 degrees Celsius.


    - This approach works well enough, but there are 3 problems:

        1. We could parallelize the processing, but dividing the work into equal-sized pieces
             isn't always easy or obvious.

        2. Combining the results may require further processing.

        3. You are limited to the processing capacity of a single machine.



- Map and Reduce

    - MapReduce breaks the processing into 2 phases: map and reduce.  Each phase has a 
        key-value pair as it input and output.  The programmer also specifies a function
        for each phase.


    - In our map function, we take each of the lines in the file and extract out just the
        year and temperature.

        (1950, 0)
        (1950, 22)
        (1950, −11)
        (1949, 111)
        (1949, 78)


    - The output of the map function is processed by the MapReduce framework before being
        sent to the reduce function.  This processing sorts and groups the key-value pairs
        by key.  This is known as the 'shuffle'.

        (1949, [111, 78])
        (1950, [0, 22, −11])


    - Finally, the reduce function just needs to iterate through the list and pick the 
        maximum reading:

        (1949, 111)
        (1950, 22)



- Java MapReduce

    - We need 3 things:

        1. A 'map' function
        2. A 'reduce' function
        3. Some code to run the job



- Java Mapper

    - Here is the mapper function:

        ...
        import org.apache.hadoop.io.IntWritable;
        import org.apache.hadoop.io.LongWritable;
        import org.apache.hadoop.io.Text;
        import org.apache.hadoop.mapreduce.Mapper;

        public class MaxTemperatureMapper 
            extends Mapper<LongWritable, Text, Text, IntWritable> {
        
            private static final int MISSING = 9999;
        
            @Override
            public void map(LongWritable key, Text value, Context context) 
                throws IOException, InterruptedException {
                ...
                if (airTemperature != MISSING && quality.matches("[01459]")) {
                    context.write(new Text(year),new IntWritable(airTemperature));
                }
            }
        }


    - The 'Mapper' generic class has 4 type parameters:

        1. The input key       (LongWritable - A long integer offset)
        2. The input value     (Text - A line of text)
        3. The output key      (Text - A year)
        4. The output value    (IntWritable - The air temperature)


    - Rather than using built-in Java types, Hadoop provides its own set of basic types
        which are optimized for network serialization.  These types are found in the
        'org.apache.hadoop.io' package.

      Here, we use 'LongWritable' (like a Java Long), 'Text' (like a Java String), and
        'IntWritable' (like a Java Integer).


    - The 'map()' method is passed a key and value.  We convert the text value into a
        Java String, and then we can extract the columns we need from it.  


    - It is also passed an instance of 'Context', which we write the output to.  We write 
        the year as Text (since we're just using it as a key) and the temperature as an
        IntWritable.  We write an output record only if the temperature is present and
        the quality code indicates the reading is OK.



- Java Reducer

    - Here is the reducer function:

        import org.apache.hadoop.io.IntWritable;
        import org.apache.hadoop.io.Text;
        import org.apache.hadoop.mapreduce.Reducer;
        
        public class MaxTemperatureReducer
            extends Reducer<Text, IntWritable, Text, IntWritable> {
        
            @Override
            public void reduce(Text key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
                ...
                context.write(key, new IntWritable(maxValue));
            }
        }



- Java Job Runner

    - Finally, we have the code that runs the MapReduce job.

        import org.apache.hadoop.fs.Path;
        import org.apache.hadoop.io.IntWritable;
        import org.apache.hadoop.io.Text;
        import org.apache.hadoop.mapreduce.Job;
        import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
        import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
        
        public class MaxTemperature { 
        
            public static void main(String[]args) throws Exception {
                if (args.length!=2) {
                    System.err.println("Usage: MaxTemperature <input path> <output path>");
                    System.exit(-1);
                }
        
                Job job = new Job();
                job.setJarByClass(MaxTemperature.class);
                job.setJobName("Max temperature");
        
                FileInputFormat.addInputPath(job, newPath(args[0]));
                FileOutputFormat.setOutputPath(job,newPath(args[1]));
        
                job.setMapperClass(MaxTemperatureMapper.class);
                job.setReducerClass(MaxTemperatureReducer.class);
        
                job.setOutputKeyClass(Text.class);
                job.setOutputValueClass(IntWritable.class);
        
                System.exit(job.waitForCompletion(true)?0:1);
            }
        }


    - The 'Job' object forms the specification of the job and gives you control over
        how the job is run.  When we run this job on a Hadoop cluster, we will package
        the code into a JAR file, which Hadoop will distribute around the cluster.


    - Rather than explicitly specifying the name of the JAR file, we can pass a class in
        the 'setJarByClass()' method, which Hadoop will use to locate the relevant JAR by
        looking for the JAR file containing this class.


    - The input and output paths can be a single file, a directory, or a file pattern.
        The 'addInputPath()' method can be called more than once to use input from multiple
        paths.


    - The 'waitForCompletion()' method returns a boolean indicating success (true) or
        failure (false).



- Test Run

    - Now, we can test the job locally.  First, we need to set the HADOOP_CLASSPATH
        environment variable.

        $ export HADOOP_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)


    - Next, we compile our code against that classpath.

        $ cd code/ch01/java
        $ javac -classpath ${HADOOP_CLASSPATH} *


    - And we build our JAR.

        $ jar -cvf MaxTemperature.jar .


    - Now, we can run our job.

        $ /usr/local/hadoop/bin/hadoop jar MaxTemperature.jar MaxTemperature ../../../data/weather output


    - And we can check the results.

        $ ls output
        part-r-00000 _SUCCESS

        $ cat output/part-r-00000
        1901    317
        1902    244



- Scaling Out - Data Flow


- Combiner Functions


- Specifying a Combiner Function


- Hadoop Streaming


- Ruby Example


- Python Example