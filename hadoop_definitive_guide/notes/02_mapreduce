-----------------------------------------------------------------------
|  CHAPTER 2 - MAPREDUCE                                              |
-----------------------------------------------------------------------

- NCDC Weather Data

    - The data files we have are ASCII, with each line of the file representing a record
        from a weather station. 


    - Datafiles are organized by data and weather station.  There is a directory for each
        year from 1901 to 2001, each containing a gzipped file for each weather station
        with its reading for the year.


    - There are tens of thousands of weather stations, so the data was preprocessed so
        that each year's readings were concatenated into a single file.

        1901.gz
        1902.gz
        ...



- Analyzing the Data with Unix Tools

    - We created a simple Unix script to find the maximum recorded temperature for each year.
        An awk script extracts 2 fields from each record: the air temperature and quality
        code.


    - Here we run it:

        $ ./max-temperature.sh
        1901     317
        1902     244


    - The temperatures are scaled by a factor of 10, so the high temperature in 1901 was 
        31.7 degrees Celsius.


    - This approach works well enough, but there are 3 problems:

        1. We could parallelize the processing, but dividing the work into equal-sized pieces
             isn't always easy or obvious.

        2. Combining the results may require further processing.

        3. You are limited to the processing capacity of a single machine.



- Map and Reduce

    - MapReduce breaks the processing into 2 phases: map and reduce.  Each phase has a 
        key-value pair as it input and output.  The programmer also specifies a function
        for each phase.


    - In our map function, we take each of the lines in the file and extract out just the
        year and temperature.

        (1950, 0)
        (1950, 22)
        (1950, −11)
        (1949, 111)
        (1949, 78)


    - The output of the map function is processed by the MapReduce framework before being
        sent to the reduce function.  This processing sorts and groups the key-value pairs
        by key.  This is known as the 'shuffle'.

        (1949, [111, 78])
        (1950, [0, 22, −11])


    - Finally, the reduce function just needs to iterate through the list and pick the 
        maximum reading:

        (1949, 111)
        (1950, 22)



- Java MapReduce

    - We need 3 things:

        1. A 'map' function
        2. A 'reduce' function
        3. Some code to run the job



- Java Mapper

    - Here is the mapper function:

        ...
        import org.apache.hadoop.io.IntWritable;
        import org.apache.hadoop.io.LongWritable;
        import org.apache.hadoop.io.Text;
        import org.apache.hadoop.mapreduce.Mapper;

        public class MaxTemperatureMapper 
            extends Mapper<LongWritable, Text, Text, IntWritable> {
        
            private static final int MISSING = 9999;
        
            @Override
            public void map(LongWritable key, Text value, Context context) 
                throws IOException, InterruptedException {
                ...
                if (airTemperature != MISSING && quality.matches("[01459]")) {
                    context.write(new Text(year),new IntWritable(airTemperature));
                }
            }
        }


    - The 'Mapper' generic class has 4 type parameters:

        1. The input key       (LongWritable - A long integer offset)
        2. The input value     (Text - A line of text)
        3. The output key      (Text - A year)
        4. The output value    (IntWritable - The air temperature)


    - Rather than using built-in Java types, Hadoop provides its own set of basic types
        which are optimized for network serialization.  These types are found in the
        'org.apache.hadoop.io' package.

      Here, we use 'LongWritable' (like a Java Long), 'Text' (like a Java String), and
        'IntWritable' (like a Java Integer).


    - The 'map()' method is passed a key and value.  We convert the text value into a
        Java String, and then we can extract the columns we need from it.  


    - It is also passed an instance of 'Context', which we write the output to.  We write 
        the year as Text (since we're just using it as a key) and the temperature as an
        IntWritable.  We write an output record only if the temperature is present and
        the quality code indicates the reading is OK.



- Java Reducer

    - Here is the reducer function:

        import org.apache.hadoop.io.IntWritable;
        import org.apache.hadoop.io.Text;
        import org.apache.hadoop.mapreduce.Reducer;
        
        public class MaxTemperatureReducer
            extends Reducer<Text, IntWritable, Text, IntWritable> {
        
            @Override
            public void reduce(Text key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
                ...
                context.write(key, new IntWritable(maxValue));
            }
        }



- Java Job Runner

    - Finally, we have the code that runs the MapReduce job.

        import org.apache.hadoop.fs.Path;
        import org.apache.hadoop.io.IntWritable;
        import org.apache.hadoop.io.Text;
        import org.apache.hadoop.mapreduce.Job;
        import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
        import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
        
        public class MaxTemperature { 
        
            public static void main(String[]args) throws Exception {
                if (args.length!=2) {
                    System.err.println("Usage: MaxTemperature <input path> <output path>");
                    System.exit(-1);
                }
        
                Job job = new Job();
                job.setJarByClass(MaxTemperature.class);
                job.setJobName("Max temperature");
        
                FileInputFormat.addInputPath(job, newPath(args[0]));
                FileOutputFormat.setOutputPath(job,newPath(args[1]));
        
                job.setMapperClass(MaxTemperatureMapper.class);
                job.setReducerClass(MaxTemperatureReducer.class);
        
                job.setOutputKeyClass(Text.class);
                job.setOutputValueClass(IntWritable.class);
        
                System.exit(job.waitForCompletion(true)?0:1);
            }
        }


    - The 'Job' object forms the specification of the job and gives you control over
        how the job is run.  When we run this job on a Hadoop cluster, we will package
        the code into a JAR file, which Hadoop will distribute around the cluster.


    - Rather than explicitly specifying the name of the JAR file, we can pass a class in
        the 'setJarByClass()' method, which Hadoop will use to locate the relevant JAR by
        looking for the JAR file containing this class.


    - The input and output paths can be a single file, a directory, or a file pattern.
        The 'addInputPath()' method can be called more than once to use input from multiple
        paths.


    - The 'waitForCompletion()' method returns a boolean indicating success (true) or
        failure (false).



- Test Run

    - Now, we can test the job locally.  First, we need to set the HADOOP_CLASSPATH
        environment variable.

        $ export HADOOP_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)


    - Next, we compile our code against that classpath.

        $ cd code/ch02/java
        $ javac -classpath ${HADOOP_CLASSPATH} *


    - And we build our JAR.

        $ jar -cvf MaxTemperature.jar .


    - We can view the contents of the JAR if we want to.

        $ jar tf MaxTemperature.jar


    - Now, we can run our job.

        $ /usr/local/hadoop/bin/hadoop jar MaxTemperature.jar MaxTemperature ../../../data/weather output


    - And we can check the results.

        $ ls output
        part-r-00000 _SUCCESS

        $ cat output/part-r-00000
        1901    317
        1902    244



- Scaling Out - Data Flow

    - A MapReduce 'job' is a unit of work to be performed.  It consists of the input data,
        a MapReduce program, and configuration.


    - Hadoop runs the job by dividing it into 'tasks'.  There are 2 types of tasks: 'map tasks'
        and 'reduce tasks'.  The tasks are scheduled using YARN and run on nodes in the cluster.
        If a task fails, it will be automatically scheduled to run on a different node.


    - Hadoop divides tasks into fixed-size pieces called 'input splits' and creates one map task
        for each split.  If splits are too big, the processing won't be balanced.  If the splits
        are too small, the management overhead will decrease efficiency.  The default split
        is 128 MB, the size of an HDFS block.


    - Hadoop will try to run the map task:

        1. On the machine where the data resides (this is most common and desired)
        2. On another node on the same server rack
        3. On another node on another server rack


    - Map tasks write their output to the local disk, not to HDFS.  This is because it is
        intermediate output - it is used by reduce tasks, and once used it can be thrown
        away.  So, it's not worth storing on HDFS in a replicated manner.


    - Reduce tasks don't have the advantage of data locality.  The input to a single reduce
        task is the output from all mappers.  If our example, we have a single reduce task
        that is fed from all the map tasks.  The number of reduce tasks is not governed by
        the size of the input, but instead is specified independently.

      Therefore, the sorted map outputs have to be transferred across the network to the node 
        where the reduce task is running, where they are merged and passed to the user-defined
        reduce function.


    - The output of the reduce tasks is typically stored in HDFS for reliability.


    - When there are multiple reducers, the map tasks partition their output, each creating
        one partition for each reduce task.  The partitioning can be controlled by a user-defined
        partitioning function, but the default key-hashing function usually works well.


    - The data flow between map and reduce tasks is known as the 'shuffle'.  It can have a big
        effect on execution time.


    - Finally, it is possible to have no reduce task in situations where the processing can be
        carried out entirely in parallel.



- Combiner Functions

    - It pays to always look for ways to minimize traffic between map and reduce tasks.  One thing
        we can do is specify a 'combiner' function to be run on the map output, which forms the
        input to the reducer function.


    - Because the combiner function is an optimization, There is no guarantee on whether it will
        be called.  So calling it 0, 1, or many times should still produce the same output to the
        reduce function.


    - For example:

        Imagine our weather data for 1950 was processed by 2 maps.  

          # First map output
          (1950, 0)
          (1950, 20)
          (1950, 10)

          # Second map output
          (1950, 25)
          (1950, 15)

        Our reduce function would be passed the input:

          # Input to reduce function
          (1950, [0, 20, 10, 25, 15])

          # Output of reduce function
          (1950, 25)


        But if we used a combiner function on the map output before passing it to
          the reducer, we could send this input instead:

          # First map output (after combiner)
          (1950, 20)

          # Second map output (after combiner)
          (1950, 25)

          # Input to reduce function
          (1950, [20, 25])


    - Note that not all function have this property.  For instance, we would not be
        able to calculate the mean this way.  But when we can use it, it can cut down
        on the shuffling between the map and reduce functions.



- Specifying a Combiner Function

    - We can specify the combiner class just like the mapper and reducer.

        public class MaxTemperatureWithCombiner {
            public static void main (String args[]) throws Exception {
                ...
                job.setMapperClass(MaxTemperatureMapper.class);
                job.setCombinerClass(MaxTemperatureReducer.class);
                job.setReducerClass(MaxTemperatureReducer.class);
                ...
            }
        }



- Hadoop Streaming

    - Hadoop provides an API that lets you write map and reduce functions in other languages
        besides Java.  'Hadoop Streaming' uses Unix standard streams as the interface
        between Hadoop and your program, so you can use any language that can read from stdin
        and write to stdout.



- Ruby Example

    - Here is a map function in Ruby:

        #!/usr/bin/env ruby

        STDIN.each_line do |line|
          val = line
          year, temp, q = val[15,4], val[87,5], val[92,1]
          puts "#{year}\t#{temp}" if (temp != "+9999" && q =~ /[01459]/)
        end


    - Here is a reduce function in Ruby:

        #!/usr/bin/env ruby

        last_key, max_val = nil, -1000000
        STDIN.each_line do |line|
          key, val = line.split("\t")
          if last_key && last_key != key
            puts "#{last_key}\t#{max_val}"
            last_key, max_val = key, val.to_i
          else
            last_key, max_val = key, [max_val, val.to_i].max
          end
        end
        puts "#{last_key}\t#{max_val}" if last_key


    - Now, we can simulate the whole MapReduce pipeline with a Unix pipeline:

        $ cat input/ncdc/sample.txt | \
          max_temperature_map.rb | \
          sort | \
          max_temperature_reduce.rb


    - And we can run it using Hadoop:

        $ hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \  
          -input input/ncdc/sample.txt \  
          -output output \  
          -mapper ruby/max_temperature_map.rb \  
          -reducer ruby/max_temperature_reduce.rb


    - If we also want to use a combiner:

        $ hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \  
          -files ruby/max_temperature_map.rb, ruby/max_temperature_reduce.rb \  
          -input input/ncdc/sample.txt \  
          -output output \  
          -mapper ruby/max_temperature_map.rb \  
          -combiner ruby/max_temperature_reduce.rb \  
          -reducer ruby/max_temperature_reduce.rb



- Python Example

    - Here is an example of a Python map function:

        #!/usr/bin/env python

        import re
        import sys
        
        for line in sys.stdin:
            val = line.strip()
            (year, temp, q) = (val[15:19], val[87:92], val[92:93])
            if (temp != "+9999" and re.match("[01459]", q)):
                print("%s\t%s" % (year,temp))


    - Here is an example of a Python reduce function:

        #!/usr/bin/env python

        import sys

        (last_key, max_val) = (None, -sys.maxint)
        for line in sys.stdin:
            (key,val) = line.strip().split("\t")
            if last_key and last_key != key:
                print("%s\t%s" % (last_key,max_val))
                (last_key, max_val) = (key, int(val))
            else:
                (last_key, max_val) = (key, max(max_val,int(val)))

        if last_key:
            print("%s\t%s" % (last_key,max_val))


    - And we can run it:

        $ cat input/ncdc/sample.txt | \
          max_temperature_map.py | \
          sort | \
          max_temperature_reduce.py